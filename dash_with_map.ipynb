{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the result to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "# Process the 'reeds_ba_list' column to expand the sets into individual rows, keeping 'state' intact\n",
    "from ast import literal_eval\n",
    "\n",
    "def createshapefile():\n",
    "    # Get the current directory where your script is running\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Construct the path to your resources folder dynamically\n",
    "    resources_path = os.path.join(current_directory, 'resources')\n",
    "\n",
    "    # Now, build the full path to your CSV files\n",
    "    polygons_csv_path = os.path.join(resources_path, 'US_CAN_MEX_PCA_polygons.csv')\n",
    "    state_to_ba_csv_path = os.path.join(resources_path, 'state_to_ba_mapping.csv')\n",
    "    # Finally, use pandas to read the CSV files\n",
    "    polygons_df = pd.read_csv(polygons_csv_path)\n",
    "    state_to_ba_df = pd.read_csv(state_to_ba_csv_path)\n",
    "    # Filter polygons\n",
    "    polygons_df = polygons_df[polygons_df['rb'].isin([f'p{i}' for i in range(1, 135)])]\n",
    "\n",
    "    # Since the instructions for processing the state mapping are a bit unclear,\n",
    "    # Convert the string representation of sets in 'reeds_ba_list' to actual sets\n",
    "    state_to_ba_df['reeds_ba_list'] = state_to_ba_df['reeds_ba_list'].apply(literal_eval)\n",
    "\n",
    "    # Explode the sets into separate rows\n",
    "    exploded_df = state_to_ba_df.explode('reeds_ba_list')[['state','reeds_ba_list']]\n",
    "    polygons_df_filtered = polygons_df[polygons_df['rb'].isin([f'p{i}' for i in range(1, 135)])]\n",
    "    polygons_gdf = gpd.GeoDataFrame(polygons_df_filtered, geometry=gpd.GeoSeries.from_wkt(polygons_df_filtered['WKT']))\n",
    "\n",
    "    # Step 2: Merge exploded_df with polygons_gdf on the 'reeds_ba_list' and 'rb' columns\n",
    "    merged_gdf = polygons_gdf.merge(exploded_df, left_on='rb', right_on='reeds_ba_list')\n",
    "    merged_gdf['country']='USA'\n",
    "    merged_gdf=merged_gdf[['rb','state','country','geometry']]\n",
    "\n",
    "    # Country - Group by 'country' and dissolve to merge geometries\n",
    "    gdf_country = merged_gdf.dissolve(by='country').reset_index()\n",
    "\n",
    "    # State - Group by 'state' and dissolve to merge geometries\n",
    "    gdf_state = merged_gdf.dissolve(by='state').reset_index()\n",
    "\n",
    "    # Subregion (rb) - No need to group since each 'rb' is already unique\n",
    "    gdf_subregion = merged_gdf[['rb', 'geometry']].drop_duplicates()\n",
    "\n",
    "\n",
    "    # Convert each GeoDataFrame to GeoJSON for use in Plotly\n",
    "    geojson_country = json.loads(gdf_country.to_json())\n",
    "    geojson_state = json.loads(gdf_state.to_json())\n",
    "    geojson_subregion = json.loads(gdf_subregion.to_json())\n",
    "    import json\n",
    "    current_directory= os.path.join(current_directory, 'web_page_data')\n",
    "    # Define file paths for saving\n",
    "    geojson_country_path = os.path.join(current_directory, 'geojson_country.json')\n",
    "    geojson_state_path = os.path.join(current_directory, 'geojson_state.json')\n",
    "    geojson_subregion_path = os.path.join(current_directory, 'geojson_subregion.json')\n",
    "    # Define file paths for saving\n",
    "    gdf_country_path = os.path.join(current_directory, 'gdf_country.gpkg')\n",
    "    gdf_state_path = os.path.join(current_directory, 'gdf_state.gpkg')\n",
    "    gdf_subregion_path = os.path.join(current_directory, 'gdf_subregion.gpkg')\n",
    "\n",
    "    # Save the GeoDataFrames\n",
    "    gdf_country.to_file(gdf_country_path, driver='GPKG')\n",
    "    gdf_state.to_file(gdf_state_path, driver='GPKG')\n",
    "    gdf_subregion.to_file(gdf_subregion_path, driver='GPKG')\n",
    "\n",
    "    # Function to save GeoJSON to a file\n",
    "    def save_geojson(data, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    # Save the GeoJSON data\n",
    "    save_geojson(geojson_country, geojson_country_path)\n",
    "    save_geojson(geojson_state, geojson_state_path)\n",
    "    save_geojson(geojson_subregion, geojson_subregion_path)\n",
    "    # Function to read GeoJSON from a file\n",
    "    def read_geojson(file_path):\n",
    "        with open(file_path) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Read the GeoJSON data\n",
    "    geojson_country_read = read_geojson(geojson_country_path)\n",
    "    geojson_state_read = read_geojson(geojson_state_path)\n",
    "    geojson_subregion_read = read_geojson(geojson_subregion_path)\n",
    "    print(geojson_country == geojson_country_read)\n",
    "    print(geojson_state == geojson_state_read)\n",
    "    print(geojson_subregion == geojson_subregion_read)\n",
    "    # Read the GeoDataFrames\n",
    "    gdf_country_read = gpd.read_file(gdf_country_path)\n",
    "    gdf_state_read = gpd.read_file(gdf_state_path)\n",
    "    gdf_subregion_read = gpd.read_file(gdf_subregion_path)\n",
    "    print(gdf_country.shape == gdf_country_read.shape)\n",
    "    print(gdf_state.shape == gdf_state_read.shape)\n",
    "    print(gdf_subregion.shape == gdf_subregion_read.shape)\n",
    "\n",
    "    # Sample a row and compare geometries (example using 'gdf_country')\n",
    "    print(gdf_country.geometry.iloc[0].equals(gdf_country_read.geometry.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create lower the bound for future test\n",
    "so we take the lowerest month in the training data and hottest month, teh 97.5 confidence inteval, the hottest month donate the upper bound, lowest month donate colde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/602522168.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  grouped_yearly_data.reset_index(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "for case in ['projection']:\n",
    "    # Define the directory where the CSV files are located and other initial setups\n",
    "    directory = f'/Volumes/T7/prediction/{case}'\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = list(range(2020, 2100))\n",
    "\n",
    "    def string_to_set(string):\n",
    "        elements = string.replace('{', '').replace('}', '').split(',')\n",
    "        return set(e.strip().replace(\"'\", \"\") for e in elements)\n",
    "\n",
    "    def upper_95(x):\n",
    "        return x.quantile(0.95)\n",
    "\n",
    "    def lower_5(x):\n",
    "        return x.quantile(0.05)\n",
    "\n",
    "    def process_rb_files_hourly(rb_code, year):\n",
    "        file_name = f\"{rb_code}_{year}_mlp_output.csv\"\n",
    "        file_path = os.path.join(directory, str(year), file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            temp_df = pd.read_csv(file_path, usecols=['Time_UTC', 'Load'])\n",
    "            temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "            temp_df[rb_code]=temp_df['Load']\n",
    "            # temp_df['Hour'] = temp_df['Time_UTC'].dt.hour\n",
    "            # temp_df['Year'] = temp_df['Time_UTC'].dt.year\n",
    "            # temp_df['Weekend_or_Weekday'] = np.where(temp_df['Time_UTC'].dt.dayofweek < 5, 'Weekday', 'Weekend')\n",
    "            # temp_df['Day'] = temp_df['Time_UTC'].dt.day\n",
    "            # temp_df['Month'] = temp_df['Time_UTC'].dt.month\n",
    "            temp_df.drop(columns=['Load'], inplace=True)\n",
    "            return temp_df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def aggregate_state_and_usa_daily(df, state_to_ba_mapping):\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])\n",
    "            state_column = row['state']\n",
    "            df[state_column] = df[[col for col in df.columns if col in rb_list]].sum(axis=1)\n",
    "        df['USA'] = df[[row['state'] for index, row in state_to_ba_mapping.iterrows()]].sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def process_yearly_data(year):\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "        First = True\n",
    "\n",
    "        for rb_code in rb_codes:\n",
    "            temp_df = process_rb_files_hourly(rb_code, year)\n",
    "            if First:\n",
    "                processed_df=temp_df.copy()\n",
    "                First=False\n",
    "            else:\n",
    "                processed_df = processed_df.merge(temp_df, on=['Time_UTC'], how='inner')\n",
    "        yearly_data=processed_df \n",
    "\n",
    "        yearly_data = aggregate_state_and_usa_daily(yearly_data, state_to_ba_mapping)\n",
    "        yearly_data['Hour'] = yearly_data['Time_UTC'].dt.hour\n",
    "        yearly_data['Year'] = yearly_data['Time_UTC'].dt.year\n",
    "        yearly_data['Weekend_or_Weekday'] = np.where(yearly_data['Time_UTC'].dt.dayofweek < 5, 'Weekday', 'Weekend')\n",
    "        yearly_data.drop(columns=['Time_UTC'], inplace=True)\n",
    "        columns_to_aggregate = [col for col in yearly_data.columns if col not in ['Hour', 'Year', 'Weekend_or_Weekday']]\n",
    "        aggregations = {col: ['mean', upper_95, lower_5,'max'] for col in columns_to_aggregate}\n",
    "            \n",
    "        # Group by 'Hour', 'Year', 'Weekend_or_Weekday', then aggregate\n",
    "        grouped_yearly_data = yearly_data.groupby(['Hour', 'Year', 'Weekend_or_Weekday']).agg(aggregations)\n",
    "        # Flatten the MultiIndex columns if you have multiple columns being aggregated\n",
    "        grouped_yearly_data.columns = ['_'.join(col).strip() for col in grouped_yearly_data.columns.values]\n",
    "\n",
    "        # Reset index if you want 'Hour', 'Year', 'Weekend_or_Weekday' back as regular columns\n",
    "        grouped_yearly_data.reset_index(inplace=True)\n",
    "        grouped_yearly_data.rename(columns=lambda x: x.replace('mean', 'mean').replace('upper_95', 'upper').replace('lower_5', 'lower'), inplace=True)\n",
    "\n",
    "        return grouped_yearly_data\n",
    "\n",
    "    final_dfs = []\n",
    "    for year in years:\n",
    "        yearly_data = process_yearly_data(year)\n",
    "        if not yearly_data.empty:\n",
    "            final_dfs.append(yearly_data)\n",
    "\n",
    "    final_df = pd.concat(final_dfs, ignore_index=True)\n",
    "    final_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/mock_{case}_yearly_aggregated.csv')\n",
    "    final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in ['projection']:\n",
    "\n",
    "    # Define the directory where the CSV files are located\n",
    "    directory = f'/Volumes/T7/prediction/{case}'\n",
    "\n",
    "    # Create a list of all 'rb' codes and years\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = list(range(2020, 2100))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "\n",
    "    def process_rb_files_daily(rb_code, years):\n",
    "        rb_df_list = []\n",
    "        for year in years:\n",
    "            folder_path = os.path.join(directory, str(year))\n",
    "            file_name = f\"{rb_code}_{year}_mlp_output.csv\"\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                temp_df = pd.read_csv(file_path)[['Time_UTC', 'Load']]\n",
    "                temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "                # Additional grouping by day\n",
    "                temp_df['Day'] = temp_df['Time_UTC'].dt.day\n",
    "                temp_df['Year'] = temp_df['Time_UTC'].dt.year\n",
    "                temp_df['Month'] = temp_df['Time_UTC'].dt.month\n",
    "                aggregated_df = temp_df.groupby(['Year', 'Month', 'Day'])['Load'].sum().reset_index()\n",
    "                rb_df_list.append(aggregated_df)\n",
    "        concatenated_df = pd.concat(rb_df_list, ignore_index=True)\n",
    "        return concatenated_df\n",
    "    def aggregate_state_and_usa_daily(final_rb_df):\n",
    "        # Read the state to rb mapping\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "        \n",
    "        # For each state, sum the relevant 'rb' daily sums\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])  # Convert string to list/set of rb codes\n",
    "            state_column = row['state']\n",
    "            # Ensure only existing columns are summed\n",
    "            rb_columns = [rb for rb in rb_list if rb in final_rb_df.columns]\n",
    "            # Summing for each state\n",
    "            final_rb_df[state_column] = final_rb_df[rb_columns].sum(axis=1)\n",
    "        \n",
    "        # Summing all states to get USA total\n",
    "        state_columns = state_to_ba_mapping['state'].tolist()\n",
    "        final_rb_df['USA'] = final_rb_df[state_columns].sum(axis=1)\n",
    "        \n",
    "        return final_rb_df\n",
    "\n",
    "    def upper_95(x):\n",
    "        return x.quantile(0.95)\n",
    "\n",
    "    def lower_5(x):\n",
    "        return x.quantile(0.05)\n",
    "    # Define a function to reorganize each column into an array grouped by Year\n",
    "    def reorganize_into_array(series):\n",
    "        # Pre-fill the array with NaNs to handle missing weekdays\n",
    "        array = [np.nan] * 7  # One entry for each day of the week\n",
    "        for idx, value in series.items():\n",
    "            # idx is a tuple (Year, weekday), value is the column value\n",
    "            # Subtract 1 from idx[1] if weekday starts from 1 in your dataset\n",
    "            array[idx[1]] = value  # Or idx[1] - 1 if weekday is 1-based\n",
    "        return array\n",
    "\n",
    "    final_dfs = []\n",
    "    for year in years:\n",
    "        # Process data for the year (pseudo-code placeholders for actual processing)\n",
    "        First=True\n",
    "        for rb_code in rb_codes:\n",
    "            temp_df = process_rb_files_daily(rb_code, [year])\n",
    "            if First:\n",
    "                processed_df=temp_df.copy()\n",
    "                First=False\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "            else:\n",
    "                # Assuming processed_df and temp_df are defined and have the appropriate columns\n",
    "                processed_df = processed_df.merge(temp_df, on=['Year', 'Month', 'Day'], how='inner')\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "        processed_df = aggregate_state_and_usa_daily(processed_df)\n",
    "        processed_df['Date'] = pd.to_datetime(processed_df[['Year', 'Month', 'Day']])\n",
    "\n",
    "        # Create the 'weekday' column, note that dt.dayofweek returns Monday=0 through Sunday=6, so we add 1\n",
    "        processed_df['weekday'] = processed_df['Date'].dt.dayofweek \n",
    "        processed_df=processed_df.drop(['Month','Day','Date'], axis=1)\n",
    "        # Group by 'weekday' and 'Year', then apply the aggregations\n",
    "        aggregations = {col: ['mean','max', upper_95, lower_5] for col in processed_df.columns if col not in ['Year', 'weekday']}\n",
    "        grouped_df = processed_df.groupby(['Year', 'weekday']).agg(aggregations)\n",
    "        # Now, flatten the MultiIndex in columns created by aggregation\n",
    "        grouped_df.columns = ['{}_{}'.format(col[0], col[1]) for col in grouped_df.columns]\n",
    "\n",
    "        # To further match your requirement, rename the aggregation methods in column names\n",
    "        grouped_df.rename(columns=lambda x: x.replace('mean', 'mean').replace('upper_95', 'upper').replace('lower_5', 'lower'), inplace=True)\n",
    "\n",
    "        # Reset index to turn 'Year' and 'weekday' back into columns if needed\n",
    "        grouped_df= grouped_df.reset_index()\n",
    "        # First, let's ensure 'weekday' is in the correct data type if not already\n",
    "        grouped_df['weekday'] = grouped_df['weekday'].astype(int)\n",
    "        final_dfs.append(grouped_df)\n",
    "        \n",
    "\n",
    "    final_df = pd.concat(final_dfs, axis=0, ignore_index=True)\n",
    "    final_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/mock_{case}_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# Placeholder function to read 'state_to_ba_mapping.csv' and perform aggregation\n",
    "for case in ['projection']:\n",
    "    def string_to_set(string):\n",
    "        # Remove curly braces and split the string\n",
    "        elements = string.replace('{', '').replace('}', '').split(',')\n",
    "        # Remove any extra whitespace and single quotes from each element\n",
    "        elements = [e.strip().replace(\"'\", \"\") for e in elements]\n",
    "        return set(elements)\n",
    "    # Define the directory where the CSV files are located\n",
    "    directory = f'/Volumes/T7/prediction/{case}'\n",
    "\n",
    "    # Create a list of all 'rb' codes and years\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = list(range(2020, 2100))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "\n",
    "    def process_rb_files_daily(rb_code, years):\n",
    "        rb_df_list = []\n",
    "        for year in years:\n",
    "            folder_path = os.path.join(directory, str(year))\n",
    "            file_name = f\"{rb_code}_{year}_mlp_output.csv\"\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                temp_df = pd.read_csv(file_path)[['Time_UTC', 'Load']]\n",
    "                temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "                rb_df_list.append(temp_df)\n",
    "        concatenated_df = pd.concat(rb_df_list, ignore_index=True)\n",
    "        return concatenated_df\n",
    "    def aggregate_state_and_usa_daily(final_rb_df):\n",
    "        # Read the state to rb mapping\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "        \n",
    "        # For each state, sum the relevant 'rb' daily sums\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])  # Convert string to list/set of rb codes\n",
    "            state_column = row['state']\n",
    "            # Ensure only existing columns are summed\n",
    "            rb_columns = [rb for rb in rb_list if rb in final_rb_df.columns]\n",
    "            # Summing for each state\n",
    "            final_rb_df[state_column] = final_rb_df[rb_columns].sum(axis=1)\n",
    "        \n",
    "        # Summing all states to get USA total\n",
    "        state_columns = state_to_ba_mapping['state'].tolist()\n",
    "        final_rb_df['USA'] = final_rb_df[state_columns].sum(axis=1)\n",
    "        \n",
    "        return final_rb_df\n",
    "\n",
    "\n",
    "\n",
    "    final_dfs = []\n",
    "    for year in years:\n",
    "        # Process data for the year (pseudo-code placeholders for actual processing)\n",
    "        First=True\n",
    "        for rb_code in rb_codes:\n",
    "            temp_df = process_rb_files_daily(rb_code, [year])\n",
    "            if First:\n",
    "                processed_df=temp_df.copy()\n",
    "                First=False\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "            else:\n",
    "                # Assuming processed_df and temp_df are defined and have the appropriate columns\n",
    "                processed_df = processed_df.merge(temp_df, on=['Time_UTC'], how='inner')\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "        processed_df = aggregate_state_and_usa_daily(processed_df)\n",
    "        processed_df['Time_UTC'] = pd.to_datetime(processed_df['Time_UTC'])\n",
    "        processed_df['Year'] = processed_df['Time_UTC'].dt.year\n",
    "        processed_df['Month'] = processed_df['Time_UTC'].dt.month\n",
    "\n",
    "\n",
    "        processed_df=processed_df.drop(['Time_UTC'], axis=1)\n",
    "        # Group by 'weekday' and 'Year', then apply the aggregations\n",
    "        # Define aggregations to apply 'max' to all columns except 'Year', 'Month', and 'weekday'\n",
    "        aggregations = {col: 'max' for col in processed_df.columns if col not in ['Year', 'Month']}\n",
    "\n",
    "        # Group by 'Year' and 'Month', then aggregate\n",
    "        grouped_df = processed_df.groupby(['Year', 'Month']).agg(aggregations)\n",
    "\n",
    "\n",
    "\n",
    "        # Reset index to turn 'Year' and 'Month' back into columns if needed\n",
    "        grouped_df.reset_index(inplace=True)\n",
    "        final_dfs.append(grouped_df)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    final_df = pd.concat(final_dfs, axis=0, ignore_index=True)\n",
    "    final_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/max_{case}_monthlly.csv')\n",
    "    final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "cases = ['projection']\n",
    "for case in cases:\n",
    "    # Define the directory where the CSV files are located\n",
    "    directory = f'/Volumes/T7/prediction/{case}'\n",
    "\n",
    "    # Create a list of all 'rb' codes and years\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "\n",
    "\n",
    "\n",
    "    # Function to read and process files for a given rb code\n",
    "    def process_rb_files(rb_code):\n",
    "        rb_df_list = []\n",
    "        # Loop through all years for the given rb_code\n",
    "        for year in years:\n",
    "            # Construct file name\n",
    "            folder_path = os.path.join(directory, str(year))\n",
    "            file_name = f\"{rb_code}_{year}_mlp_output.csv\"\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Check if file exists to avoid FileNotFoundError\n",
    "            if os.path.isfile(file_path):\n",
    "                # Read the CSV file\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                temp_df=temp_df[['Time_UTC','Load']]\n",
    "                # Convert 'Time_UTC' to datetime and extract 'Year' and 'Month'\n",
    "                temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "                temp_df['Year'] = temp_df['Time_UTC'].dt.year\n",
    "                temp_df['Month'] = temp_df['Time_UTC'].dt.month\n",
    "                # Aggregate Load by 'Year' and 'Month'\n",
    "                aggregated_df = temp_df.groupby(['Year', 'Month'])['Load'].sum().reset_index()\n",
    "                # Add the aggregated data to the list\n",
    "                rb_df_list.append(aggregated_df)\n",
    "        \n",
    "        # Vertically concatenate all yearly data for the rb code\n",
    "        concatenated_df = pd.concat(rb_df_list, ignore_index=True)\n",
    "        return concatenated_df\n",
    "\n",
    "    years = list(range(2020, 2100))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "    # Create a DataFrame with all combinations of 'Year' and 'Month'\n",
    "    year_month_df = pd.DataFrame([(y, m) for y in years for m in months], columns=['Year', 'Month'])\n",
    "\n",
    "    # Placeholder to simulate merging data for multiple rb codes\n",
    "    final_df = year_month_df.copy()\n",
    "    for rb_code in rb_codes: \n",
    "        rb_df = process_rb_files(rb_code)\n",
    "        final_df = final_df.merge(rb_df, on=['Year', 'Month'], how='left').rename(columns={'Load': rb_code})\n",
    "    # Placeholder function to read 'state_to_ba_mapping.csv' and perform aggregation\n",
    "    def string_to_set(string):\n",
    "        # Remove curly braces and split the string\n",
    "        elements = string.replace('{', '').replace('}', '').split(',')\n",
    "        # Remove any extra whitespace and single quotes from each element\n",
    "        elements = [e.strip().replace(\"'\", \"\") for e in elements]\n",
    "        return set(elements)\n",
    "    def aggregate_state_data(final_df):\n",
    "        # Placeholder to simulate reading the 'state_to_ba_mapping.csv' file\n",
    "        # In practice, this should read the actual file\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "\n",
    "        # Add columns for each state by summing the relevant rb columns based on the mapping\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])\n",
    "            state_column = row['state']\n",
    "            # Sum the columns specified in reeds_ba_list for each state\n",
    "            final_df[state_column] = final_df[list(rb_list)].sum(axis=1)\n",
    "\n",
    "        # Calculate the total for the USA by summing all state columns\n",
    "        state_columns = state_to_ba_mapping['state'].values\n",
    "        final_df['USA'] = final_df[state_columns].sum(axis=1)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    # Apply the placeholder aggregation function to the simulated final_df\n",
    "    final_aggregated_df = aggregate_state_data(final_df)\n",
    "\n",
    "    def aggregate_state_data(final_df):\n",
    "        # Placeholder to simulate reading the 'state_to_ba_mapping.csv' file\n",
    "        # In practice, this should read the actual file\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "\n",
    "        # Add columns for each state by summing the relevant rb columns based on the mapping\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])\n",
    "            state_column = row['state']\n",
    "            # Sum the columns specified in reeds_ba_list for each state\n",
    "            final_df[state_column] = final_df[list(rb_list)].sum(axis=1)\n",
    "\n",
    "        # Calculate the total for the USA by summing all state columns\n",
    "        state_columns = state_to_ba_mapping['state'].values\n",
    "        final_df['USA'] = final_df[state_columns].sum(axis=1)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    # Apply the placeholder aggregation function to the simulated final_df\n",
    "    final_aggregated_df = aggregate_state_data(final_df)\n",
    "    final_aggregated_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/mock_{case}csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import os\n",
    "\n",
    "# Define the directory where your CSV files are located\n",
    "directory = '/Users/ansonkong/Downloads/Data for nyu work/output/demand_weather_combine_2007'\n",
    "ci_output_directory = os.path.join('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources', 'CI_results')\n",
    "os.makedirs(ci_output_directory, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "def calculate_quantile_ci(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval using quantiles for a given dataset.\n",
    "    \"\"\"\n",
    "    # Calculate the lower and upper percentile bounds for the confidence interval\n",
    "    lower_percentile = 100 * (1 - confidence) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "    \n",
    "    # Use np.percentile to find the lower and upper quantiles\n",
    "    ci_lower = np.percentile(data, lower_percentile)\n",
    "    ci_upper = np.percentile(data, upper_percentile)\n",
    "    \n",
    "    return ci_lower, ci_upper\n",
    "\n",
    "# List all files in the directory\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "for file in files:\n",
    "    filepath = os.path.join(directory, file)\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Extract 'rb' from filename\n",
    "    rb_code = file.split('_')[0]\n",
    "    \n",
    "    # Convert Time_UTC to datetime and extract Year, Month, and Day\n",
    "    df['Time_UTC'] = pd.to_datetime(df['Time_UTC'])\n",
    "    df['Month'] = df['Time_UTC'].dt.month\n",
    "    \n",
    "    # Initialize lists to store CI results\n",
    "    ci_results_max = []\n",
    "    ci_results_min = []\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        month_data = df[df['Month'] == month]\n",
    "        daily_max = month_data.groupby(month_data['Time_UTC'].dt.date)['T2'].max()\n",
    "        daily_min = month_data.groupby(month_data['Time_UTC'].dt.date)['T2'].min()\n",
    "        \n",
    "        ci_max_lower, ci_max_upper = calculate_quantile_ci(daily_max)\n",
    "        ci_min_lower, ci_min_upper = calculate_quantile_ci(daily_min)\n",
    "        \n",
    "        ci_results_max.append([rb_code, ci_max_lower, ci_max_upper])\n",
    "        ci_results_min.append([rb_code, ci_min_lower, ci_min_upper])\n",
    "    \n",
    "    # Convert CI results to DataFrames\n",
    "    ci_df_max = pd.DataFrame(ci_results_max, columns=['rb',  'CI_max_lower', 'CI_max_upper'])\n",
    "    ci_df_min = pd.DataFrame(ci_results_min, columns=['rb',  'CI_min_lower', 'CI_min_upper'])\n",
    "    # For ci_df_max, keep only the maximum upper CI for each rb\n",
    "    ci_df_max_reduced = ci_df_max.groupby('rb')['CI_max_upper'].max().reset_index()\n",
    "\n",
    "    # For ci_df_min, keep only the minimum lower CI for each rb\n",
    "    ci_df_min_reduced = ci_df_min.groupby('rb')['CI_min_lower'].min().reset_index()\n",
    "\n",
    "    # Reindex the DataFrames\n",
    "    # ci_df_max_reduced.reset_index(drop=True, inplace=True)\n",
    "    # ci_df_min_reduced.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Save CI DataFrames to CSV files\n",
    "    ci_df_max_reduced.to_csv(os.path.join(ci_output_directory, f'{rb_code}_CI_max.csv'), index=False)\n",
    "    ci_df_min_reduced.to_csv(os.path.join(ci_output_directory, f'{rb_code}_CI_min.csv'), index=False)\n",
    "# Define the directory where your CSV files are located\n",
    "directory = '/Users/ansonkong/Downloads/Data for nyu work/output/demand_weather_combine_2007'\n",
    "ci_output_directory = os.path.join('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources', 'CI_results')\n",
    "os.makedirs(ci_output_directory, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "def calculate_quantile_ci(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval using quantiles for a given dataset.\n",
    "    \"\"\"\n",
    "    # Calculate the lower and upper percentile bounds for the confidence interval\n",
    "    lower_percentile = 100 * (1 - confidence) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "    \n",
    "    # Use np.percentile to find the lower and upper quantiles\n",
    "    ci_lower = np.percentile(data, lower_percentile)\n",
    "    ci_upper = np.percentile(data, upper_percentile)\n",
    "    \n",
    "    return ci_lower, ci_upper\n",
    "\n",
    "# List all files in the directory\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "states=['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "           'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "          'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "          'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "          'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming','usa']\n",
    "rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "for state in states:\n",
    "    state_df=pd.DataFrame()\n",
    "    for year in range(2007,2013):\n",
    "        file=f'{state}_averaged_weather_{year}.csv'\n",
    "\n",
    "        filepath = os.path.join('/Users/ansonkong/Downloads/Data for nyu work/averaged_historical_weather', file)\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Concatenate this DataFrame with the accumulating state_df\n",
    "        state_df = pd.concat([state_df, df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Convert Time_UTC to datetime and extract Year, Month, and Day\n",
    "    state_df['Time_UTC'] = pd.to_datetime(state_df['Time_UTC'])\n",
    "    state_df['Month'] = state_df['Time_UTC'].dt.month\n",
    "    \n",
    "    # Initialize lists to store CI results\n",
    "    ci_results_max = []\n",
    "    ci_results_min = []\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        month_data = state_df[state_df['Month'] == month]\n",
    "        daily_max = month_data.groupby(month_data['Time_UTC'].dt.date)['T2'].max()\n",
    "        daily_min = month_data.groupby(month_data['Time_UTC'].dt.date)['T2'].min()\n",
    "        \n",
    "        ci_max_lower, ci_max_upper = calculate_quantile_ci(daily_max)\n",
    "        ci_min_lower, ci_min_upper = calculate_quantile_ci(daily_min)\n",
    "        \n",
    "        ci_results_max.append([ state, ci_max_lower, ci_max_upper])\n",
    "        ci_results_min.append([ state, ci_min_lower, ci_min_upper])\n",
    "    \n",
    "    # Convert CI results to DataFrames\n",
    "    ci_df_max = pd.DataFrame(ci_results_max, columns=['State',  'CI_max_lower', 'CI_max_upper'])\n",
    "    ci_df_min = pd.DataFrame(ci_results_min, columns=['State',  'CI_min_lower', 'CI_min_upper'])\n",
    "    # For ci_df_max, keep only the maximum upper CI for each rb\n",
    "    ci_df_max_reduced = ci_df_max.groupby('State')['CI_max_upper'].max().reset_index()\n",
    "\n",
    "    # For ci_df_min, keep only the minimum lower CI for each rb\n",
    "    ci_df_min_reduced = ci_df_min.groupby('State')['CI_min_lower'].min().reset_index()\n",
    "\n",
    "\n",
    "    # Save CI DataFrames to CSV files\n",
    "    ci_df_max_reduced.to_csv(os.path.join(ci_output_directory, f'{state}_CI_max.csv'), index=False)\n",
    "    ci_df_min_reduced.to_csv(os.path.join(ci_output_directory, f'{state}_CI_min.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # Directories initialization\n",
    "# directory = '/Users/ansonkong/Downloads/Data for nyu work/averaged_rcp85hotter_weather'\n",
    "# ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "# output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "# os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# states=['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "#            'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "#           'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "#           'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "#           'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming','usa']\n",
    "# years = range(2020, 2100)\n",
    "\n",
    "# extreme_outliers_max_list = []\n",
    "# extreme_outliers_min_list = []\n",
    "\n",
    "# for state in states:\n",
    "#     extreme_outliers_max_list = []\n",
    "#     extreme_outliers_min_list = []\n",
    "#     ci_max_path = os.path.join(ci_directory, f'{state}_CI_max.csv')\n",
    "#     ci_min_path = os.path.join(ci_directory, f'{state}_CI_min.csv')\n",
    "    \n",
    "#     if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "#         ci_max_df = pd.read_csv(ci_max_path)\n",
    "#         ci_min_df = pd.read_csv(ci_min_path)\n",
    "        \n",
    "#         for year in years:\n",
    "#             weather_file_path = os.path.join(directory, f'{state}_averaged_weather_{year}.csv')\n",
    "#             if os.path.exists(weather_file_path):\n",
    "#                 df = pd.read_csv(weather_file_path)\n",
    "#                 df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "#                 df['Month'] = pd.to_datetime(df['Time_UTC']).dt.month\n",
    "                    \n",
    "#                 # Only proceed if the month matches\n",
    "#                 ci_max_month = ci_max_df[ (ci_max_df['State'] == state)]\n",
    "#                 ci_min_month = ci_min_df[ (ci_min_df['State'] == state)]\n",
    "                    \n",
    "#                 # Find the dates with the highest and lowest T2 for each day\n",
    "#                 grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "#                 dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist()\n",
    "#                 # print(ci_max_df)\n",
    "#                 # print(grouped)\n",
    "#                 # print(grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist())\n",
    "#                 for date in dates_above_max_ci:\n",
    "#                     extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "#                 # Find all dates where min T2 is below the CI_min_lower\n",
    "#                 dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]].index.tolist()\n",
    "#                 for date in dates_below_min_ci:\n",
    "#                     extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "#     # Convert lists to DataFrames\n",
    "#     extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "#     extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "#     # Save the results\n",
    "#     extreme_outliers_max.to_csv(os.path.join(output_directory, f'{state}_extreme_outliers_max.csv'), index=False)\n",
    "#     extreme_outliers_min.to_csv(os.path.join(output_directory, f'{state}_extreme_outliers_min.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty file: p9_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p20_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p67_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Wyoming_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Wisconsin_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p50_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p17_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Illinois_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Minnesota_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p74_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p33_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p108_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p43_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p107_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p85_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p130_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p68_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p6_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Texas_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p18_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p23_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p64_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Kansas_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Utah_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p53_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p14_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p77_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p30_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p40_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p104_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p78_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Arkansas_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p133_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p5_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Colorado_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: New Jersey_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Connecticut_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Indiana_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p11_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p56_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p61_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p26_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p45_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p35_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p72_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p83_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: usa_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p59_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p112_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: South Dakota_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: North Dakota_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p29_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Maine_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p129_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p12_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p55_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p62_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p25_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p46_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p36_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p71_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p49_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p80_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Montana_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p39_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p126_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: New Hampshire_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p3_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Oklahoma_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p111_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Vermont_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p42_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p75_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p32_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p51_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p16_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p21_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p8_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p19_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p122_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p7_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p69_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p84_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p131_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Massachusetts_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p106_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p41_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p76_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p31_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Washington_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Arizona_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p52_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p15_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: California_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p22_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p65_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p119_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Pennsylvania_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: New Mexico_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p4_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p132_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Oregon_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p105_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p79_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p34_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p73_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Idaho_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p44_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p60_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p27_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p10_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p57_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p1_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p113_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p28_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Iowa_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p82_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p37_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p70_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p47_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p63_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p24_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Nebraska_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p128_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p13_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p54_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Ohio_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p2_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: New York_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p127_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p103_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p38_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Nevada_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Rhode Island_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p81_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p48_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p134_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Michigan_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: Missouri_extreme_outliers_max_projection.csv\n",
      "Skipping empty file: p88_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p65_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p119_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Pennsylvania_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p94_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p121_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p116_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p87_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p132_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Tennessee_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p105_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Mississippi_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p109_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p98_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p51_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p66_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p97_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p122_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p115_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p84_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p106_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Kentucky_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p47_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p128_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Ohio_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Alabama_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p110_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p92_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p103_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p48_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Michigan_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: North Carolina_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p60_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p57_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p113_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p91_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p58_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p124_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p100_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p64_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p118_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Delaware_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Florida_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p89_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p104_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Louisiana_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p86_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Arkansas_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p117_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p95_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p120_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p67_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p50_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p99_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: West Virginia_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Illinois_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p108_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p107_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p85_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Maryland_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Texas_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p114_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: South Carolina_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p96_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p123_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p55_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p62_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p80_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p102_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p93_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p126_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p111_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Oklahoma_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Indiana_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: New Jersey_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Connecticut_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p61_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Georgia_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p101_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p90_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p125_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: Virginia_extreme_outliers_min_projection.csv\n",
      "Skipping empty file: p112_extreme_outliers_min_projection.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "cases = ['projection']#'rcp85hotter', 'rcp45hotter', 'rcp85cooler', 'rcp45cooler',\n",
    "for case in cases:#, 'rcp45hotter', 'rcp45cooler']\n",
    "    # Directories initialization\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/output/future_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "  \n",
    "    for rb_code in rb_codes:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "        hdd_list = []\n",
    "        cdd_list = []\n",
    "        ci_max_path = os.path.join(ci_directory, f'{rb_code}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{rb_code}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "            \n",
    "            for year in years:\n",
    "                weather_file_path = os.path.join(directory, f'{rb_code}_WRF_Hourly_Mean_Meteorology_{year}.csv')\n",
    "                if os.path.exists(weather_file_path):\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    df['Month'] = pd.to_datetime(df['Time_UTC']).dt.month\n",
    "                        \n",
    "                    # Only proceed if the month matches\n",
    "                    ci_max_month = ci_max_df[ (ci_max_df['rb'] == rb_code)]\n",
    "                    ci_min_month = ci_min_df[ (ci_min_df['rb'] == rb_code)]\n",
    "                        \n",
    "                    # Find the dates with the highest and lowest T2 for each day\n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist()\n",
    "                    # print(ci_max_df)\n",
    "                    # print(grouped)\n",
    "                    # print(grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist())\n",
    "                    for date in dates_above_max_ci:\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "                    # Find all dates where min T2 is below the CI_min_lower\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]].index.tolist()\n",
    "                    for date in dates_below_min_ci:\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_outliers_max_{case}.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_outliers_min_{case}.csv'), index=False)\n",
    "        for year in years:\n",
    "            weather_file_path = os.path.join(directory, f'{rb_code}_WRF_Hourly_Mean_Meteorology_{year}.csv')\n",
    "            hdd=0\n",
    "            cdd=0\n",
    "            if os.path.exists(weather_file_path):\n",
    "                df = pd.read_csv(weather_file_path)\n",
    "                df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                        \n",
    "   \n",
    "                # Find the dates with the highest and lowest T2 for each day\n",
    "                grouped = df.groupby('Date')['T2'].agg(['mean'])\n",
    "                dates_above_hdd = grouped[grouped['mean'] > 291.45]\n",
    "                for index, row in dates_above_hdd.iterrows():\n",
    "                    hdd+=row['mean']-291.45\n",
    "                if not hdd:\n",
    "                    hdd_list.append({'rb': rb_code, 'Year': year, 'hdd': 0.0})\n",
    "                else:\n",
    "                    hdd_list.append({'rb': rb_code, 'Year': year, 'hdd': hdd})\n",
    "\n",
    "                # Find all dates where min T2 is below the CI_min_lower\n",
    "                dates_below_cdd = grouped[grouped['mean'] < 291.45]\n",
    "                for index, row in dates_below_cdd.iterrows():\n",
    "                    cdd+=291.45-row['mean']\n",
    "                cdd_list.append({'rb': rb_code, 'Year': year, 'cdd': cdd})\n",
    "         # Convert lists to DataFrames\n",
    "        hdd_list = pd.DataFrame(hdd_list)\n",
    "        cdd_list = pd.DataFrame(cdd_list)\n",
    "\n",
    "        # Save the results\n",
    "        hdd_list.to_csv(os.path.join(output_directory, f'{rb_code}_hdd_{case}.csv'), index=False)\n",
    "        cdd_list.to_csv(os.path.join(output_directory, f'{rb_code}_cdd_{case}.csv'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Directories initialization\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/averaged_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    states=['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "            'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "            'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "            'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "            'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming','usa']\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    extreme_outliers_max_list = []\n",
    "    extreme_outliers_min_list = []\n",
    "\n",
    "    for state in states:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "        hdd_list = []\n",
    "        cdd_list = []\n",
    "        ci_max_path = os.path.join(ci_directory, f'{state}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{state}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "            \n",
    "            for year in years:\n",
    "                weather_file_path = os.path.join(directory, f'{state}_averaged_weather_{year}.csv')\n",
    "                if os.path.exists(weather_file_path):\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    df['Month'] = pd.to_datetime(df['Time_UTC']).dt.month\n",
    "                        \n",
    "                    # Only proceed if the month matches\n",
    "                    ci_max_month = ci_max_df[ (ci_max_df['State'] == state)]\n",
    "                    ci_min_month = ci_min_df[ (ci_min_df['State'] == state)]\n",
    "                        \n",
    "                    # Find the dates with the highest and lowest T2 for each day\n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist()\n",
    "                    # print(ci_max_df)\n",
    "                    # print(grouped)\n",
    "                    # print(grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist())\n",
    "                    for date in dates_above_max_ci:\n",
    "                        extreme_outliers_max_list.append({'state': state, 'Year': year, 'Date': date})\n",
    "\n",
    "                    # Find all dates where min T2 is below the CI_min_lower\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]].index.tolist()\n",
    "                    for date in dates_below_min_ci:\n",
    "                        extreme_outliers_min_list.append({'state': state, 'Year': year, 'Date': date})\n",
    "        for year in years:\n",
    "            weather_file_path = os.path.join(directory, f'{state}_averaged_weather_{year}.csv')\n",
    "            hdd=0\n",
    "            cdd=0\n",
    "\n",
    "            if os.path.exists(weather_file_path):\n",
    "                df = pd.read_csv(weather_file_path)\n",
    "                df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "            \n",
    "                # Find the dates with the highest and lowest T2 for each day\n",
    "                grouped = df.groupby('Date')['T2'].agg(['mean'])\n",
    "                dates_above_hdd = grouped[grouped['mean'] > 291.45]\n",
    "                for index, row in dates_above_hdd.iterrows():\n",
    "                    hdd+=row['mean']-291.45\n",
    "                hdd_list.append({'state': state, 'Year': year, 'hdd': hdd})\n",
    "\n",
    "                # Find all dates where min T2 is below the CI_min_lower\n",
    "                dates_below_cdd = grouped[grouped['mean'] < 291.45]\n",
    "                for index, row in dates_below_cdd.iterrows():\n",
    "                    cdd+=291.45-row['mean']\n",
    "                cdd_list.append({'state': state, 'Year': year, 'cdd': cdd})\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{state}_extreme_outliers_max_{case}.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{state}_extreme_outliers_min_{case}.csv'), index=False)\n",
    "\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        hdd_list = pd.DataFrame(hdd_list)\n",
    "        cdd_list = pd.DataFrame(cdd_list)\n",
    "\n",
    "        # Save the results\n",
    "        hdd_list.to_csv(os.path.join(output_directory, f'{state}_hdd_{case}.csv'), index=False)\n",
    "        cdd_list.to_csv(os.path.join(output_directory, f'{state}_cdd_{case}.csv'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Specify the folder\n",
    "    folder = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results\n",
    "    all_results_df = pd.DataFrame()\n",
    "\n",
    "    # List all \"max\" files in the folder\n",
    "    max_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_outliers_max_{case}.csv')]\n",
    "\n",
    "    for file in max_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_outliers_max_{case}.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        # Group by 'Year' and count the number of observations for each year\n",
    "        year_counts = df.groupby('Year').size().reset_index(name='number_of_days')\n",
    "        \n",
    "        # Add the region to the DataFrame\n",
    "        year_counts['region'] = region\n",
    "        \n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, year_counts], ignore_index=True)\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'number_of_days']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"max\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_max_outliers_summary_{case}.csv', index=False)\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results for minimum outliers\n",
    "    all_min_results_df = pd.DataFrame()\n",
    "\n",
    "    # List all \"min\" files in the folder\n",
    "    min_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_outliers_min_{case}.csv')]\n",
    "\n",
    "    for file in min_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_outliers_min_{case}.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        \n",
    "        # Group by 'Year' and count the number of observations for each year\n",
    "        year_counts = df.groupby('Year').size().reset_index(name='number_of_days')\n",
    "        \n",
    "        # Add the region to the DataFrame\n",
    "        year_counts['region'] = region\n",
    "        \n",
    "        # Append the result to the all_min_results_df DataFrame\n",
    "        all_min_results_df = pd.concat([all_min_results_df, year_counts], ignore_index=True)\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_min_results_df = all_min_results_df[['region', 'Year', 'number_of_days']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"min\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_min_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_min_outliers_summary_{case}.csv', index=False)\n",
    "\n",
    "    # List all \"hdd\" files in the folder\n",
    "    hdd_files = [f for f in os.listdir(folder) if f.endswith(f'_hdd_{case}.csv')]\n",
    "\n",
    "    for file in hdd_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_hdd_{case}.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        \n",
    "        # Add the region to the DataFrame\n",
    "        df['region'] = region\n",
    "\n",
    "        df=df[['Year','region','hdd']]\n",
    "        \n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, df], ignore_index=True)\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'hdd']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"max\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_hdd_{case}.csv', index=False)\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results for minimum outliers\n",
    "    all_min_results_df = pd.DataFrame()\n",
    "\n",
    "    # List all \"min\" files in the folder\n",
    "    cdd_files = [f for f in os.listdir(folder) if f.endswith(f'_cdd_{case}.csv')]\n",
    "\n",
    "    for file in cdd_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_cdd_{case}.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        \n",
    "        # Add the region to the DataFrame\n",
    "        df['region'] = region\n",
    "\n",
    "        df=df[['Year','region','cdd']]\n",
    "        \n",
    "        # Append the result to the all_min_results_df DataFrame\n",
    "        all_min_results_df = pd.concat([all_min_results_df, df], ignore_index=True)\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_min_results_df = all_min_results_df[['region', 'Year', 'cdd']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"min\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_min_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_cdd_{case}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average demand on extreme day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/838353845.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty file: p103_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Utah_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p76_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p4_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p31_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Arkansas_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p52_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p127_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p15_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p39_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Missouri_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p130_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p69_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p45_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p84_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p61_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p26_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p47_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p132_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p24_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Nevada_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Wyoming_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p63_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p33_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p6_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Wisconsin_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p74_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Oklahoma_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p17_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: New York_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p50_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p77_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p30_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p5_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p126_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p53_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p38_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p14_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p44_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p68_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p131_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Indiana_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: New Mexico_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p85_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p60_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p27_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p133_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Kansas_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p46_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p25_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Oregon_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: New Hampshire_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p62_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Texas_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p7_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p32_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p75_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p59_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p16_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p51_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: South Dakota_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p108_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Arizona_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: usa_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p23_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p111_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p64_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p48_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Rhode Island_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p81_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p40_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Colorado_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p119_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p9_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Massachusetts_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: California_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p10_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p122_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p57_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p18_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p34_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Connecticut_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p1_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Iowa_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p73_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p106_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p55_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p79_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Ohio_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Illinois_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p12_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Maine_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p104_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: North Dakota_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p128_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p71_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p3_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p36_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Idaho_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Nebraska_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p113_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p21_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p42_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p83_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p29_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Montana_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p22_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Pennsylvania_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p49_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p65_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Vermont_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p80_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p134_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p41_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Washington_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p11_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p8_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p56_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: New Jersey_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p35_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p19_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p107_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p72_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p78_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p54_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p13_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p70_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p129_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p105_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Michigan_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p37_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p2_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p112_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p67_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: Minnesota_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p20_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p43_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p28_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p82_extreme_demand_outliers_max_projection.csv\n",
      "Skipping empty file: p120_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p55_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2088    97769.810000\n",
      "2089    91639.470000\n",
      "2090    89912.990000\n",
      "2091    90540.786667\n",
      "2092    89694.658387\n",
      "2093    90190.776216\n",
      "2094    90152.005385\n",
      "2095    90899.437778\n",
      "2096    90611.056809\n",
      "2097    90861.483913\n",
      "2098    91379.442692\n",
      "2099    91361.997547\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2051    43022.370000\n",
      "2053    42885.186000\n",
      "2054    43406.331500\n",
      "2055    43828.470909\n",
      "2056    43955.888919\n",
      "2057    44248.391860\n",
      "2058    44391.180800\n",
      "2059    44549.110164\n",
      "2060    44795.407206\n",
      "2061    45105.036986\n",
      "2062    45269.013976\n",
      "2063    45663.603735\n",
      "2064    45825.139130\n",
      "2065    45982.950404\n",
      "2066    46229.484200\n",
      "2067    46459.938491\n",
      "2068    46794.417748\n",
      "2069    47044.495877\n",
      "2070    47333.654310\n",
      "2071    47530.852623\n",
      "2072    47801.211520\n",
      "2073    48048.498268\n",
      "2074    48356.322769\n",
      "2075    48553.364179\n",
      "2076    48837.865362\n",
      "2077    49026.421214\n",
      "2078    49298.280417\n",
      "2079    49428.836667\n",
      "2080    49796.130526\n",
      "2081    50106.866579\n",
      "2082    50210.653500\n",
      "2083    50511.949562\n",
      "2084    50811.032561\n",
      "2085    50995.250893\n",
      "2086    51179.857299\n",
      "2087    51513.328908\n",
      "2088    51769.140904\n",
      "2089    51864.301196\n",
      "2090    52111.175187\n",
      "2091    52341.953455\n",
      "2092    52566.822487\n",
      "2093    52806.204724\n",
      "2094    52967.793645\n",
      "2095    53236.324078\n",
      "2096    53441.830519\n",
      "2097    53684.483411\n",
      "2098    53862.552694\n",
      "2099    53924.291542\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p94_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Ohio_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Tennessee_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Illinois_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2030    34621.266667\n",
      "2031    34677.876522\n",
      "2032    34700.991000\n",
      "2033    34924.806275\n",
      "2034    34879.232400\n",
      "            ...     \n",
      "2095    55986.448493\n",
      "2096    56536.713415\n",
      "2097    57055.495644\n",
      "2098    57594.801315\n",
      "2099    58129.306493\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Year\n",
      "2054    4644.252000\n",
      "2055    4411.056667\n",
      "2056    4641.737037\n",
      "2057    4371.999189\n",
      "2058    4567.867308\n",
      "2059    4613.493750\n",
      "2060    4575.182154\n",
      "2061    4575.139688\n",
      "2062    4685.149589\n",
      "2063    4687.084000\n",
      "2064    4725.391341\n",
      "2065    4779.136163\n",
      "2066    4763.104545\n",
      "2067    4751.331596\n",
      "2068    4763.621224\n",
      "2069    4722.538889\n",
      "2070    4726.856117\n",
      "2071    4711.853905\n",
      "2072    4711.267103\n",
      "2073    4695.731696\n",
      "2074    4670.922589\n",
      "2075    4681.667009\n",
      "2076    4648.205126\n",
      "2077    4655.352377\n",
      "2078    4625.195039\n",
      "2079    4589.612923\n",
      "2080    4609.988731\n",
      "2081    4562.476058\n",
      "2082    4574.857429\n",
      "2083    4493.481538\n",
      "2084    4525.530272\n",
      "2085    4467.795097\n",
      "2086    4447.163704\n",
      "2087    4405.100417\n",
      "2088    4371.774104\n",
      "2089    4363.096497\n",
      "2090    4348.234341\n",
      "2091    4322.540543\n",
      "2092    4331.622340\n",
      "2093    4337.655820\n",
      "2094    4284.435590\n",
      "2095    4294.067347\n",
      "2096    4269.674179\n",
      "2097    4262.189257\n",
      "2098    4236.091683\n",
      "2099    4228.107962\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Virginia_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p104_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2038    38140.905227\n",
      "2039    37830.449385\n",
      "2040    37895.733662\n",
      "2041    37888.737975\n",
      "2042    37885.931786\n",
      "            ...     \n",
      "2095    35782.958904\n",
      "2096    35782.298552\n",
      "2097    35782.442575\n",
      "2098    35779.138740\n",
      "2099    35771.001397\n",
      "Name: Load_sum, Length: 62, dtype: float64\n",
      "Skipping empty file: p128_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2036    9266.276579\n",
      "2037    9103.310145\n",
      "2038    9065.924286\n",
      "2039    9034.926905\n",
      "2040    9044.436111\n",
      "           ...     \n",
      "2095    9210.548164\n",
      "2096    9216.846667\n",
      "2097    9237.421726\n",
      "2098    9249.527178\n",
      "2099    9257.547699\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Year\n",
      "2044    46510.266000\n",
      "2045    46231.223571\n",
      "2046    46394.127073\n",
      "2047    46585.070189\n",
      "2048    46858.258571\n",
      "2049    46441.450649\n",
      "2050    46334.161111\n",
      "2051    46543.854286\n",
      "2052    46940.574158\n",
      "2053    47146.158037\n",
      "2054    47451.476909\n",
      "2055    47598.928609\n",
      "2056    47843.240661\n",
      "2057    48117.950650\n",
      "2058    48372.251953\n",
      "2059    48672.139695\n",
      "2060    48894.397664\n",
      "2061    49081.494397\n",
      "2062    49245.324422\n",
      "2063    49250.176962\n",
      "2064    49489.534303\n",
      "2065    49684.054438\n",
      "2066    49772.601591\n",
      "2067    49958.280601\n",
      "2068    50146.615208\n",
      "2069    50456.983299\n",
      "2070    50742.642050\n",
      "2071    50993.992780\n",
      "2072    51251.733033\n",
      "2073    51531.347243\n",
      "2074    51798.644658\n",
      "2075    52108.537500\n",
      "2076    52559.475177\n",
      "2077    52695.245411\n",
      "2078    52970.277574\n",
      "2079    53256.161004\n",
      "2080    53624.904426\n",
      "2081    53617.319048\n",
      "2082    53255.421771\n",
      "2083    53619.687839\n",
      "2084    53764.468163\n",
      "2085    54066.995610\n",
      "2086    54389.533608\n",
      "2087    54722.104966\n",
      "2088    54971.089703\n",
      "2089    55202.788958\n",
      "2090    55626.060227\n",
      "2091    55902.853524\n",
      "2092    56229.220435\n",
      "2093    56587.388266\n",
      "2094    56464.362819\n",
      "2095    56334.458697\n",
      "2096    56453.125233\n",
      "2097    56933.483945\n",
      "2098    57457.352356\n",
      "2099    57977.225041\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p113_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p66_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2036    94514.220000\n",
      "2037    82021.181111\n",
      "2038    78455.023750\n",
      "2039    77105.802698\n",
      "2040    76740.438286\n",
      "            ...     \n",
      "2095    37184.828411\n",
      "2096    36813.209809\n",
      "2097    36459.992219\n",
      "2098    36078.269479\n",
      "2099    35680.798521\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Year\n",
      "2036     79451.238000\n",
      "2037     79489.587241\n",
      "2038     79818.174750\n",
      "2039     80351.484800\n",
      "2040     80953.199333\n",
      "            ...      \n",
      "2095    114994.750411\n",
      "2096    116290.129863\n",
      "2097    117504.037205\n",
      "2098    118762.203041\n",
      "2099    120013.714274\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Year\n",
      "2034    28021.766923\n",
      "2035    27285.585938\n",
      "2036    27377.493023\n",
      "2037    26926.580000\n",
      "2038    26762.136023\n",
      "            ...     \n",
      "2095    35760.717808\n",
      "2096    36050.082896\n",
      "2097    36336.986658\n",
      "2098    36620.529753\n",
      "2099    36895.206767\n",
      "Name: Load_sum, Length: 66, dtype: float64\n",
      "Year\n",
      "2054    14266.933636\n",
      "2055    14070.412800\n",
      "2056    13871.492000\n",
      "2057    13795.267170\n",
      "2058    13794.971864\n",
      "2059    13795.106984\n",
      "2060    13783.462121\n",
      "2061    13777.068750\n",
      "2062    13771.806800\n",
      "2063    13775.584500\n",
      "2064    13799.677831\n",
      "2065    13775.361059\n",
      "2066    13771.383523\n",
      "2067    13795.506778\n",
      "2068    13787.259239\n",
      "2069    13794.029787\n",
      "2070    13797.256804\n",
      "2071    13802.124184\n",
      "2072    13787.691188\n",
      "2073    13795.259515\n",
      "2074    13798.724327\n",
      "2075    13783.119813\n",
      "2076    13786.580090\n",
      "2077    13795.209910\n",
      "2078    13771.506609\n",
      "2079    13778.066496\n",
      "2080    13791.166303\n",
      "2081    13790.527983\n",
      "2082    13779.950887\n",
      "2083    13740.317769\n",
      "2084    13754.210682\n",
      "2085    13733.083926\n",
      "2086    13723.491151\n",
      "2087    13716.045775\n",
      "2088    13684.020811\n",
      "2089    13676.319404\n",
      "2090    13685.243922\n",
      "2091    13683.849675\n",
      "2092    13692.144304\n",
      "2093    13689.774000\n",
      "2094    13684.210864\n",
      "2095    13651.617857\n",
      "2096    13652.259593\n",
      "2097    13617.896857\n",
      "2098    13650.245886\n",
      "2099    13612.498000\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032    2649.888000\n",
      "2033    2737.650000\n",
      "2034    2947.658276\n",
      "2035    3010.526316\n",
      "2036    3066.064510\n",
      "           ...     \n",
      "2095    5577.698877\n",
      "2096    5679.514536\n",
      "2097    5765.644438\n",
      "2098    5862.285699\n",
      "2099    5960.241808\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2040    49055.347857\n",
      "2041    48961.666190\n",
      "2042    48332.757442\n",
      "2043    47942.772500\n",
      "2044    48123.855435\n",
      "2045    48245.643163\n",
      "2046    48374.182212\n",
      "2047    48533.402703\n",
      "2048    48714.662672\n",
      "2049    48872.684034\n",
      "2050    48933.752143\n",
      "2051    49077.835385\n",
      "2052    49253.404710\n",
      "2053    48858.476624\n",
      "2054    48910.817317\n",
      "2055    49035.251420\n",
      "2056    49107.540674\n",
      "2057    49207.864891\n",
      "2058    49256.162732\n",
      "2059    49341.130198\n",
      "2060    49530.059665\n",
      "2061    49586.067814\n",
      "2062    49724.040045\n",
      "2063    49755.130395\n",
      "2064    49892.708426\n",
      "2065    50048.877908\n",
      "2066    50146.938816\n",
      "2067    50173.662569\n",
      "2068    50135.391547\n",
      "2069    50218.491513\n",
      "2070    50239.384714\n",
      "2071    50282.936414\n",
      "2072    50501.992331\n",
      "2073    50590.173234\n",
      "2074    50751.932621\n",
      "2075    50894.020127\n",
      "2076    51178.486293\n",
      "2077    50933.684018\n",
      "2078    50420.678182\n",
      "2079    50741.290795\n",
      "2080    51172.784208\n",
      "2081    51535.058055\n",
      "2082    51924.354329\n",
      "2083    52301.530438\n",
      "2084    52740.222377\n",
      "2085    53122.511507\n",
      "2086    53536.176575\n",
      "2087    53950.353452\n",
      "2088    54364.362213\n",
      "2089    54758.278438\n",
      "2090    55178.717151\n",
      "2091    55604.663233\n",
      "2092    56066.963279\n",
      "2093    56458.545507\n",
      "2094    56862.825534\n",
      "2095    57299.234932\n",
      "2096    57764.244754\n",
      "2097    58184.237123\n",
      "2098    58622.582986\n",
      "2099    59056.316493\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2093    21189.935000\n",
      "2094    20187.478571\n",
      "2095    19267.549000\n",
      "2096    19616.742778\n",
      "2097    18819.882400\n",
      "2098    18789.680000\n",
      "2099    18549.337000\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2093    1.120990e+07\n",
      "2094    1.161295e+07\n",
      "2095    1.182638e+07\n",
      "2096    1.186732e+07\n",
      "2097    1.188310e+07\n",
      "2098    1.193353e+07\n",
      "2099    1.204199e+07\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032    3341.600000\n",
      "2033    3254.989091\n",
      "2034    3600.412692\n",
      "2035    3859.320000\n",
      "2036    3891.600952\n",
      "           ...     \n",
      "2095    4677.306767\n",
      "2096    4717.133060\n",
      "2097    4749.452164\n",
      "2098    4783.425890\n",
      "2099    4816.301014\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2032    200297.671000\n",
      "2033    201445.076875\n",
      "2034    201634.408387\n",
      "2035    201816.699773\n",
      "2036    202995.652742\n",
      "            ...      \n",
      "2095    371533.785425\n",
      "2096    377383.172787\n",
      "2097    382713.564274\n",
      "2098    388281.015507\n",
      "2099    393727.911616\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p89_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p64_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p48_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p111_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Mississippi_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2077    45467.843333\n",
      "2079    44493.623000\n",
      "2080    42889.190000\n",
      "2081    43367.623462\n",
      "2082    42361.313939\n",
      "2083    41876.695143\n",
      "2084    41576.518293\n",
      "2085    42694.034681\n",
      "2086    41807.617692\n",
      "2087    42323.108621\n",
      "2088    41938.807213\n",
      "2089    43044.385574\n",
      "2090    42725.703906\n",
      "2091    42767.040000\n",
      "2092    43330.558873\n",
      "2093    43768.492267\n",
      "2094    42628.251299\n",
      "2095    43932.827073\n",
      "2096    44145.462299\n",
      "2097    43944.866591\n",
      "2098    44296.177444\n",
      "2099    43804.565667\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2083    26132.115000\n",
      "2085    25671.510000\n",
      "2086    26197.065000\n",
      "2087    25893.902500\n",
      "2088    26146.382308\n",
      "2089    26459.905556\n",
      "2090    26438.548065\n",
      "2091    26613.962059\n",
      "2092    26539.302500\n",
      "2093    26704.824103\n",
      "2094    26745.468000\n",
      "2095    26795.142979\n",
      "2096    26886.530556\n",
      "2097    26944.640500\n",
      "2098    27046.211228\n",
      "2099    27137.089138\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p119_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2035    32563.453333\n",
      "2036    30937.894444\n",
      "2037    29968.768261\n",
      "2038    28970.070282\n",
      "2039    29027.254474\n",
      "            ...     \n",
      "2095    16177.295205\n",
      "2096    16178.716995\n",
      "2097    16172.320466\n",
      "2098    16159.426877\n",
      "2099    16135.094630\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2041    178739.818333\n",
      "2042    176930.978333\n",
      "2043    175511.974000\n",
      "2044    175378.272500\n",
      "2045    176154.268043\n",
      "2046    174914.986538\n",
      "2047    175585.889886\n",
      "2048    176658.929579\n",
      "2049    177321.430700\n",
      "2050    178683.905524\n",
      "2051    179655.342500\n",
      "2052    180927.849231\n",
      "2053    181830.458333\n",
      "2054    183134.178065\n",
      "2055    183753.085639\n",
      "2056    184401.315616\n",
      "2057    184935.884969\n",
      "2058    185257.573239\n",
      "2059    186393.938516\n",
      "2060    187572.464439\n",
      "2061    188531.257461\n",
      "2062    189639.323450\n",
      "2063    190704.273107\n",
      "2064    191919.158592\n",
      "2065    192836.209543\n",
      "2066    193691.606388\n",
      "2067    194853.854936\n",
      "2068    196104.743333\n",
      "2069    197366.268000\n",
      "2070    198698.011235\n",
      "2071    199886.254556\n",
      "2072    200775.161790\n",
      "2073    202250.286109\n",
      "2074    203266.439962\n",
      "2075    204317.463407\n",
      "2076    205381.569783\n",
      "2077    206259.526972\n",
      "2078    206724.233919\n",
      "2079    207512.230557\n",
      "2080    207754.286471\n",
      "2081    208054.891982\n",
      "2082    208241.570392\n",
      "2083    209156.582438\n",
      "2084    210940.710273\n",
      "2085    212621.719014\n",
      "2086    214386.830685\n",
      "2087    216138.861589\n",
      "2088    217866.432787\n",
      "2089    219601.696274\n",
      "2090    221328.322548\n",
      "2091    223132.343699\n",
      "2092    225010.938361\n",
      "2093    226720.892822\n",
      "2094    228443.403918\n",
      "2095    230285.351397\n",
      "2096    232160.289863\n",
      "2097    233934.488959\n",
      "2098    235756.596740\n",
      "2099    237577.860247\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2077    196859.495000\n",
      "2078    206302.385000\n",
      "2079    208002.610000\n",
      "2080    200243.838000\n",
      "2081    201689.325556\n",
      "2082    202396.888182\n",
      "2083    202850.811613\n",
      "2084    204240.353714\n",
      "2085    204638.606098\n",
      "2086    204825.707609\n",
      "2087    205218.938800\n",
      "2088    206081.053273\n",
      "2089    206849.901053\n",
      "2090    207481.721695\n",
      "2091    208296.548030\n",
      "2092    209258.780746\n",
      "2093    209531.226620\n",
      "2094    209814.711688\n",
      "2095    210693.162593\n",
      "2096    211537.590843\n",
      "2097    212750.596625\n",
      "2098    212639.602527\n",
      "2099    213603.932088\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Kentucky_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2032    7.361502e+05\n",
      "2033    7.200812e+05\n",
      "2034    7.257929e+05\n",
      "2035    7.294256e+05\n",
      "2036    7.303431e+05\n",
      "            ...     \n",
      "2095    1.249659e+06\n",
      "2096    1.264401e+06\n",
      "2097    1.278013e+06\n",
      "2098    1.292443e+06\n",
      "2099    1.306931e+06\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2030    348197.878750\n",
      "2031    352426.179500\n",
      "2032    350532.169677\n",
      "2033    348948.384583\n",
      "2034    349553.791237\n",
      "            ...      \n",
      "2095    778871.644274\n",
      "2096    789910.120492\n",
      "2097    800014.427808\n",
      "2098    810811.159918\n",
      "2099    821654.197507\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Year\n",
      "2049    321780.631250\n",
      "2050    323354.870714\n",
      "2051    326556.963636\n",
      "2052    329306.236970\n",
      "2053    329161.985263\n",
      "2054    330854.826977\n",
      "2055    329530.912857\n",
      "2056    329334.708824\n",
      "2057    329132.102396\n",
      "2058    329862.674851\n",
      "2059    330921.891442\n",
      "2060    330962.709279\n",
      "2061    331857.721947\n",
      "2062    332489.650342\n",
      "2063    333201.339680\n",
      "2064    332706.332158\n",
      "2065    332309.352733\n",
      "2066    332582.547628\n",
      "2067    333771.395660\n",
      "2068    335166.818171\n",
      "2069    336596.769030\n",
      "2070    337590.354881\n",
      "2071    339053.684824\n",
      "2072    340310.427184\n",
      "2073    341393.709602\n",
      "2074    342485.944451\n",
      "2075    343895.792595\n",
      "2076    344996.521710\n",
      "2077    344185.380385\n",
      "2078    344881.760046\n",
      "2079    346218.825157\n",
      "2080    347818.830881\n",
      "2081    349255.027792\n",
      "2082    350615.152468\n",
      "2083    351896.287344\n",
      "2084    353110.781084\n",
      "2085    354684.503676\n",
      "2086    356784.026667\n",
      "2087    358494.929806\n",
      "2088    360143.731069\n",
      "2089    361859.952679\n",
      "2090    363675.011269\n",
      "2091    365350.314579\n",
      "2092    366952.444000\n",
      "2093    368483.728768\n",
      "2094    369726.025103\n",
      "2095    370277.126403\n",
      "2096    368892.993533\n",
      "2097    368696.055690\n",
      "2098    369907.571781\n",
      "2099    372294.809370\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p96_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Louisiana_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p57_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p122_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2079    200240.723333\n",
      "2080    198290.293333\n",
      "2081    198400.694000\n",
      "2082    199157.330625\n",
      "2083    196106.125217\n",
      "2084    197060.846552\n",
      "2085    197857.208125\n",
      "2086    196717.574324\n",
      "2087    196152.303488\n",
      "2088    194372.558393\n",
      "2089    193547.378088\n",
      "2090    192498.797500\n",
      "2091    192489.044118\n",
      "2092    191974.083137\n",
      "2093    192554.943069\n",
      "2094    192491.486542\n",
      "2095    192817.422523\n",
      "2096    193957.544513\n",
      "2097    194212.593565\n",
      "2098    194301.521901\n",
      "2099    194866.181803\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Connecticut_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2051    118258.128333\n",
      "2052    117580.808929\n",
      "2053    117932.439348\n",
      "2054    118042.341636\n",
      "2055    117164.606667\n",
      "2056    118687.128806\n",
      "2057    118791.479857\n",
      "2058    119772.420506\n",
      "2059    119765.792561\n",
      "2060    119205.986235\n",
      "2061    119651.684176\n",
      "2062    119157.275870\n",
      "2063    119314.232755\n",
      "2064    118972.657087\n",
      "2065    118824.694078\n",
      "2066    117932.396204\n",
      "2067    117865.188273\n",
      "2068    117853.112589\n",
      "2069    117491.197739\n",
      "2070    117307.312881\n",
      "2071    116972.346583\n",
      "2072    116558.661905\n",
      "2073    116210.546692\n",
      "2074    116032.613881\n",
      "2075    115547.992826\n",
      "2076    115321.901986\n",
      "2077    114602.115473\n",
      "2078    114285.529610\n",
      "2079    113495.109693\n",
      "2080    113464.461446\n",
      "2081    113028.938721\n",
      "2082    112571.427865\n",
      "2083    111959.432198\n",
      "2084    112084.233892\n",
      "2085    111707.245474\n",
      "2086    111351.112010\n",
      "2087    111029.359397\n",
      "2088    110618.010594\n",
      "2089    110337.827404\n",
      "2090    110080.308585\n",
      "2091    109868.210694\n",
      "2092    109695.378378\n",
      "2093    109468.639556\n",
      "2094    108955.326438\n",
      "2095    108854.292669\n",
      "2096    108701.960451\n",
      "2097    108626.467570\n",
      "2098    108369.662016\n",
      "2099    108333.003359\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2040    13689.637143\n",
      "2041    13779.159500\n",
      "2042    13827.312979\n",
      "2043    13878.335132\n",
      "2044    13932.356129\n",
      "2045    13982.256735\n",
      "2046    14017.069907\n",
      "2047    14074.625044\n",
      "2048    14143.166050\n",
      "2049    14189.354274\n",
      "2050    14267.048281\n",
      "2051    14292.908478\n",
      "2052    14263.945390\n",
      "2053    14286.246522\n",
      "2054    14309.747396\n",
      "2055    14323.134551\n",
      "2056    14373.027647\n",
      "2057    14395.493436\n",
      "2058    14407.280874\n",
      "2059    14449.384434\n",
      "2060    14511.104429\n",
      "2061    14541.446622\n",
      "2062    14595.662130\n",
      "2063    14607.779833\n",
      "2064    14643.226452\n",
      "2065    14711.084661\n",
      "2066    14666.530379\n",
      "2067    14700.783750\n",
      "2068    14721.675371\n",
      "2069    14800.693462\n",
      "2070    14825.392432\n",
      "2071    14889.918013\n",
      "2072    14944.681258\n",
      "2073    14973.656625\n",
      "2074    15018.987969\n",
      "2075    14895.762989\n",
      "2076    14859.099973\n",
      "2077    14980.464986\n",
      "2078    15121.907178\n",
      "2079    15261.527178\n",
      "2080    15416.151448\n",
      "2081    15547.042795\n",
      "2082    15688.849699\n",
      "2083    15827.643014\n",
      "2084    15985.120874\n",
      "2085    16119.459479\n",
      "2086    16267.728767\n",
      "2087    16416.006329\n",
      "2088    16565.034235\n",
      "2089    16705.371370\n",
      "2090    16852.767507\n",
      "2091    17004.877151\n",
      "2092    17167.372295\n",
      "2093    17304.965205\n",
      "2094    17449.399342\n",
      "2095    17603.728959\n",
      "2096    17764.440902\n",
      "2097    17909.814904\n",
      "2098    18063.073890\n",
      "2099    18213.976384\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2044    47257.355000\n",
      "2045    46255.789333\n",
      "2046    46642.099130\n",
      "2047    46611.001290\n",
      "2048    46740.080476\n",
      "2049    46682.162586\n",
      "2050    46818.273165\n",
      "2051    47056.130345\n",
      "2052    47161.705158\n",
      "2053    47337.054343\n",
      "2054    47515.463137\n",
      "2055    47654.733738\n",
      "2056    47794.634174\n",
      "2057    48006.743932\n",
      "2058    48175.835932\n",
      "2059    48318.454960\n",
      "2060    48381.860000\n",
      "2061    48536.780288\n",
      "2062    48638.452245\n",
      "2063    48629.659815\n",
      "2064    48787.999353\n",
      "2065    48884.862260\n",
      "2066    48994.319022\n",
      "2067    49232.259574\n",
      "2068    49436.185573\n",
      "2069    49539.654050\n",
      "2070    49690.848689\n",
      "2071    49888.158619\n",
      "2072    50005.172237\n",
      "2073    50235.946227\n",
      "2074    50414.660622\n",
      "2075    50595.175913\n",
      "2076    50842.408426\n",
      "2077    50943.436417\n",
      "2078    51152.108735\n",
      "2079    51363.391377\n",
      "2080    51633.845339\n",
      "2081    51823.727283\n",
      "2082    52011.685251\n",
      "2083    52191.669924\n",
      "2084    52444.376292\n",
      "2085    52491.455942\n",
      "2086    52672.863701\n",
      "2087    52780.524118\n",
      "2088    52865.718060\n",
      "2089    52997.774197\n",
      "2090    52838.709415\n",
      "2091    52882.932285\n",
      "2092    52839.124129\n",
      "2093    52932.435769\n",
      "2094    53200.571041\n",
      "2095    53527.114822\n",
      "2096    53860.022377\n",
      "2097    54173.691534\n",
      "2098    54490.241096\n",
      "2099    54805.910959\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Florida_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p106_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2097    3120.390000\n",
      "2098    3014.523333\n",
      "2099    2869.481667\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2083    20091.845000\n",
      "2084    19203.178000\n",
      "2085    18493.908750\n",
      "2086    19060.066154\n",
      "2087    18359.928636\n",
      "2088    18277.711379\n",
      "2089    18208.169189\n",
      "2090    18166.967647\n",
      "2091    18264.601951\n",
      "2092    17987.999184\n",
      "2093    17931.790400\n",
      "2094    17610.638519\n",
      "2095    17669.276250\n",
      "2096    17366.507460\n",
      "2097    17248.148065\n",
      "2098    17122.802424\n",
      "2099    16835.860137\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2059    36651.745000\n",
      "2060    37464.250833\n",
      "2061    35008.602308\n",
      "2062    33842.550000\n",
      "2063    32919.145106\n",
      "2064    34202.674000\n",
      "2065    32390.857538\n",
      "2066    32082.154925\n",
      "2067    31489.571233\n",
      "2068    31920.409880\n",
      "2069    31399.368333\n",
      "2070    30689.388889\n",
      "2071    30115.321848\n",
      "2072    29246.365876\n",
      "2073    28780.093131\n",
      "2074    27695.035490\n",
      "2075    26620.329626\n",
      "2076    26257.265727\n",
      "2077    25033.191171\n",
      "2078    24306.303363\n",
      "2079    23074.015641\n",
      "2080    22673.344590\n",
      "2081    21264.807073\n",
      "2082    20539.723016\n",
      "2083    19256.616231\n",
      "2084    18728.761314\n",
      "2085    17968.303165\n",
      "2086    16127.454324\n",
      "2087    15097.710872\n",
      "2088    13625.598418\n",
      "2089    12212.896728\n",
      "2090    10964.491845\n",
      "2091     9879.825843\n",
      "2092     9161.861823\n",
      "2093     7936.439459\n",
      "2094     6780.308011\n",
      "2095     5761.630933\n",
      "2096     4964.844596\n",
      "2097     3804.089497\n",
      "2098     2930.852574\n",
      "2099     1864.529758\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p121_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2029     61181.141667\n",
      "2030     61809.377895\n",
      "2031     62384.270556\n",
      "2032     62770.795849\n",
      "2033     63235.147619\n",
      "            ...      \n",
      "2095    170138.234932\n",
      "2096    172897.033251\n",
      "2097    175425.849671\n",
      "2098    178109.272164\n",
      "2099    180799.420849\n",
      "Name: Load_sum, Length: 71, dtype: float64\n",
      "Skipping empty file: p95_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2060    20005.630000\n",
      "2061    20253.003333\n",
      "2062    20096.449091\n",
      "2063    20164.376111\n",
      "2064    20280.680435\n",
      "2065    20352.594118\n",
      "2066    20468.412308\n",
      "2067    20603.510233\n",
      "2068    20680.446792\n",
      "2069    20763.311967\n",
      "2070    20835.624242\n",
      "2071    20951.291096\n",
      "2072    21031.422785\n",
      "2073    21140.982278\n",
      "2074    21210.046265\n",
      "2075    21262.501818\n",
      "2076    21384.502609\n",
      "2077    21443.022258\n",
      "2078    21540.645400\n",
      "2079    21631.704272\n",
      "2080    21732.277944\n",
      "2081    21795.089722\n",
      "2082    21854.343362\n",
      "2083    21945.263362\n",
      "2084    22008.412893\n",
      "2085    22097.493629\n",
      "2086    22199.306905\n",
      "2087    22283.467795\n",
      "2088    22342.160687\n",
      "2089    22407.833881\n",
      "2090    22478.666204\n",
      "2091    22595.466403\n",
      "2092    22702.686408\n",
      "2093    22724.966027\n",
      "2094    22821.114414\n",
      "2095    22921.952381\n",
      "2096    22951.201634\n",
      "2097    23067.033182\n",
      "2098    23161.806282\n",
      "2099    23208.581375\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p105_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2054    74857.000000\n",
      "2055    73270.545000\n",
      "2056    72251.229630\n",
      "2057    71238.122821\n",
      "2058    71823.333600\n",
      "2059    71645.607222\n",
      "2060    72120.976613\n",
      "2061    72936.523971\n",
      "2062    73217.009857\n",
      "2063    74014.393718\n",
      "2064    74106.977317\n",
      "2065    74681.748256\n",
      "2066    74244.807222\n",
      "2067    74302.258587\n",
      "2068    74559.014316\n",
      "2069    74538.393636\n",
      "2070    74276.868039\n",
      "2071    74219.207048\n",
      "2072    73960.280275\n",
      "2073    73768.400734\n",
      "2074    73911.819204\n",
      "2075    73479.065517\n",
      "2076    73572.764000\n",
      "2077    73315.412521\n",
      "2078    73137.846905\n",
      "2079    73144.632344\n",
      "2080    73143.153582\n",
      "2081    72957.745221\n",
      "2082    72733.185175\n",
      "2083    72131.549172\n",
      "2084    72191.337320\n",
      "2085    71897.938217\n",
      "2086    71725.545679\n",
      "2087    71454.800647\n",
      "2088    71199.886782\n",
      "2089    71180.795337\n",
      "2090    70971.458389\n",
      "2091    70771.460107\n",
      "2092    70795.568796\n",
      "2093    70627.698351\n",
      "2094    70256.155990\n",
      "2095    70281.494039\n",
      "2096    70088.357696\n",
      "2097    70107.369242\n",
      "2098    70011.307500\n",
      "2099    69940.652262\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2049    105317.884000\n",
      "2050    104173.571667\n",
      "2051    103502.561429\n",
      "2052    104880.311282\n",
      "2053    105140.536250\n",
      "2054    105679.870714\n",
      "2055    104810.934189\n",
      "2056    104108.210208\n",
      "2057    104473.351386\n",
      "2058    104881.789159\n",
      "2059    105131.835398\n",
      "2060    105699.367672\n",
      "2061    105959.196417\n",
      "2062    106308.044286\n",
      "2063    106975.572422\n",
      "2064    107020.467338\n",
      "2065    107517.471844\n",
      "2066    107235.240724\n",
      "2067    107735.209419\n",
      "2068    108141.022531\n",
      "2069    108596.941687\n",
      "2070    109118.353471\n",
      "2071    109515.882229\n",
      "2072    109794.741648\n",
      "2073    110133.675936\n",
      "2074    110440.264897\n",
      "2075    110655.413498\n",
      "2076    111109.326810\n",
      "2077    111639.433981\n",
      "2078    112113.057176\n",
      "2079    112410.233661\n",
      "2080    113367.501867\n",
      "2081    113805.435633\n",
      "2082    114219.827051\n",
      "2083    114579.465774\n",
      "2084    114500.119363\n",
      "2085    114315.678821\n",
      "2086    114740.044684\n",
      "2087    115513.421697\n",
      "2088    116065.913949\n",
      "2089    116654.852302\n",
      "2090    117243.281908\n",
      "2091    118026.524211\n",
      "2092    118720.447182\n",
      "2093    119014.933636\n",
      "2094    119413.702632\n",
      "2095    119773.594759\n",
      "2096    120117.018910\n",
      "2097    120444.411067\n",
      "2098    120528.145882\n",
      "2099    120588.150878\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Georgia_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Michigan_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2040    29426.870667\n",
      "2041    28945.380351\n",
      "2042    28921.866000\n",
      "2043    29000.968750\n",
      "2044    28985.454744\n",
      "2045    28962.763171\n",
      "2046    28885.198372\n",
      "2047    28889.171573\n",
      "2048    28863.063229\n",
      "2049    28751.615510\n",
      "2050    28734.844412\n",
      "2051    28661.530190\n",
      "2052    28595.211009\n",
      "2053    28499.522124\n",
      "2054    28432.458435\n",
      "2055    28357.538760\n",
      "2056    28233.613828\n",
      "2057    28227.475581\n",
      "2058    28075.201691\n",
      "2059    27937.330694\n",
      "2060    27876.262416\n",
      "2061    27809.438105\n",
      "2062    27742.052179\n",
      "2063    27682.031429\n",
      "2064    27677.483976\n",
      "2065    27601.698235\n",
      "2066    27573.593526\n",
      "2067    27489.694190\n",
      "2068    27466.400161\n",
      "2069    27424.361693\n",
      "2070    27379.215492\n",
      "2071    27324.339442\n",
      "2072    27290.391520\n",
      "2073    27230.265000\n",
      "2074    27200.787700\n",
      "2075    27153.468578\n",
      "2076    27112.520844\n",
      "2077    27038.224348\n",
      "2078    26976.836624\n",
      "2079    26940.063485\n",
      "2080    26913.305904\n",
      "2081    26873.787391\n",
      "2082    26817.359923\n",
      "2083    26774.219668\n",
      "2084    26759.574250\n",
      "2085    26698.485086\n",
      "2086    26682.869801\n",
      "2087    26653.954615\n",
      "2088    26663.262988\n",
      "2089    26653.152061\n",
      "2090    26636.231288\n",
      "2091    26624.928055\n",
      "2092    26619.030710\n",
      "2093    26601.345178\n",
      "2094    26579.611096\n",
      "2095    26572.410740\n",
      "2096    26565.451885\n",
      "2097    26545.020849\n",
      "2098    26529.611562\n",
      "2099    26513.453699\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p67_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2054    244973.905000\n",
      "2055    242418.510000\n",
      "2056    238809.421818\n",
      "2057    238813.062941\n",
      "2058    237173.113404\n",
      "2059    237623.791636\n",
      "2060    236606.231563\n",
      "2061    237400.637887\n",
      "2062    238090.746000\n",
      "2063    238837.325375\n",
      "2064    239457.365476\n",
      "2065    239869.524186\n",
      "2066    240042.922444\n",
      "2067    240557.079130\n",
      "2068    241551.146237\n",
      "2069    241535.470412\n",
      "2070    241794.023600\n",
      "2071    242104.217379\n",
      "2072    242031.916481\n",
      "2073    243334.468585\n",
      "2074    243795.094352\n",
      "2075    243519.713860\n",
      "2076    244175.562931\n",
      "2077    243912.580336\n",
      "2078    244653.273607\n",
      "2079    244828.142016\n",
      "2080    245343.771705\n",
      "2081    245313.449466\n",
      "2082    245324.246496\n",
      "2083    245275.267000\n",
      "2084    245333.182945\n",
      "2085    245513.172148\n",
      "2086    245759.607386\n",
      "2087    245924.014551\n",
      "2088    246371.081988\n",
      "2089    245735.798494\n",
      "2090    245793.476608\n",
      "2091    246017.906552\n",
      "2092    246716.502768\n",
      "2093    246680.054833\n",
      "2094    246790.108634\n",
      "2095    246935.659574\n",
      "2096    247427.695026\n",
      "2097    246848.192172\n",
      "2098    246855.876915\n",
      "2099    247103.812927\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p112_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2036    7257.620000\n",
      "2037    7202.920000\n",
      "2038    6801.659062\n",
      "2039    6454.385714\n",
      "2040    6408.327927\n",
      "           ...     \n",
      "2095    7962.402274\n",
      "2096    8018.081421\n",
      "2097    8070.378082\n",
      "2098    8122.782795\n",
      "2099    8173.974548\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Year\n",
      "2056    211785.966364\n",
      "2057    209041.939444\n",
      "2058    206858.217941\n",
      "2059    207149.136889\n",
      "2060    205946.178793\n",
      "2061    206035.202857\n",
      "2062    206948.165588\n",
      "2063    206803.202432\n",
      "2064    207765.743210\n",
      "2065    208223.124819\n",
      "2066    208262.162024\n",
      "2067    208990.938315\n",
      "2068    209304.195870\n",
      "2069    209993.802043\n",
      "2070    210501.191959\n",
      "2071    210627.076162\n",
      "2072    211099.383900\n",
      "2073    211490.687788\n",
      "2074    211851.616571\n",
      "2075    212183.892991\n",
      "2076    212640.491250\n",
      "2077    212689.053571\n",
      "2078    213048.146466\n",
      "2079    213289.169748\n",
      "2080    213945.039091\n",
      "2081    213470.978189\n",
      "2082    213710.099380\n",
      "2083    213371.262794\n",
      "2084    213950.739275\n",
      "2085    213729.133958\n",
      "2086    214480.141310\n",
      "2087    214464.722400\n",
      "2088    214049.435443\n",
      "2089    214608.991447\n",
      "2090    214604.968272\n",
      "2091    214572.412917\n",
      "2092    214552.912644\n",
      "2093    214549.110226\n",
      "2094    214479.168389\n",
      "2095    215021.539399\n",
      "2096    215459.458717\n",
      "2097    215530.321323\n",
      "2098    215646.387216\n",
      "2099    215652.101675\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2077    17218.212500\n",
      "2078    17097.353333\n",
      "2079    16757.126000\n",
      "2080    16339.158462\n",
      "2081    16156.143939\n",
      "2082    15992.446250\n",
      "2083    15745.612083\n",
      "2084    15893.371132\n",
      "2085    15931.466226\n",
      "2086    15864.936441\n",
      "2087    15861.829219\n",
      "2088    15567.092308\n",
      "2089    15742.526338\n",
      "2090    15916.253867\n",
      "2091    15811.075513\n",
      "2092    15991.505122\n",
      "2093    15849.803647\n",
      "2094    15596.370000\n",
      "2095    15679.838242\n",
      "2096    15631.425978\n",
      "2097    15640.780842\n",
      "2098    15429.065979\n",
      "2099    15322.503030\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2033    145579.173571\n",
      "2034    145943.414167\n",
      "2035    147110.402703\n",
      "2036    148099.321765\n",
      "2037    148276.923182\n",
      "            ...      \n",
      "2095    263675.830685\n",
      "2096    267985.695656\n",
      "2097    271926.025233\n",
      "2098    275999.847397\n",
      "2099    279979.515699\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: p88_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Pennsylvania_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2037    3531.803333\n",
      "2038    3871.204667\n",
      "2039    4772.894524\n",
      "2040    4991.122029\n",
      "2041    5045.251685\n",
      "           ...     \n",
      "2095    4241.187644\n",
      "2096    4264.264044\n",
      "2097    4278.059233\n",
      "2098    4297.492274\n",
      "2099    4318.331699\n",
      "Name: Load_sum, Length: 63, dtype: float64\n",
      "Skipping empty file: p110_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2086    2056.410000\n",
      "2087    2023.483333\n",
      "2088    2001.333077\n",
      "2089    2014.092308\n",
      "2090    2007.270000\n",
      "2091    2021.430000\n",
      "2092    2027.981429\n",
      "2093    2012.751200\n",
      "2094    2018.946970\n",
      "2095    2007.242500\n",
      "2096    2014.555833\n",
      "2097    2018.103684\n",
      "2098    2023.820500\n",
      "2099    2022.988293\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p65_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2060    20005.630000\n",
      "2061    20253.003333\n",
      "2062    20096.449091\n",
      "2063    20164.376111\n",
      "2064    20280.680435\n",
      "2065    20352.594118\n",
      "2066    20468.412308\n",
      "2067    20603.510233\n",
      "2068    20680.446792\n",
      "2069    20763.311967\n",
      "2070    20835.624242\n",
      "2071    20951.291096\n",
      "2072    21031.422785\n",
      "2073    21140.982278\n",
      "2074    21210.046265\n",
      "2075    21262.501818\n",
      "2076    21384.502609\n",
      "2077    21443.022258\n",
      "2078    21540.645400\n",
      "2079    21631.704272\n",
      "2080    21732.277944\n",
      "2081    21795.089722\n",
      "2082    21854.343362\n",
      "2083    21945.263362\n",
      "2084    22008.412893\n",
      "2085    22097.493629\n",
      "2086    22199.306905\n",
      "2087    22283.467795\n",
      "2088    22342.160687\n",
      "2089    22407.833881\n",
      "2090    22478.666204\n",
      "2091    22595.466403\n",
      "2092    22702.686408\n",
      "2093    22724.966027\n",
      "2094    22821.114414\n",
      "2095    22921.952381\n",
      "2096    22951.201634\n",
      "2097    23067.033182\n",
      "2098    23161.806282\n",
      "2099    23208.581375\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p80_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2040    41773.258182\n",
      "2041    41717.357333\n",
      "2042    40400.596545\n",
      "2043    40355.613433\n",
      "2044    40114.060395\n",
      "2045    39952.674458\n",
      "2046    39576.750337\n",
      "2047    39265.605652\n",
      "2048    38958.772900\n",
      "2049    38326.255096\n",
      "2050    37924.306075\n",
      "2051    37474.647387\n",
      "2052    37350.957759\n",
      "2053    36706.115000\n",
      "2054    36462.725203\n",
      "2055    35673.917231\n",
      "2056    35264.332239\n",
      "2057    34732.356071\n",
      "2058    33927.603200\n",
      "2059    33161.477875\n",
      "2060    32208.136343\n",
      "2061    31634.844222\n",
      "2062    31060.557135\n",
      "2063    30485.543109\n",
      "2064    29981.259545\n",
      "2065    29498.647960\n",
      "2066    28828.752536\n",
      "2067    28287.234159\n",
      "2068    27620.691577\n",
      "2069    27012.853593\n",
      "2070    26484.637395\n",
      "2071    25903.507213\n",
      "2072    25355.543720\n",
      "2073    24758.445759\n",
      "2074    24179.681477\n",
      "2075    23739.125390\n",
      "2076    23094.058149\n",
      "2077    22298.652088\n",
      "2078    21326.604143\n",
      "2079    20601.111929\n",
      "2080    19824.939194\n",
      "2081    19262.273918\n",
      "2082    18790.580055\n",
      "2083    18268.485342\n",
      "2084    17842.727896\n",
      "2085    17377.572548\n",
      "2086    16936.175863\n",
      "2087    16476.291260\n",
      "2088    15938.234508\n",
      "2089    15493.165425\n",
      "2090    15028.108329\n",
      "2091    14582.760932\n",
      "2092    14148.190956\n",
      "2093    13665.674110\n",
      "2094    13155.311671\n",
      "2095    12727.350986\n",
      "2096    12265.792814\n",
      "2097    11838.224137\n",
      "2098    11383.305589\n",
      "2099    10922.500767\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2060    361798.450000\n",
      "2061    368616.553333\n",
      "2062    370829.422353\n",
      "2063    370351.566087\n",
      "2064    370783.484412\n",
      "2065    371001.813333\n",
      "2066    371753.411220\n",
      "2067    369808.994737\n",
      "2068    367687.672703\n",
      "2069    367153.747674\n",
      "2070    366012.579278\n",
      "2071    367458.517228\n",
      "2072    368748.443302\n",
      "2073    369638.827636\n",
      "2074    372124.705455\n",
      "2075    372150.841949\n",
      "2076    375561.768632\n",
      "2077    376073.908667\n",
      "2078    377344.946613\n",
      "2079    378967.573465\n",
      "2080    380538.309318\n",
      "2081    379705.884507\n",
      "2082    381463.025139\n",
      "2083    382040.586892\n",
      "2084    383540.302662\n",
      "2085    384490.253987\n",
      "2086    386775.357358\n",
      "2087    387580.461091\n",
      "2088    389092.886391\n",
      "2089    391566.434524\n",
      "2090    391845.702102\n",
      "2091    394476.640057\n",
      "2092    396584.321611\n",
      "2093    397493.748533\n",
      "2094    398137.330995\n",
      "2095    398942.430102\n",
      "2096    401386.086400\n",
      "2097    402548.205343\n",
      "2098    404780.394320\n",
      "2099    405944.563538\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p118_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2051    43022.370000\n",
      "2053    42885.186000\n",
      "2054    43406.331500\n",
      "2055    43828.470909\n",
      "2056    43955.888919\n",
      "2057    44248.391860\n",
      "2058    44391.180800\n",
      "2059    44549.110164\n",
      "2060    44795.407206\n",
      "2061    45105.036986\n",
      "2062    45269.013976\n",
      "2063    45663.603735\n",
      "2064    45825.139130\n",
      "2065    45982.950404\n",
      "2066    46229.484200\n",
      "2067    46459.938491\n",
      "2068    46794.417748\n",
      "2069    47044.495877\n",
      "2070    47333.654310\n",
      "2071    47530.852623\n",
      "2072    47801.211520\n",
      "2073    48048.498268\n",
      "2074    48356.322769\n",
      "2075    48553.364179\n",
      "2076    48837.865362\n",
      "2077    49026.421214\n",
      "2078    49298.280417\n",
      "2079    49428.836667\n",
      "2080    49796.130526\n",
      "2081    50106.866579\n",
      "2082    50210.653500\n",
      "2083    50511.949562\n",
      "2084    50811.032561\n",
      "2085    50995.250893\n",
      "2086    51179.857299\n",
      "2087    51513.328908\n",
      "2088    51769.140904\n",
      "2089    51864.301196\n",
      "2090    52111.175187\n",
      "2091    52341.953455\n",
      "2092    52566.822487\n",
      "2093    52806.204724\n",
      "2094    52967.793645\n",
      "2095    53236.324078\n",
      "2096    53441.830519\n",
      "2097    53684.483411\n",
      "2098    53862.552694\n",
      "2099    53924.291542\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p97_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2033    4492.249000\n",
      "2034    4504.260500\n",
      "2035    4260.862500\n",
      "2036    4071.851231\n",
      "2037    3975.576250\n",
      "           ...     \n",
      "2095    5498.019288\n",
      "2096    5548.458962\n",
      "2097    5600.094712\n",
      "2098    5651.377479\n",
      "2099    5702.100055\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2030     51027.885556\n",
      "2031     51769.023125\n",
      "2032     51460.853896\n",
      "2033     51648.713100\n",
      "2034     51955.289730\n",
      "            ...      \n",
      "2095    102536.380575\n",
      "2096    103758.805984\n",
      "2097    104877.907288\n",
      "2098    106072.798849\n",
      "2099    107280.305945\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Skipping empty file: West Virginia_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p123_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2095    2196.775000\n",
      "2096    3540.786000\n",
      "2097     524.995000\n",
      "2098    1476.779167\n",
      "2099      -0.501765\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: New Jersey_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2035    1789.012500\n",
      "2036    1704.578810\n",
      "2037    1678.435417\n",
      "2038    1666.499367\n",
      "2039    1662.347209\n",
      "           ...     \n",
      "2095    1382.402521\n",
      "2096    1378.537842\n",
      "2097    1376.510932\n",
      "2098    1372.504274\n",
      "2099    1367.357233\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2033    329.941538\n",
      "2034    327.141132\n",
      "2035    328.427143\n",
      "2036    329.844643\n",
      "2037    329.857079\n",
      "           ...    \n",
      "2095    369.650877\n",
      "2096    372.137486\n",
      "2097    374.406575\n",
      "2098    376.817644\n",
      "2099    379.144877\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: North Carolina_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2092    46773.635000\n",
      "2093    40831.937000\n",
      "2094    42401.630588\n",
      "2095    39548.559474\n",
      "2096    38682.711111\n",
      "2097    39248.714063\n",
      "2098    35840.721429\n",
      "2099    37178.826667\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p107_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p86_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Delaware_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p132_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p47_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2033    20054.351250\n",
      "2034    20427.519444\n",
      "2035    20829.985143\n",
      "2036    21143.311613\n",
      "2037    21164.159195\n",
      "            ...     \n",
      "2095    25649.759123\n",
      "2096    25877.710383\n",
      "2097    26098.260658\n",
      "2098    26324.378219\n",
      "2099    26543.820740\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2029     95893.469091\n",
      "2030     96231.330952\n",
      "2031     96947.301842\n",
      "2032     97249.658421\n",
      "2033     97794.374118\n",
      "            ...      \n",
      "2095    226124.683425\n",
      "2096    229433.746667\n",
      "2097    232481.345315\n",
      "2098    235704.073479\n",
      "2099    238928.727342\n",
      "Name: Load_sum, Length: 71, dtype: float64\n",
      "Year\n",
      "2032    55879.080000\n",
      "2033    54181.760909\n",
      "2034    55260.323750\n",
      "2035    56309.853902\n",
      "2036    56374.700247\n",
      "            ...     \n",
      "2095    70328.971342\n",
      "2096    70909.190383\n",
      "2097    71462.758712\n",
      "2098    72025.826137\n",
      "2099    72573.660219\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2081    361510.055000\n",
      "2082    366842.224286\n",
      "2083    337893.978000\n",
      "2084    355311.071905\n",
      "2085    352153.574762\n",
      "2086    365319.003600\n",
      "2087    360242.526786\n",
      "2088    363812.445135\n",
      "2089    367412.413846\n",
      "2090    362681.646250\n",
      "2091    366481.541957\n",
      "2092    370140.615417\n",
      "2093    372688.277455\n",
      "2094    367060.609833\n",
      "2095    369303.955231\n",
      "2096    370254.233239\n",
      "2097    369444.294545\n",
      "2098    370444.321538\n",
      "2099    368262.705854\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p116_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p99_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2048    35275.570000\n",
      "2049    34811.650000\n",
      "2050    34961.244615\n",
      "2051    34701.151915\n",
      "2052    34970.476333\n",
      "2053    34886.128158\n",
      "2054    34736.511828\n",
      "2055    34774.862830\n",
      "2056    34698.140168\n",
      "2057    34376.431185\n",
      "2058    34147.923425\n",
      "2059    34188.412333\n",
      "2060    34249.548974\n",
      "2061    34237.188938\n",
      "2062    34368.801411\n",
      "2063    34502.838976\n",
      "2064    34554.474425\n",
      "2065    34628.457159\n",
      "2066    34695.852570\n",
      "2067    34595.075904\n",
      "2068    34699.535544\n",
      "2069    34710.677766\n",
      "2070    34658.828744\n",
      "2071    34485.463694\n",
      "2072    34557.851991\n",
      "2073    34610.186538\n",
      "2074    34544.331803\n",
      "2075    34563.319048\n",
      "2076    34628.309151\n",
      "2077    34624.799583\n",
      "2078    34748.067004\n",
      "2079    34820.672831\n",
      "2080    34952.276931\n",
      "2081    35102.045199\n",
      "2082    35199.843074\n",
      "2083    35286.708432\n",
      "2084    35410.948157\n",
      "2085    35517.162939\n",
      "2086    35581.834557\n",
      "2087    35597.215732\n",
      "2088    35600.717859\n",
      "2089    35486.175585\n",
      "2090    35404.304067\n",
      "2091    35523.770822\n",
      "2092    35746.948579\n",
      "2093    35921.944110\n",
      "2094    36100.637808\n",
      "2095    36306.964055\n",
      "2096    36533.080000\n",
      "2097    36732.948685\n",
      "2098    36941.968137\n",
      "2099    37147.595315\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2041    129383.205714\n",
      "2042    129310.238000\n",
      "2043    128739.346452\n",
      "2044    129102.315682\n",
      "2045    128337.648133\n",
      "2046    128654.297011\n",
      "2047    129284.285521\n",
      "2048    130046.410667\n",
      "2049    130976.586972\n",
      "2050    131879.805000\n",
      "2051    132647.914286\n",
      "2052    133792.439431\n",
      "2053    134585.820866\n",
      "2054    135289.901159\n",
      "2055    135706.144331\n",
      "2056    136372.987048\n",
      "2057    137083.752299\n",
      "2058    137954.532143\n",
      "2059    138741.232187\n",
      "2060    139460.639310\n",
      "2061    140390.081019\n",
      "2062    141339.487358\n",
      "2063    142183.642557\n",
      "2064    143295.606062\n",
      "2065    144042.270983\n",
      "2066    145170.732894\n",
      "2067    146084.455768\n",
      "2068    146928.385542\n",
      "2069    147951.051865\n",
      "2070    149063.729098\n",
      "2071    150063.932510\n",
      "2072    150849.192836\n",
      "2073    151949.138259\n",
      "2074    152672.090072\n",
      "2075    153562.469790\n",
      "2076    154139.367081\n",
      "2077    154337.517197\n",
      "2078    154621.507727\n",
      "2079    154869.537925\n",
      "2080    155519.891074\n",
      "2081    156610.225205\n",
      "2082    158012.375753\n",
      "2083    159352.937781\n",
      "2084    160806.519044\n",
      "2085    162197.389781\n",
      "2086    163646.443178\n",
      "2087    165090.061562\n",
      "2088    166515.542186\n",
      "2089    167946.937507\n",
      "2090    169373.284438\n",
      "2091    170858.322959\n",
      "2092    172396.112760\n",
      "2093    173813.696219\n",
      "2094    175242.832877\n",
      "2095    176758.236575\n",
      "2096    178300.267486\n",
      "2097    179760.797425\n",
      "2098    181266.355644\n",
      "2099    182771.949288\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Oklahoma_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p101_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2070    219665.616667\n",
      "2071    219598.052500\n",
      "2072    217244.875000\n",
      "2073    216871.862963\n",
      "2074    216081.745676\n",
      "2075    216735.295116\n",
      "2076    216650.462000\n",
      "2077    217146.186939\n",
      "2078    216836.308000\n",
      "2079    215294.699153\n",
      "2080    216050.363729\n",
      "2081    213658.096232\n",
      "2082    214047.999710\n",
      "2083    211848.300270\n",
      "2084    211536.718000\n",
      "2085    211059.195181\n",
      "2086    211050.422326\n",
      "2087    210902.824382\n",
      "2088    209992.602857\n",
      "2089    210031.161868\n",
      "2090    210359.395745\n",
      "2091    210428.463229\n",
      "2092    210170.356633\n",
      "2093    209795.474752\n",
      "2094    209305.673301\n",
      "2095    209245.748365\n",
      "2096    209415.016415\n",
      "2097    209026.658636\n",
      "2098    209047.375091\n",
      "2099    208606.420442\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p58_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2087    7735.544000\n",
      "2088    7744.993333\n",
      "2089    7725.167083\n",
      "2090    7754.672174\n",
      "2091    7757.558214\n",
      "2092    7798.195152\n",
      "2093    7799.959444\n",
      "2094    7822.271026\n",
      "2095    7906.981163\n",
      "2096    7908.039778\n",
      "2097    7938.734651\n",
      "2098    8010.364565\n",
      "2099    8016.121633\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2045    26444.060000\n",
      "2046    26531.027895\n",
      "2047    26701.077667\n",
      "2048    26516.011111\n",
      "2049    26478.700000\n",
      "2050    26491.581579\n",
      "2051    26485.713462\n",
      "2052    26630.432222\n",
      "2053    26708.338036\n",
      "2054    26761.730924\n",
      "2055    26872.141066\n",
      "2056    26927.773721\n",
      "2057    27014.758872\n",
      "2058    27020.966853\n",
      "2059    26971.926452\n",
      "2060    27044.311852\n",
      "2061    27117.334759\n",
      "2062    27160.826802\n",
      "2063    27248.103277\n",
      "2064    27300.019839\n",
      "2065    27304.578918\n",
      "2066    27403.303200\n",
      "2067    27395.197799\n",
      "2068    27538.516056\n",
      "2069    27611.785183\n",
      "2070    27682.619643\n",
      "2071    27817.988855\n",
      "2072    27908.049185\n",
      "2073    27974.250211\n",
      "2074    28083.941950\n",
      "2075    28034.732789\n",
      "2076    28036.714618\n",
      "2077    27961.917426\n",
      "2078    28050.243309\n",
      "2079    28182.257758\n",
      "2080    28284.588131\n",
      "2081    28347.428068\n",
      "2082    28466.540633\n",
      "2083    28518.800423\n",
      "2084    28725.753722\n",
      "2085    28786.741524\n",
      "2086    28832.843963\n",
      "2087    28844.311802\n",
      "2088    28486.720275\n",
      "2089    28633.751151\n",
      "2090    28856.506575\n",
      "2091    29079.861753\n",
      "2092    29325.588306\n",
      "2093    29534.028000\n",
      "2094    29752.212411\n",
      "2095    29981.050301\n",
      "2096    30231.047104\n",
      "2097    30453.126630\n",
      "2098    30687.404384\n",
      "2099    30921.863452\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p91_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: South Carolina_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2095    458771.920000\n",
      "2096    457051.760000\n",
      "2097    450284.622000\n",
      "2098    441365.198889\n",
      "2099    446398.105217\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p50_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p109_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p125_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2060    35813.292857\n",
      "2061    35315.773571\n",
      "2062    35088.285333\n",
      "2063    35138.025294\n",
      "2064    35182.276500\n",
      "2065    35028.348000\n",
      "2066    34634.734423\n",
      "2067    34695.285323\n",
      "2068    34160.618529\n",
      "2069    34188.597361\n",
      "2070    34132.711867\n",
      "2071    33975.367215\n",
      "2072    33723.337011\n",
      "2073    33629.530568\n",
      "2074    33618.784396\n",
      "2075    33589.135158\n",
      "2076    33582.873437\n",
      "2077    33352.828673\n",
      "2078    33409.263168\n",
      "2079    33353.362621\n",
      "2080    33280.066296\n",
      "2081    33171.733333\n",
      "2082    33162.479189\n",
      "2083    33017.062672\n",
      "2084    33063.493333\n",
      "2085    33031.524746\n",
      "2086    32966.734194\n",
      "2087    32923.541760\n",
      "2088    32725.557500\n",
      "2089    32696.743383\n",
      "2090    32523.565177\n",
      "2091    32566.339574\n",
      "2092    32356.470993\n",
      "2093    32267.673725\n",
      "2094    32174.158506\n",
      "2095    32120.430189\n",
      "2096    32039.173253\n",
      "2097    32058.337091\n",
      "2098    31973.089286\n",
      "2099    31933.540233\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2033     91906.995000\n",
      "2034     93009.690833\n",
      "2035     91950.439355\n",
      "2036     92661.873721\n",
      "2037     93474.896731\n",
      "            ...      \n",
      "2095    137184.445973\n",
      "2096    138424.104536\n",
      "2097    139634.556877\n",
      "2098    140824.153151\n",
      "2099    141977.304904\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: p103_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2040    50195.603333\n",
      "2041    50264.127895\n",
      "2042    50516.160769\n",
      "2043    50861.402895\n",
      "2044    51090.510698\n",
      "2045    51222.036964\n",
      "2046    51561.301803\n",
      "2047    51627.756528\n",
      "2048    51845.304177\n",
      "2049    51829.104588\n",
      "2050    52025.717789\n",
      "2051    52298.254040\n",
      "2052    52677.807404\n",
      "2053    52715.086757\n",
      "2054    52982.399652\n",
      "2055    53148.852941\n",
      "2056    53473.677805\n",
      "2057    53721.935354\n",
      "2058    54063.064538\n",
      "2059    54303.583971\n",
      "2060    54571.540709\n",
      "2061    54602.327800\n",
      "2062    54705.659057\n",
      "2063    54708.573140\n",
      "2064    55108.288466\n",
      "2065    55313.175110\n",
      "2066    55539.764521\n",
      "2067    55953.524316\n",
      "2068    56264.806345\n",
      "2069    56490.493039\n",
      "2070    56809.249474\n",
      "2071    57130.778186\n",
      "2072    57387.145759\n",
      "2073    57755.196283\n",
      "2074    58140.858788\n",
      "2075    58584.494638\n",
      "2076    58999.023167\n",
      "2077    59234.349755\n",
      "2078    59580.760159\n",
      "2079    59885.930781\n",
      "2080    60371.855824\n",
      "2081    60618.119588\n",
      "2082    61201.486007\n",
      "2083    61428.809164\n",
      "2084    61849.300036\n",
      "2085    62211.865825\n",
      "2086    62645.541557\n",
      "2087    62859.380168\n",
      "2088    62038.988450\n",
      "2089    61122.745110\n",
      "2090    61727.823890\n",
      "2091    62415.956548\n",
      "2092    63139.762678\n",
      "2093    63764.962795\n",
      "2094    64420.091644\n",
      "2095    65127.300822\n",
      "2096    65860.278716\n",
      "2097    66539.693151\n",
      "2098    67248.366959\n",
      "2099    67959.374301\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Arkansas_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2040    17843.906667\n",
      "2041    17844.464375\n",
      "2042    17897.567879\n",
      "2043    17923.741915\n",
      "2044    18069.514000\n",
      "2045    18177.897797\n",
      "2046    18086.715479\n",
      "2047    18038.245412\n",
      "2048    18102.945699\n",
      "2049    18193.424490\n",
      "2050    18261.150673\n",
      "2051    18355.059626\n",
      "2052    18460.710088\n",
      "2053    18556.156379\n",
      "2054    18613.333636\n",
      "2055    18696.114516\n",
      "2056    18815.475426\n",
      "2057    18916.764436\n",
      "2058    19037.359407\n",
      "2059    19087.779574\n",
      "2060    19160.983401\n",
      "2061    19213.458684\n",
      "2062    19192.405250\n",
      "2063    19266.082182\n",
      "2064    19366.168555\n",
      "2065    19384.767722\n",
      "2066    19413.677500\n",
      "2067    19518.387150\n",
      "2068    19636.007677\n",
      "2069    19718.063005\n",
      "2070    19753.616190\n",
      "2071    19865.905841\n",
      "2072    20037.540829\n",
      "2073    20130.516545\n",
      "2074    20259.691429\n",
      "2075    20385.956476\n",
      "2076    20484.094809\n",
      "2077    20593.821186\n",
      "2078    20703.561162\n",
      "2079    20670.775657\n",
      "2080    20583.630410\n",
      "2081    20639.601164\n",
      "2082    20748.468901\n",
      "2083    20830.560623\n",
      "2084    21028.873106\n",
      "2085    21123.492676\n",
      "2086    21257.390328\n",
      "2087    21417.486461\n",
      "2088    21586.536134\n",
      "2089    21753.521810\n",
      "2090    21845.111610\n",
      "2091    21952.555196\n",
      "2092    21885.016068\n",
      "2093    21893.744110\n",
      "2094    22123.646959\n",
      "2095    22369.991726\n",
      "2096    22619.349426\n",
      "2097    22853.933151\n",
      "2098    23102.203370\n",
      "2099    23350.917068\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2091    381847.010000\n",
      "2093    385231.776667\n",
      "2094    384023.555833\n",
      "2095    389325.464375\n",
      "2096    391424.477500\n",
      "2097    392245.298571\n",
      "2098    391541.076000\n",
      "2099    390769.731389\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2064    4741.868182\n",
      "2065    4803.386875\n",
      "2066    4648.737368\n",
      "2067    4601.399167\n",
      "2068    4628.100000\n",
      "2069    4629.684375\n",
      "2070    4615.930526\n",
      "2071    4607.118947\n",
      "2072    4605.816170\n",
      "2073    4531.781636\n",
      "2074    4459.691061\n",
      "2075    4434.556579\n",
      "2076    4435.934756\n",
      "2077    4433.838977\n",
      "2078    4433.844302\n",
      "2079    4442.813152\n",
      "2080    4474.991250\n",
      "2081    4458.311458\n",
      "2082    4469.118600\n",
      "2083    4463.347600\n",
      "2084    4480.707745\n",
      "2085    4479.351226\n",
      "2086    4479.812703\n",
      "2087    4470.491261\n",
      "2088    4494.105841\n",
      "2089    4489.812348\n",
      "2090    4492.685431\n",
      "2091    4486.624917\n",
      "2092    4507.177040\n",
      "2093    4507.038211\n",
      "2094    4516.386560\n",
      "2095    4508.479688\n",
      "2096    4502.893926\n",
      "2097    4517.171642\n",
      "2098    4498.561064\n",
      "2099    4488.349371\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2077    102138.233333\n",
      "2078     88579.917143\n",
      "2079     92035.645000\n",
      "2080     91534.087619\n",
      "2081     86826.083571\n",
      "2082     87245.802500\n",
      "2083     84413.186047\n",
      "2084     90197.210784\n",
      "2085     87133.676731\n",
      "2086     81941.327636\n",
      "2087     84840.659077\n",
      "2088     85457.033857\n",
      "2089     85512.982192\n",
      "2090     84473.620274\n",
      "2091     85132.705500\n",
      "2092     84124.776627\n",
      "2093     81280.154302\n",
      "2094     79808.352874\n",
      "2095     79229.522111\n",
      "2096     76646.971398\n",
      "2097     74516.433750\n",
      "2098     73563.698454\n",
      "2099     69751.657800\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p93_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2035    48259.589474\n",
      "2036    48778.100303\n",
      "2037    49214.265385\n",
      "2038    49383.749400\n",
      "2039    49768.196724\n",
      "            ...     \n",
      "2095    75501.375041\n",
      "2096    76434.183907\n",
      "2097    77313.940685\n",
      "2098    78227.285288\n",
      "2099    79134.326164\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2035    6355.981429\n",
      "2036    6246.497059\n",
      "2037    6261.524400\n",
      "2038    6185.332857\n",
      "2039    6123.312549\n",
      "           ...     \n",
      "2095    8280.182219\n",
      "2096    8368.700000\n",
      "2097    8449.447616\n",
      "2098    8535.537014\n",
      "2099    8623.203123\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2054    14152.333333\n",
      "2055    14072.124762\n",
      "2056    14028.674545\n",
      "2057    13936.552391\n",
      "2058    13954.692222\n",
      "2059    13961.454746\n",
      "2060    13972.996471\n",
      "2061    14014.358310\n",
      "2062    14064.514177\n",
      "2063    14048.524634\n",
      "2064    14080.314118\n",
      "2065    14086.646444\n",
      "2066    14021.367553\n",
      "2067    14030.873789\n",
      "2068    14011.420099\n",
      "2069    13979.685882\n",
      "2070    13956.749714\n",
      "2071    13913.117156\n",
      "2072    13871.807946\n",
      "2073    13872.064144\n",
      "2074    13828.169576\n",
      "2075    13827.409915\n",
      "2076    13797.242195\n",
      "2077    13723.920160\n",
      "2078    13685.800458\n",
      "2079    13651.370515\n",
      "2080    13622.086338\n",
      "2081    13585.364444\n",
      "2082    13515.024238\n",
      "2083    13456.313922\n",
      "2084    13413.399317\n",
      "2085    13396.077546\n",
      "2086    13315.402907\n",
      "2087    13277.011080\n",
      "2088    13227.595746\n",
      "2089    13222.024889\n",
      "2090    13140.883085\n",
      "2091    13138.711436\n",
      "2092    13098.810464\n",
      "2093    13052.921726\n",
      "2094    12970.068732\n",
      "2095    12943.606827\n",
      "2096    12892.154744\n",
      "2097    12868.935945\n",
      "2098    12818.452703\n",
      "2099    12765.583684\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2042    31925.186667\n",
      "2043    30974.021429\n",
      "2044    30833.037500\n",
      "2045    30740.998871\n",
      "2046    30803.113043\n",
      "2047    30748.140769\n",
      "2048    30760.573780\n",
      "2049    30576.432326\n",
      "2050    30607.513333\n",
      "2051    30494.659583\n",
      "2052    30347.469216\n",
      "2053    30226.274660\n",
      "2054    30125.967685\n",
      "2055    29977.107182\n",
      "2056    29904.671140\n",
      "2057    29810.863621\n",
      "2058    29653.971803\n",
      "2059    29491.816378\n",
      "2060    29278.045188\n",
      "2061    29171.642222\n",
      "2062    28989.275674\n",
      "2063    28808.617162\n",
      "2064    28685.951026\n",
      "2065    28393.023012\n",
      "2066    28203.051824\n",
      "2067    28044.846102\n",
      "2068    27902.577166\n",
      "2069    27821.699471\n",
      "2070    27676.036340\n",
      "2071    27541.555075\n",
      "2072    27403.532837\n",
      "2073    27288.147109\n",
      "2074    27172.019631\n",
      "2075    27069.555928\n",
      "2076    26941.378304\n",
      "2077    26801.779915\n",
      "2078    26707.621494\n",
      "2079    26580.403936\n",
      "2080    26514.055850\n",
      "2081    26394.074449\n",
      "2082    26309.302500\n",
      "2083    26214.194301\n",
      "2084    26138.652109\n",
      "2085    26065.970032\n",
      "2086    26005.258265\n",
      "2087    25973.615620\n",
      "2088    25873.990109\n",
      "2089    25806.554877\n",
      "2090    25734.900356\n",
      "2091    25667.853096\n",
      "2092    25602.194536\n",
      "2093    25519.414301\n",
      "2094    25420.050329\n",
      "2095    25358.636712\n",
      "2096    25293.047104\n",
      "2097    25227.619041\n",
      "2098    25154.187068\n",
      "2099    25079.372384\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2066    42193.705000\n",
      "2067    42545.435714\n",
      "2068    42827.214118\n",
      "2069    42944.475000\n",
      "2070    43068.467941\n",
      "2071    43388.738919\n",
      "2072    43677.618000\n",
      "2073    43957.748627\n",
      "2074    44364.027931\n",
      "2075    44623.083810\n",
      "2076    44904.115152\n",
      "2077    45068.671739\n",
      "2078    45557.552917\n",
      "2079    45888.716962\n",
      "2080    46128.215000\n",
      "2081    46560.234458\n",
      "2082    46852.925057\n",
      "2083    47000.876563\n",
      "2084    47354.706939\n",
      "2085    47725.116289\n",
      "2086    47933.911346\n",
      "2087    48171.568818\n",
      "2088    48504.990268\n",
      "2089    48784.058957\n",
      "2090    49100.171538\n",
      "2091    49454.139402\n",
      "2092    49716.312683\n",
      "2093    49923.756772\n",
      "2094    50184.540625\n",
      "2095    50588.001473\n",
      "2096    50892.454511\n",
      "2097    51212.778182\n",
      "2098    51462.807956\n",
      "2099    51749.900500\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p84_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p114_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p61_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2032    3749.772500\n",
      "2033    3615.226364\n",
      "2034    4150.862500\n",
      "2035    4259.474500\n",
      "2036    4358.940847\n",
      "           ...     \n",
      "2095    5695.690301\n",
      "2096    5747.855082\n",
      "2097    5788.252411\n",
      "2098    5834.359342\n",
      "2099    5880.181233\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p87_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2052    87159.440000\n",
      "2053    77507.123158\n",
      "2054    75299.688462\n",
      "2055    73606.148966\n",
      "2056    71392.204222\n",
      "2057    68538.045246\n",
      "2058    67105.118592\n",
      "2059    65693.420133\n",
      "2060    64292.044416\n",
      "2061    62901.741429\n",
      "2062    62305.716471\n",
      "2063    61234.005217\n",
      "2064    60287.176289\n",
      "2065    57852.301959\n",
      "2066    55906.883592\n",
      "2067    54678.088846\n",
      "2068    53574.246321\n",
      "2069    52178.280541\n",
      "2070    49958.357168\n",
      "2071    48371.937583\n",
      "2072    46143.921967\n",
      "2073    45441.370887\n",
      "2074    43201.131860\n",
      "2075    41709.319538\n",
      "2076    39959.678750\n",
      "2077    37597.660643\n",
      "2078    35499.915068\n",
      "2079    33387.215592\n",
      "2080    30450.144845\n",
      "2081    28493.969583\n",
      "2082    26279.060172\n",
      "2083    24083.028807\n",
      "2084    22480.429180\n",
      "2085    20430.657433\n",
      "2086    18493.894375\n",
      "2087    16390.896443\n",
      "2088    14573.689648\n",
      "2089    12226.444216\n",
      "2090    10545.509175\n",
      "2091     8790.134028\n",
      "2092     6607.244470\n",
      "2093     4635.407342\n",
      "2094     2472.729244\n",
      "2095      315.275541\n",
      "2096    -1715.991568\n",
      "2097    -3697.949375\n",
      "2098    -5669.761405\n",
      "2099    -7611.060040\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2067    30754.285000\n",
      "2069    30829.983750\n",
      "2070    29868.366667\n",
      "2071    30042.090769\n",
      "2072    29797.571944\n",
      "2073    29776.567143\n",
      "2074    29801.409400\n",
      "2075    29711.302364\n",
      "2076    29863.781250\n",
      "2077    29591.253175\n",
      "2078    29730.470455\n",
      "2079    29669.641250\n",
      "2080    29644.793333\n",
      "2081    29643.813704\n",
      "2082    29678.915000\n",
      "2083    29697.189759\n",
      "2084    29753.236136\n",
      "2085    29759.287333\n",
      "2086    29817.530543\n",
      "2087    29861.238947\n",
      "2088    29861.197938\n",
      "2089    29915.350619\n",
      "2090    29958.000300\n",
      "2091    29983.134804\n",
      "2092    30075.815588\n",
      "2093    30061.847500\n",
      "2094    30029.008704\n",
      "2095    30090.059817\n",
      "2096    30230.478056\n",
      "2097    30247.897568\n",
      "2098    30203.488596\n",
      "2099    30240.640261\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2083    26132.115000\n",
      "2085    25671.510000\n",
      "2086    26197.065000\n",
      "2087    25893.902500\n",
      "2088    26146.382308\n",
      "2089    26459.905556\n",
      "2090    26438.548065\n",
      "2091    26613.962059\n",
      "2092    26539.302500\n",
      "2093    26704.824103\n",
      "2094    26745.468000\n",
      "2095    26795.142979\n",
      "2096    26886.530556\n",
      "2097    26944.640500\n",
      "2098    27046.211228\n",
      "2099    27137.089138\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2066    42193.705000\n",
      "2067    42545.435714\n",
      "2068    42827.214118\n",
      "2069    42944.475000\n",
      "2070    43068.467941\n",
      "2071    43388.738919\n",
      "2072    43677.618000\n",
      "2073    43957.748627\n",
      "2074    44364.027931\n",
      "2075    44623.083810\n",
      "2076    44904.115152\n",
      "2077    45068.671739\n",
      "2078    45557.552917\n",
      "2079    45888.716962\n",
      "2080    46128.215000\n",
      "2081    46560.234458\n",
      "2082    46852.925057\n",
      "2083    47000.876563\n",
      "2084    47354.706939\n",
      "2085    47725.116289\n",
      "2086    47933.911346\n",
      "2087    48171.568818\n",
      "2088    48504.990268\n",
      "2089    48784.058957\n",
      "2090    49100.171538\n",
      "2091    49454.139402\n",
      "2092    49716.312683\n",
      "2093    49923.756772\n",
      "2094    50184.540625\n",
      "2095    50588.001473\n",
      "2096    50892.454511\n",
      "2097    51212.778182\n",
      "2098    51462.807956\n",
      "2099    51749.900500\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2033     92418.230000\n",
      "2034     88438.776250\n",
      "2035     87567.251613\n",
      "2036     88277.536512\n",
      "2037     89025.168039\n",
      "            ...      \n",
      "2095    131488.755671\n",
      "2096    132676.249454\n",
      "2097    133846.304466\n",
      "2098    134989.793808\n",
      "2099    136097.123671\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2051    164654.193333\n",
      "2052    168705.634500\n",
      "2053    168481.710741\n",
      "2054    168844.525526\n",
      "2055    168169.127755\n",
      "2056    167061.240152\n",
      "2057    166625.202195\n",
      "2058    165390.664286\n",
      "2059    165286.057315\n",
      "2060    165939.039730\n",
      "2061    165990.247288\n",
      "2062    166196.282951\n",
      "2063    165707.692000\n",
      "2064    165737.869931\n",
      "2065    165321.895132\n",
      "2066    165227.301329\n",
      "2067    166291.828734\n",
      "2068    166626.839760\n",
      "2069    167265.961845\n",
      "2070    167888.799128\n",
      "2071    168210.393842\n",
      "2072    168435.814162\n",
      "2073    169542.519891\n",
      "2074    169365.290838\n",
      "2075    169400.055900\n",
      "2076    170302.737805\n",
      "2077    169850.708651\n",
      "2078    170247.539369\n",
      "2079    170847.736960\n",
      "2080    171222.796352\n",
      "2081    171277.468870\n",
      "2082    170942.104618\n",
      "2083    170634.877954\n",
      "2084    171139.471321\n",
      "2085    171977.856955\n",
      "2086    172442.087269\n",
      "2087    173194.635036\n",
      "2088    173852.980072\n",
      "2089    174383.715426\n",
      "2090    175107.200350\n",
      "2091    175842.430519\n",
      "2092    176638.883797\n",
      "2093    177021.982633\n",
      "2094    176961.485370\n",
      "2095    177313.026006\n",
      "2096    176644.640593\n",
      "2097    176724.429162\n",
      "2098    176710.587131\n",
      "2099    177305.431890\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Texas_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p117_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p62_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2031    4653.140000\n",
      "2032    4712.545294\n",
      "2033    4759.408372\n",
      "2034    4775.155769\n",
      "2035    4781.435955\n",
      "           ...     \n",
      "2095    4752.898055\n",
      "2096    4776.188661\n",
      "2097    4795.618877\n",
      "2098    4815.281726\n",
      "2099    4834.618521\n",
      "Name: Load_sum, Length: 69, dtype: float64\n",
      "Year\n",
      "2032    4343.865000\n",
      "2033    4238.500714\n",
      "2034    4228.152759\n",
      "2035    4270.620750\n",
      "2036    4296.115192\n",
      "           ...     \n",
      "2095    6443.588055\n",
      "2096    6524.855628\n",
      "2097    6609.523452\n",
      "2098    6694.734164\n",
      "2099    6779.664822\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p98_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2064    33604.290000\n",
      "2065    34861.572222\n",
      "2066    35090.392381\n",
      "2067    34734.378125\n",
      "2068    34985.705263\n",
      "2069    35010.770476\n",
      "2070    35083.197609\n",
      "2071    35163.584694\n",
      "2072    35084.237719\n",
      "2073    34890.293871\n",
      "2074    35043.066349\n",
      "2075    34629.465135\n",
      "2076    34594.139091\n",
      "2077    34455.184756\n",
      "2078    34553.129157\n",
      "2079    34670.414096\n",
      "2080    34624.791333\n",
      "2081    34748.541111\n",
      "2082    34673.849785\n",
      "2083    34552.418021\n",
      "2084    34649.858333\n",
      "2085    34795.444257\n",
      "2086    34857.584757\n",
      "2087    34944.608571\n",
      "2088    34714.242252\n",
      "2089    34798.744000\n",
      "2090    34800.186957\n",
      "2091    34966.161391\n",
      "2092    34969.656116\n",
      "2093    35073.278992\n",
      "2094    34886.447236\n",
      "2095    34947.077132\n",
      "2096    34974.831818\n",
      "2097    35047.655956\n",
      "2098    35091.485000\n",
      "2099    35086.909510\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2050    16477.623333\n",
      "2051    15920.556667\n",
      "2052    16290.049524\n",
      "2053    16444.335926\n",
      "2054    16409.848182\n",
      "2055    16352.742857\n",
      "2056    16474.657111\n",
      "2057    16558.958077\n",
      "2058    16485.905410\n",
      "2059    16559.702059\n",
      "2060    16440.735467\n",
      "2061    16419.527590\n",
      "2062    16439.146667\n",
      "2063    16457.579158\n",
      "2064    16487.943535\n",
      "2065    16521.462524\n",
      "2066    16485.531215\n",
      "2067    16516.667768\n",
      "2068    16489.898718\n",
      "2069    16479.777951\n",
      "2070    16554.762358\n",
      "2071    16578.504444\n",
      "2072    16578.238837\n",
      "2073    16571.242761\n",
      "2074    16596.080072\n",
      "2075    16637.867305\n",
      "2076    16630.847383\n",
      "2077    16604.398477\n",
      "2078    16549.379512\n",
      "2079    16586.922256\n",
      "2080    16596.915814\n",
      "2081    16538.031492\n",
      "2082    16536.219837\n",
      "2083    16530.674840\n",
      "2084    16556.386237\n",
      "2085    16574.851421\n",
      "2086    16580.437450\n",
      "2087    16535.716890\n",
      "2088    16535.133953\n",
      "2089    16473.154439\n",
      "2090    16513.448616\n",
      "2091    16570.506637\n",
      "2092    16573.631931\n",
      "2093    16576.271949\n",
      "2094    16581.177899\n",
      "2095    16579.363539\n",
      "2096    16535.764167\n",
      "2097    16533.661882\n",
      "2098    16523.128391\n",
      "2099    16498.356067\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p100_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p90_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2036    17850.305000\n",
      "2037    16727.700625\n",
      "2038    16166.935882\n",
      "2039    16263.376829\n",
      "2040    16367.423333\n",
      "            ...     \n",
      "2095    21716.274301\n",
      "2096    21899.915820\n",
      "2097    22076.467699\n",
      "2098    22254.710301\n",
      "2099    22430.729452\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Skipping empty file: p124_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2032    37823.302500\n",
      "2033    34621.458462\n",
      "2034    34245.434366\n",
      "2035    34392.362278\n",
      "2036    34501.597093\n",
      "            ...     \n",
      "2095    28730.300356\n",
      "2096    28714.662787\n",
      "2097    28688.251397\n",
      "2098    28665.778301\n",
      "2099    28641.745945\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p108_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p51_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Maryland_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p102_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2065    9083.720000\n",
      "2066    9233.856923\n",
      "2067    9108.139524\n",
      "2068    9049.229333\n",
      "2069    8966.025897\n",
      "2070    8948.385000\n",
      "2071    8933.036800\n",
      "2072    8907.359811\n",
      "2073    8876.509375\n",
      "2074    8820.948209\n",
      "2075    8811.135417\n",
      "2076    8792.738961\n",
      "2077    8733.325823\n",
      "2078    8734.375976\n",
      "2079    8739.378214\n",
      "2080    8696.386742\n",
      "2081    8697.513596\n",
      "2082    8670.265745\n",
      "2083    8632.475957\n",
      "2084    8622.590612\n",
      "2085    8615.091386\n",
      "2086    8584.153725\n",
      "2087    8567.785146\n",
      "2088    8530.031682\n",
      "2089    8512.772569\n",
      "2090    8505.273455\n",
      "2091    8499.004643\n",
      "2092    8471.242308\n",
      "2093    8446.659661\n",
      "2094    8414.514917\n",
      "2095    8401.491951\n",
      "2096    8392.578016\n",
      "2097    8376.902813\n",
      "2098    8357.089154\n",
      "2099    8326.261579\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2055    129784.340000\n",
      "2056    130778.617895\n",
      "2057    130282.241290\n",
      "2058    130940.112308\n",
      "2059    131299.652500\n",
      "2060    129823.925968\n",
      "2061    128748.099750\n",
      "2062    128002.092020\n",
      "2063    128251.907196\n",
      "2064    128512.654336\n",
      "2065    128766.874397\n",
      "2066    128798.590325\n",
      "2067    128991.814186\n",
      "2068    128427.181119\n",
      "2069    128522.580068\n",
      "2070    128746.355455\n",
      "2071    129009.135190\n",
      "2072    129413.917866\n",
      "2073    129904.856024\n",
      "2074    130199.829591\n",
      "2075    130695.615600\n",
      "2076    131183.471611\n",
      "2077    130986.573032\n",
      "2078    131430.436806\n",
      "2079    131786.382194\n",
      "2080    131946.406195\n",
      "2081    132297.343349\n",
      "2082    132260.139312\n",
      "2083    132496.960357\n",
      "2084    132977.991087\n",
      "2085    132976.124576\n",
      "2086    133130.843058\n",
      "2087    132500.796133\n",
      "2088    132793.180191\n",
      "2089    133291.123333\n",
      "2090    133391.493787\n",
      "2091    134214.826140\n",
      "2092    134686.012266\n",
      "2093    135490.345632\n",
      "2094    135683.349542\n",
      "2095    136247.627422\n",
      "2096    137038.249241\n",
      "2097    137148.824949\n",
      "2098    137458.587697\n",
      "2099    137826.992065\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032    28551.557778\n",
      "2033    28592.073750\n",
      "2034    27972.386207\n",
      "2035    27549.944889\n",
      "2036    27338.798333\n",
      "            ...     \n",
      "2095    39326.531644\n",
      "2096    39816.010956\n",
      "2097    40271.999014\n",
      "2098    40755.522137\n",
      "2099    41233.236822\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2052    76266.422000\n",
      "2053    72566.731176\n",
      "2054    70963.167308\n",
      "2055    67928.855333\n",
      "2056    66342.386667\n",
      "2057    64254.875574\n",
      "2058    62816.866761\n",
      "2059    61395.578267\n",
      "2060    59827.476282\n",
      "2061    58596.467262\n",
      "2062    57977.504235\n",
      "2063    56506.546489\n",
      "2064    55634.835408\n",
      "2065    53320.042143\n",
      "2066    51572.825146\n",
      "2067    50335.240769\n",
      "2068    49007.724393\n",
      "2069    47601.115804\n",
      "2070    45611.163451\n",
      "2071    44031.451333\n",
      "2072    41813.385492\n",
      "2073    41097.447984\n",
      "2074    38872.095271\n",
      "2075    37520.406489\n",
      "2076    35625.370735\n",
      "2077    33282.464571\n",
      "2078    31063.833041\n",
      "2079    28457.705484\n",
      "2080    26116.648282\n",
      "2081    24259.327202\n",
      "2082    22154.481886\n",
      "2083    19964.681299\n",
      "2084    18352.974130\n",
      "2085    16235.148289\n",
      "2086    14304.755312\n",
      "2087    12206.398144\n",
      "2088    10287.590547\n",
      "2089     8190.506942\n",
      "2090     6160.523524\n",
      "2091     4208.030514\n",
      "2092     2417.547364\n",
      "2093      370.043661\n",
      "2094    -1960.534758\n",
      "2095    -3813.410431\n",
      "2096    -5847.234412\n",
      "2097    -7820.539250\n",
      "2098    -9973.110528\n",
      "2099   -11948.455645\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p126_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2033    30270.274737\n",
      "2034    29422.922258\n",
      "2035    29513.316620\n",
      "2036    29641.822073\n",
      "2037    29548.291977\n",
      "            ...     \n",
      "2095    23977.402301\n",
      "2096    23938.474126\n",
      "2097    23892.632521\n",
      "2098    23850.496575\n",
      "2099    23807.127425\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2047    14971.140000\n",
      "2048    15508.362778\n",
      "2049    15460.197407\n",
      "2050    15517.671951\n",
      "2051    15617.693000\n",
      "2052    15600.340000\n",
      "2053    15508.357889\n",
      "2054    15557.848571\n",
      "2055    15620.026569\n",
      "2056    15701.942150\n",
      "2057    15768.745135\n",
      "2058    15863.640789\n",
      "2059    15941.476864\n",
      "2060    16016.967967\n",
      "2061    16050.902248\n",
      "2062    16124.824662\n",
      "2063    16186.065971\n",
      "2064    16203.451946\n",
      "2065    16284.924868\n",
      "2066    16282.983688\n",
      "2067    16340.128667\n",
      "2068    16432.113882\n",
      "2069    16433.497374\n",
      "2070    16512.512283\n",
      "2071    16555.136911\n",
      "2072    16619.090909\n",
      "2073    16692.113267\n",
      "2074    16779.549757\n",
      "2075    16878.270286\n",
      "2076    16962.842315\n",
      "2077    17052.251606\n",
      "2078    17109.326384\n",
      "2079    17215.651938\n",
      "2080    17306.354721\n",
      "2081    17423.024378\n",
      "2082    17468.543417\n",
      "2083    17473.664677\n",
      "2084    17348.618918\n",
      "2085    17431.315110\n",
      "2086    17529.464565\n",
      "2087    17617.829822\n",
      "2088    17679.475966\n",
      "2089    17765.896177\n",
      "2090    17881.646532\n",
      "2091    17965.070561\n",
      "2092    18072.079903\n",
      "2093    18185.098907\n",
      "2094    18306.355127\n",
      "2095    18379.398156\n",
      "2096    18516.569414\n",
      "2097    18424.037097\n",
      "2098    18401.952761\n",
      "2099    18448.658658\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p92_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2077    196859.495000\n",
      "2078    206302.385000\n",
      "2079    208002.610000\n",
      "2080    200243.838000\n",
      "2081    201689.325556\n",
      "2082    202396.888182\n",
      "2083    202850.811613\n",
      "2084    204240.353714\n",
      "2085    204638.606098\n",
      "2086    204825.707609\n",
      "2087    205218.938800\n",
      "2088    206081.053273\n",
      "2089    206849.901053\n",
      "2090    207481.721695\n",
      "2091    208296.548030\n",
      "2092    209258.780746\n",
      "2093    209531.226620\n",
      "2094    209814.711688\n",
      "2095    210693.162593\n",
      "2096    211537.590843\n",
      "2097    212750.596625\n",
      "2098    212639.602527\n",
      "2099    213603.932088\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Indiana_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2040    6048.703333\n",
      "2041    5913.018333\n",
      "2042    5855.907551\n",
      "2043    5878.924444\n",
      "2044    5920.143380\n",
      "2045    5951.490658\n",
      "2046    5976.801220\n",
      "2047    5995.994023\n",
      "2048    6023.550532\n",
      "2049    6029.017629\n",
      "2050    6057.196000\n",
      "2051    6080.262233\n",
      "2052    6091.028774\n",
      "2053    6105.240727\n",
      "2054    6123.985179\n",
      "2055    6124.126807\n",
      "2056    6136.355161\n",
      "2057    6152.608583\n",
      "2058    6160.281308\n",
      "2059    6156.262754\n",
      "2060    6173.197676\n",
      "2061    6164.996284\n",
      "2062    6164.219299\n",
      "2063    6171.929006\n",
      "2064    6177.355000\n",
      "2065    6177.891156\n",
      "2066    6182.513966\n",
      "2067    6189.653892\n",
      "2068    6212.507884\n",
      "2069    6203.273333\n",
      "2070    6206.623831\n",
      "2071    6219.061845\n",
      "2072    6232.072524\n",
      "2073    6232.140876\n",
      "2074    6229.966996\n",
      "2075    6240.093186\n",
      "2076    6237.627447\n",
      "2077    6236.158285\n",
      "2078    6245.864877\n",
      "2079    6224.098824\n",
      "2080    6221.943550\n",
      "2081    6214.743271\n",
      "2082    6199.772832\n",
      "2083    6156.240203\n",
      "2084    6093.287785\n",
      "2085    6044.701257\n",
      "2086    5979.204958\n",
      "2087    5974.407205\n",
      "2088    6006.058388\n",
      "2089    6027.160685\n",
      "2090    6051.274521\n",
      "2091    6076.357589\n",
      "2092    6103.128142\n",
      "2093    6127.924055\n",
      "2094    6157.162055\n",
      "2095    6182.149479\n",
      "2096    6209.222268\n",
      "2097    6230.443096\n",
      "2098    6257.497808\n",
      "2099    6286.881945\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2048    11497.155000\n",
      "2049    11133.689231\n",
      "2050    11095.011290\n",
      "2051    11085.290000\n",
      "2052    11193.891250\n",
      "2053    11243.154762\n",
      "2054    11310.573662\n",
      "2055    11319.344933\n",
      "2056    11399.332099\n",
      "2057    11423.772169\n",
      "2058    11467.888706\n",
      "2059    11506.214891\n",
      "2060    11523.017083\n",
      "2061    11563.076701\n",
      "2062    11578.839020\n",
      "2063    11600.241748\n",
      "2064    11624.966168\n",
      "2065    11635.286606\n",
      "2066    11635.322679\n",
      "2067    11670.595652\n",
      "2068    11690.446891\n",
      "2069    11684.607213\n",
      "2070    11714.992339\n",
      "2071    11725.466984\n",
      "2072    11701.663481\n",
      "2073    11717.809500\n",
      "2074    11732.327483\n",
      "2075    11694.735563\n",
      "2076    11717.911161\n",
      "2077    11708.604654\n",
      "2078    11728.976914\n",
      "2079    11746.959036\n",
      "2080    11739.911802\n",
      "2081    11714.116536\n",
      "2082    11724.516209\n",
      "2083    11736.784432\n",
      "2084    11754.615316\n",
      "2085    11741.219949\n",
      "2086    11757.384070\n",
      "2087    11749.199118\n",
      "2088    11757.692885\n",
      "2089    11763.791604\n",
      "2090    11782.290093\n",
      "2091    11747.977117\n",
      "2092    11765.048584\n",
      "2093    11730.047382\n",
      "2094    11732.658008\n",
      "2095    11726.064050\n",
      "2096    11746.731102\n",
      "2097    11703.953849\n",
      "2098    11694.928992\n",
      "2099    11685.091515\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p85_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: Alabama_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2043    68799.262857\n",
      "2044    69148.480500\n",
      "2045    69459.975000\n",
      "2046    70099.005667\n",
      "2047    70504.881026\n",
      "2048    70695.438913\n",
      "2049    70751.547368\n",
      "2050    71021.980317\n",
      "2051    71104.930685\n",
      "2052    71392.092750\n",
      "2053    71484.612941\n",
      "2054    71604.170000\n",
      "2055    71917.441354\n",
      "2056    72161.231553\n",
      "2057    72349.280550\n",
      "2058    72729.004054\n",
      "2059    73033.594087\n",
      "2060    73178.634711\n",
      "2061    73452.721290\n",
      "2062    73755.764409\n",
      "2063    74101.294242\n",
      "2064    74506.861679\n",
      "2065    74718.493830\n",
      "2066    74801.335338\n",
      "2067    74997.555385\n",
      "2068    75331.740988\n",
      "2069    75425.013023\n",
      "2070    75778.394716\n",
      "2071    76024.715054\n",
      "2072    76463.683904\n",
      "2073    76647.617254\n",
      "2074    77004.245800\n",
      "2075    77535.609303\n",
      "2076    77977.531159\n",
      "2077    78159.588037\n",
      "2078    78653.367235\n",
      "2079    78938.215536\n",
      "2080    79402.788268\n",
      "2081    79784.178718\n",
      "2082    80220.672731\n",
      "2083    80646.946570\n",
      "2084    81049.895887\n",
      "2085    81490.486440\n",
      "2086    81980.035315\n",
      "2087    82394.110541\n",
      "2088    82741.575962\n",
      "2089    83154.748694\n",
      "2090    83404.801055\n",
      "2091    83864.817536\n",
      "2092    84350.025140\n",
      "2093    84680.177663\n",
      "2094    85112.111288\n",
      "2095    85214.459314\n",
      "2096    84121.352543\n",
      "2097    83935.380164\n",
      "2098    84635.249808\n",
      "2099    85340.436192\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p60_extreme_demand_outliers_min_projection.csv\n",
      "Skipping empty file: p115_extreme_demand_outliers_min_projection.csv\n",
      "Year\n",
      "2030    23920.648333\n",
      "2031    24156.296667\n",
      "2032    24041.984000\n",
      "2033    24075.559565\n",
      "2034    24110.689821\n",
      "            ...     \n",
      "2095    62953.724219\n",
      "2096    63901.951639\n",
      "2097    64749.895589\n",
      "2098    65663.360274\n",
      "2099    66554.917288\n",
      "Name: Load_sum, Length: 70, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "def string_to_set(string):\n",
    "    # Remove curly braces and split the string\n",
    "    elements = string.replace('{', '').replace('}', '').split(',')\n",
    "    # Remove any extra whitespace and single quotes from each element\n",
    "    elements = [e.strip().replace(\"'\", \"\") for e in elements]\n",
    "    return set(elements)\n",
    "\n",
    "cases = ['projection']\n",
    "for case in cases:#, 'rcp45hotter', 'rcp45cooler']\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/output/future_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier_demand'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    for rb_code in rb_codes:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "\n",
    "        ci_max_path = os.path.join(ci_directory, f'{rb_code}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{rb_code}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "            \n",
    "            for year in years:\n",
    "                weather_file_path = os.path.join(directory, f'{rb_code}_WRF_Hourly_Mean_Meteorology_{year}.csv')\n",
    "                demand_file_path = f'/Volumes/T7/prediction/{case}/{year}/{rb_code}_{year}_mlp_output.csv'\n",
    "\n",
    "                if os.path.exists(weather_file_path) and os.path.exists(demand_file_path):\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    demand_df = pd.read_csv(demand_file_path)\n",
    "                    \n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    demand_df['Date'] = pd.to_datetime(demand_df['Time_UTC']).dt.date\n",
    "                    \n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "                    grouped_demand = demand_df.groupby('Date')['Load'].agg(['sum'])  # Changed to sum\n",
    "                    \n",
    "                    # Merge to get Load sum values for dates above and below CI thresholds\n",
    "                    grouped = grouped.merge(grouped_demand, left_index=True, right_index=True, how='left')\n",
    "                    \n",
    "                    ci_max_month = ci_max_df[(ci_max_df['rb'] == rb_code)]\n",
    "                    ci_min_month = ci_min_df[(ci_min_df['rb'] == rb_code)]\n",
    "                    \n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]]\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]]\n",
    "                    \n",
    "                    for date, row in dates_above_max_ci.iterrows():\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['sum']})\n",
    "                    \n",
    "                    for date, row in dates_below_min_ci.iterrows():\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['sum']})\n",
    "        \n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "        \n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_demand_outliers_max_{case}.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_demand_outliers_min_{case}.csv'), index=False)\n",
    "\n",
    "\n",
    "    # Directories initialization\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/averaged_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier_demand'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    states=['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "            'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "            'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "            'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "            'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming','usa']\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    extreme_outliers_max_list = []\n",
    "    extreme_outliers_min_list = []\n",
    "\n",
    "    for state in states:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "        ci_max_path = os.path.join(ci_directory, f'{state}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{state}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "\n",
    "            \n",
    "            for year in years:\n",
    "                all_rb_dfs = []\n",
    "                weather_file_path = os.path.join(directory, f'{state}_averaged_weather_{year}.csv')\n",
    "                if os.path.exists(weather_file_path):\n",
    "                    if state == 'usa':\n",
    "                        # If state is USA, set rb_list to include all RB codes from p1 to p134\n",
    "                        rb_list = [f'p{i}' for i in range(1, 135)]\n",
    "                    else:\n",
    "                        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "                        state_to_ba_mapping=state_to_ba_mapping[state_to_ba_mapping['state']==state]\n",
    "                        \n",
    "                        rb_list = string_to_set(state_to_ba_mapping.iloc[0]['reeds_ba_list'])\n",
    "                    for rb_code in list(rb_list):\n",
    "                        demand_file_path = f'/Volumes/T7/prediction/{case}/{year}/{rb_code}_{year}_mlp_output.csv'\n",
    "                        demand_df = pd.read_csv(demand_file_path)\n",
    "                    \n",
    "                        demand_df['Date'] = pd.to_datetime(demand_df['Time_UTC']).dt.date\n",
    "                        \n",
    "                        grouped_demand = demand_df.groupby('Date')['Load'].agg(['sum']).rename(columns={'sum': rb_code})  # Rename to allow identification\n",
    "\n",
    "                        all_rb_dfs.append(grouped_demand)  \n",
    "                    if all_rb_dfs:\n",
    "                        total_load_df = pd.concat(all_rb_dfs, axis=1)\n",
    "\n",
    "                        # Sum across all numerical columns for each date to calculate 'Total_Load'\n",
    "                        total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
    "\n",
    "                        total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
    "                        \n",
    "                        \n",
    "\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    # Find the dates with the highest and lowest T2 for each day\n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "\n",
    "                    total_load_df.set_index('Date', inplace=True)\n",
    "\n",
    "                    # Merge to get Load sum values for dates above and below CI thresholds\n",
    "                    grouped = grouped.merge(total_load_df, left_index=True, right_index=True, how='left')\n",
    "                        \n",
    "                    # Only proceed if the month matches\n",
    "                    ci_max_month = ci_max_df[ (ci_max_df['State'] == state)]\n",
    "                    ci_min_month = ci_min_df[ (ci_min_df['State'] == state)]\n",
    "                        \n",
    "                    \n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]]\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]]\n",
    "\n",
    " \n",
    "                    for date, row in dates_above_max_ci.iterrows():\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['Total_Load']})\n",
    "                    \n",
    "                    for date, row in dates_below_min_ci.iterrows():\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['Total_Load']})\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{state}_extreme_demand_outliers_max_{case}.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{state}_extreme_demand_outliers_min_{case}.csv'), index=False)\n",
    "    # Initialize an empty DataFrame to store all results\n",
    "    all_results_df = pd.DataFrame()\n",
    "    folder = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier_demand'\n",
    "\n",
    "    # List all \"max\" files in the folder\n",
    "    max_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_demand_outliers_max_{case}.csv')]\n",
    "\n",
    "    for file in max_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_demand_outliers_max_{case}.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        # Assuming 'Total_Load' is a column in 'df'\n",
    "        # Group by 'Year' and calculate the average 'Total_Load' for each year\n",
    "        year_average_total_load = df.groupby('Year')['Load_sum'].mean().reset_index(name='average_total_load')\n",
    "\n",
    "        # Add the region to the DataFrame\n",
    "        year_average_total_load['region'] = region\n",
    "\n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, year_average_total_load], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'average_total_load']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"max\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_max_outliers_demand_summary_{case}.csv', index=False)\n",
    "    all_results_df= pd.DataFrame()\n",
    "\n",
    "\n",
    "    # List all \"min\" files in the folder\n",
    "    min_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_demand_outliers_min_{case}.csv')]\n",
    "\n",
    "    for file in min_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_demand_outliers_min_{case}.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        print(df.groupby('Year')['Load_sum'].mean())\n",
    "        # Group by 'Year' and calculate the average 'Total_Load' for each year\n",
    "        year_average_total_load = df.groupby('Year')['Load_sum'].mean().reset_index(name='average_total_load')\n",
    "\n",
    "        # Add the region to the DataFrame\n",
    "        year_average_total_load['region'] = region\n",
    "\n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, year_average_total_load], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'average_total_load']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"min\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_min_outliers_demand_summary_{case}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   st  year           GWh  Growth Rate\n",
      "0  AL  2010  90862.952503          NaN\n",
      "1  AL  2011  88995.780682    -0.020549\n",
      "2  AL  2012  86183.388699    -0.031601\n",
      "3  AL  2013  87854.727290     0.019393\n",
      "4  AL  2014  90496.828400     0.030074\n",
      "       RB  Mean Growth Rate\n",
      "0      p1          0.000572\n",
      "1     p10          0.004970\n",
      "2    p100          0.011901\n",
      "3    p101          0.010893\n",
      "4    p102          0.010893\n",
      "..    ...               ...\n",
      "129   p95          0.002126\n",
      "130   p96          0.002126\n",
      "131   p97          0.003322\n",
      "132   p98          0.003322\n",
      "133   p99          0.011901\n",
      "\n",
      "[134 rows x 2 columns]\n",
      "All RBs from P1 to P134 are present.\n",
      "All RBs in the DataFrame are unique.\n",
      "       RB  Mean Growth Rate\n",
      "0      p1      6.533773e-08\n",
      "1     p10      5.673265e-07\n",
      "2    p100      1.358571e-06\n",
      "3    p101      1.243473e-06\n",
      "4    p102      1.243473e-06\n",
      "..    ...               ...\n",
      "129   p95      2.427114e-07\n",
      "130   p96      2.427114e-07\n",
      "131   p97      3.792140e-07\n",
      "132   p98      3.792140e-07\n",
      "133   p99      1.358571e-06\n",
      "\n",
      "[134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# There was an issue accessing the file; let's try reading the file again to proceed with the calculations.\n",
    "import pandas as pd\n",
    "# Read the state to rb mapping\n",
    "def string_to_set(string):\n",
    "    # Remove curly braces and split the string\n",
    "    elements = string.replace('{', '').replace('}', '').split(',')\n",
    "    # Remove any extra whitespace and single quotes from each element\n",
    "    elements = [e.strip().replace(\"'\", \"\") for e in elements]\n",
    "    return set(elements)\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/ansonkong/Desktop/EIA_loadbystate.csv')\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has 'Year' and 'GWh' columns\n",
    "total_gwh_by_year = df.groupby('year')['GWh'].sum().reset_index()\n",
    "total_gwh_by_year['Growth Rate'] = total_gwh_by_year['GWh'].pct_change() * 100\n",
    "total_gwh_by_year[['year','Growth Rate']]\n",
    "# Calculate the total GWh by State and Year\n",
    "gwh_by_state_year = df.groupby(['st', 'year'])['GWh'].sum().reset_index()\n",
    "\n",
    "# Sort the values by State and then Year to ensure the calculation is correct\n",
    "gwh_by_state_year.sort_values(by=['st', 'year'], inplace=True)\n",
    "\n",
    "# Calculate the growth rate by state and year\n",
    "gwh_by_state_year['Growth Rate'] = gwh_by_state_year.groupby('st')['GWh'].pct_change()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(gwh_by_state_year.head())\n",
    "\n",
    "# Calculate the mean growth rate for each state\n",
    "mean_growth_rate_by_state = gwh_by_state_year.groupby('st')['Growth Rate'].mean().reset_index()\n",
    "\n",
    "state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "    \n",
    "# For each state, sum the relevant 'rb' daily sums\n",
    "for index, row in state_to_ba_mapping.iterrows():\n",
    "    rb_list = string_to_set(row['reeds_ba_list'])  # Convert string to list/set of rb codes\n",
    "    state_column = row['state']\n",
    "    # Ensure only existing columns are summed\n",
    "    rb_columns = [rb for rb in rb_list]\n",
    "# State abbreviation mapping\n",
    "state_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
    "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
    "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
    "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "# Convert full state names to abbreviations in state_to_ba_mapping\n",
    "state_to_ba_mapping['state'] = state_to_ba_mapping['state'].apply(lambda x: state_abbrev.get(x, x))\n",
    "\n",
    "# Prepare an empty DataFrame for aggregated RB growth rates\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming mean_growth_rate_by_state and state_to_ba_mapping are already loaded\n",
    "# And state_to_ba_mapping['state'] has been adjusted to use abbreviations as per the state_abbrev mapping\n",
    "\n",
    "# Preparing an empty list for aggregated RB growth rates\n",
    "rows = []\n",
    "\n",
    "# Iterate over each state in mean_growth_rate_by_state\n",
    "for state_abbr in mean_growth_rate_by_state['st'].unique():\n",
    "    # Find the RB(s) associated with this state\n",
    "    rb_set = set()\n",
    "    for index, row in state_to_ba_mapping.iterrows():\n",
    "        if state_abbr==row['state']:\n",
    "            rb_set=string_to_set(row['reeds_ba_list'])  # Assuming there's an 'rb' column indicating the RB\n",
    "            break\n",
    "    \n",
    "    # Calculate mean growth rate for this state (and its associated RBs)\n",
    "    mean_growth_for_state = mean_growth_rate_by_state[mean_growth_rate_by_state['st'] == state_abbr]['Growth Rate'].mean()\n",
    "    \n",
    "    # For each RB associated with the state, add a new row to the list\n",
    "    # This approach might add multiple rows for the same RB with the same growth rate if multiple states map to it\n",
    "    for rb in rb_set:\n",
    "        rows.append({'RB': rb, 'Mean Growth Rate': mean_growth_for_state})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "rb_growth_rates = pd.DataFrame(rows)\n",
    "\n",
    "# Since multiple states can map to the same RB, resulting in duplicate RB entries,\n",
    "# we may need to aggregate these to get a unique mean growth rate per RB\n",
    "unique_rb_growth_rates = rb_growth_rates.groupby('RB')['Mean Growth Rate'].mean().reset_index()\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(unique_rb_growth_rates)\n",
    "\n",
    "# Generate the list of expected RBs from P1 to P134\n",
    "expected_rbs = [f'p{i}' for i in range(1, 135)]\n",
    "\n",
    "# Extract the list of RBs from the unique_rb_growth_rates DataFrame\n",
    "actual_rbs = unique_rb_growth_rates['RB'].unique().tolist()\n",
    "\n",
    "# Check for missing RBs\n",
    "missing_rbs = set(expected_rbs) - set(actual_rbs)\n",
    "\n",
    "# Verify if any RB is missing\n",
    "if missing_rbs:\n",
    "    print(f\"Missing RBs: {missing_rbs}\")\n",
    "else:\n",
    "    print(\"All RBs from P1 to P134 are present.\")\n",
    "\n",
    "# Check for duplicates to ensure uniqueness of RBs in unique_rb_growth_rates\n",
    "if len(actual_rbs) != unique_rb_growth_rates.shape[0]:\n",
    "    print(\"There are duplicate RBs in the DataFrame.\")\n",
    "else:\n",
    "    print(\"All RBs in the DataFrame are unique.\")\n",
    "\n",
    "\n",
    "# Calculate the new 'Mean Growth Rate' by raising it to the power of 1/(365*24)\n",
    "unique_rb_growth_rates['Mean Growth Rate'] = unique_rb_growth_rates['Mean Growth Rate'].apply(lambda x: x/(365*24))\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(unique_rb_growth_rates)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "# Your setup\n",
    "cases = ['projection']\n",
    "years = range(2020, 2100)\n",
    "rb_range = range(1, 135)  # Assuming RBs are numbered continuously from P1 to P134\n",
    "\n",
    "# Reference datetime for calculating the hour difference\n",
    "reference_datetime = datetime(2010, 1, 1)\n",
    "\n",
    "# Loop over each combination of case, RB, and year\n",
    "for case in cases:\n",
    "    for year in years:\n",
    "        for rb in rb_range:\n",
    "            rb_id = f'p{rb}'\n",
    "            file_path = f'/Volumes/T7/prediction/{case}/{year}/{rb_id}_{year}_mlp_output.csv'\n",
    "            \n",
    "            try:\n",
    "                # Attempt to read the current CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Find the 'Mean Growth Rate' for the current RB from unique_rb_growth_rates DataFrame\n",
    "                growth_rate = unique_rb_growth_rates.loc[unique_rb_growth_rates['RB'] == rb_id, 'Mean Growth Rate'].iloc[0]\n",
    "                \n",
    "                # Convert 'Time_UTC' to datetime and calculate the hour difference from the reference\n",
    "                df['Time_UTC'] = pd.to_datetime(df['Time_UTC'])\n",
    "                df['Hour Difference'] = df['Time_UTC'].apply(lambda x: (x - reference_datetime).total_seconds() / 3600)\n",
    "                \n",
    "                # Transform 'Load' based on the growth rate and time difference\n",
    "                df['Load'] = df.apply(lambda row: row['Load'] * (1 + growth_rate) ** row['Hour Difference'], axis=1)\n",
    "                \n",
    "                # Define the path to save the transformed DataFrame\n",
    "                save_path = f'/Volumes/T7/prediction_project/{case}/{year}/{rb_id}_{year}_mlp_output_transformed.csv'\n",
    "                \n",
    "                # Create directories if they don't exist\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                \n",
    "                # Save the transformed DataFrame\n",
    "                df.to_csv(save_path, index=False)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f'File not found: {file_path}')\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m First\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rb_code \u001b[38;5;129;01min\u001b[39;00m rb_codes:\n\u001b[0;32m--> 252\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_rb_files_daily\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrb_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43myear\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m First:\n\u001b[1;32m    254\u001b[0m         processed_df\u001b[38;5;241m=\u001b[39mtemp_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mprocess_rb_files_daily\u001b[0;34m(rb_code, years)\u001b[0m\n\u001b[1;32m    201\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file_name)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m--> 203\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_UTC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    204\u001b[0m     temp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_UTC\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(temp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_UTC\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Additional grouping by day\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1612\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_file_or_buffer(f, engine)\n\u001b[0;32m-> 1612\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1809\u001b[0m, in \u001b[0;36mTextFileReader._clean_options\u001b[0;34m(self, options, engine)\u001b[0m\n\u001b[1;32m   1807\u001b[0m keep_default_na \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_default_na\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1808\u001b[0m floatify \u001b[38;5;241m=\u001b[39m engine \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1809\u001b[0m na_values, na_fvalues \u001b[38;5;241m=\u001b[39m \u001b[43m_clean_na_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloatify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloatify\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;66;03m# handle skiprows; this is internally handled by the\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;66;03m# c-engine, so only need for python and pyarrow parsers\u001b[39;00m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:2088\u001b[0m, in \u001b[0;36m_clean_na_values\u001b[0;34m(na_values, keep_default_na, floatify)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         na_values \u001b[38;5;241m=\u001b[39m na_values \u001b[38;5;241m|\u001b[39m STR_NA_VALUES\n\u001b[1;32m   2086\u001b[0m     na_fvalues \u001b[38;5;241m=\u001b[39m _floatify_na_values(na_values)\n\u001b[0;32m-> 2088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m na_values, na_fvalues\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "for case in ['projection']:\n",
    "    # Define the directory where the CSV files are located\n",
    "    directory = f'/Volumes/T7/prediction_project/{case}'\n",
    "\n",
    "    # Create a list of all 'rb' codes and years\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    # Function to read and process files for a given rb code\n",
    "    def process_rb_files(rb_code):\n",
    "        rb_df_list = []\n",
    "        # Loop through all years for the given rb_code\n",
    "        for year in years:\n",
    "            # Construct file name\n",
    "            folder_path = os.path.join(directory, str(year))\n",
    "            file_name = f\"{rb_code}_{year}_mlp_output_transformed.csv\"\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Check if file exists to avoid FileNotFoundError\n",
    "            if os.path.isfile(file_path):\n",
    "                # Read the CSV file\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                temp_df=temp_df[['Time_UTC','Load']]\n",
    "                # Convert 'Time_UTC' to datetime and extract 'Year' and 'Month'\n",
    "                temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "                temp_df['Year'] = temp_df['Time_UTC'].dt.year\n",
    "                temp_df['Month'] = temp_df['Time_UTC'].dt.month\n",
    "                # Aggregate Load by 'Year' and 'Month'\n",
    "                aggregated_df = temp_df.groupby(['Year', 'Month'])['Load'].sum().reset_index()\n",
    "                # Add the aggregated data to the list\n",
    "                rb_df_list.append(aggregated_df)\n",
    "    \n",
    "        # Vertically concatenate all yearly data for the rb code\n",
    "        concatenated_df = pd.concat(rb_df_list, ignore_index=True)\n",
    "        return concatenated_df\n",
    "\n",
    "    years = list(range(2020, 2100))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "    # Create a DataFrame with all combinations of 'Year' and 'Month'\n",
    "    year_month_df = pd.DataFrame([(y, m) for y in years for m in months], columns=['Year', 'Month'])\n",
    "\n",
    "    # Placeholder to simulate merging data for multiple rb codes\n",
    "    final_df = year_month_df.copy()\n",
    "    for rb_code in rb_codes: \n",
    "        rb_df = process_rb_files(rb_code)\n",
    "        final_df = final_df.merge(rb_df, on=['Year', 'Month'], how='left').rename(columns={'Load': rb_code})\n",
    "    # Placeholder function to read 'state_to_ba_mapping.csv' and perform aggregation\n",
    "    def string_to_set(string):\n",
    "        # Remove curly braces and split the string\n",
    "        elements = string.replace('{', '').replace('}', '').split(',')\n",
    "        # Remove any extra whitespace and single quotes from each element\n",
    "        elements = [e.strip().replace(\"'\", \"\") for e in elements]\n",
    "        return set(elements)\n",
    "    def aggregate_state_data(final_df):\n",
    "        # Placeholder to simulate reading the 'state_to_ba_mapping.csv' file\n",
    "        # In practice, this should read the actual file\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "\n",
    "        # Add columns for each state by summing the relevant rb columns based on the mapping\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])\n",
    "            state_column = row['state']\n",
    "            # Sum the columns specified in reeds_ba_list for each state\n",
    "            final_df[state_column] = final_df[list(rb_list)].sum(axis=1)\n",
    "\n",
    "        # Calculate the total for the USA by summing all state columns\n",
    "        state_columns = state_to_ba_mapping['state'].values\n",
    "        final_df['USA'] = final_df[state_columns].sum(axis=1)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    # Apply the placeholder aggregation function to the simulated final_df\n",
    "    final_aggregated_df = aggregate_state_data(final_df)\n",
    "\n",
    "    def aggregate_state_data(final_df):\n",
    "        # Placeholder to simulate reading the 'state_to_ba_mapping.csv' file\n",
    "        # In practice, this should read the actual file\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "\n",
    "        # Add columns for each state by summing the relevant rb columns based on the mapping\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])\n",
    "            state_column = row['state']\n",
    "            # Sum the columns specified in reeds_ba_list for each state\n",
    "            final_df[state_column] = final_df[list(rb_list)].sum(axis=1)\n",
    "\n",
    "        # Calculate the total for the USA by summing all state columns\n",
    "        state_columns = state_to_ba_mapping['state'].values\n",
    "        final_df['USA'] = final_df[state_columns].sum(axis=1)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    # Apply the placeholder aggregation function to the simulated final_df\n",
    "    final_aggregated_df = aggregate_state_data(final_df)\n",
    "    final_aggregated_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/_project_mock_{case}.csv')\n",
    "    # Define the directory where the CSV files are located\n",
    "    directory = f'/Volumes/T7/prediction_project/{case}'\n",
    "\n",
    "    # Create a list of all 'rb' codes and years\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = list(range(2020, 2100))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "\n",
    "    def process_rb_files_daily(rb_code, years):\n",
    "        rb_df_list = []\n",
    "        for year in years:\n",
    "            folder_path = os.path.join(directory, str(year))\n",
    "            file_name = f\"{rb_code}_{year}_mlp_output_transformed.csv\"\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                temp_df = pd.read_csv(file_path)[['Time_UTC', 'Load']]\n",
    "                temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "                rb_df_list.append(temp_df)\n",
    "        concatenated_df = pd.concat(rb_df_list, ignore_index=True)\n",
    "        return concatenated_df\n",
    "    def aggregate_state_and_usa_daily(final_rb_df):\n",
    "        # Read the state to rb mapping\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "        \n",
    "        # For each state, sum the relevant 'rb' daily sums\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])  # Convert string to list/set of rb codes\n",
    "            state_column = row['state']\n",
    "            # Ensure only existing columns are summed\n",
    "            rb_columns = [rb for rb in rb_list if rb in final_rb_df.columns]\n",
    "            # Summing for each state\n",
    "            final_rb_df[state_column] = final_rb_df[rb_columns].sum(axis=1)\n",
    "        \n",
    "        # Summing all states to get USA total\n",
    "        state_columns = state_to_ba_mapping['state'].tolist()\n",
    "        final_rb_df['USA'] = final_rb_df[state_columns].sum(axis=1)\n",
    "        \n",
    "        return final_rb_df\n",
    "\n",
    "\n",
    "\n",
    "    final_dfs = []\n",
    "    for year in years:\n",
    "        # Process data for the year (pseudo-code placeholders for actual processing)\n",
    "        First=True\n",
    "        for rb_code in rb_codes:\n",
    "            temp_df = process_rb_files_daily(rb_code, [year])\n",
    "            if First:\n",
    "                processed_df=temp_df.copy()\n",
    "                First=False\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "            else:\n",
    "                # Assuming processed_df and temp_df are defined and have the appropriate columns\n",
    "                processed_df = processed_df.merge(temp_df, on=['Time_UTC'], how='inner')\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "        processed_df = aggregate_state_and_usa_daily(processed_df)\n",
    "        processed_df['Time_UTC'] = pd.to_datetime(processed_df['Time_UTC'])\n",
    "        processed_df['Year'] = processed_df['Time_UTC'].dt.year\n",
    "        processed_df['Month'] = processed_df['Time_UTC'].dt.month\n",
    "\n",
    "\n",
    "        processed_df=processed_df.drop(['Time_UTC'], axis=1)\n",
    "        # Group by 'weekday' and 'Year', then apply the aggregations\n",
    "        # Define aggregations to apply 'max' to all columns except 'Year', 'Month', and 'weekday'\n",
    "        aggregations = {col: 'max' for col in processed_df.columns if col not in ['Year', 'Month']}\n",
    "\n",
    "        # Group by 'Year' and 'Month', then aggregate\n",
    "        grouped_df = processed_df.groupby(['Year', 'Month']).agg(aggregations)\n",
    "\n",
    "\n",
    "\n",
    "        # Reset index to turn 'Year' and 'Month' back into columns if needed\n",
    "        grouped_df.reset_index(inplace=True)\n",
    "        final_dfs.append(grouped_df)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    final_df = pd.concat(final_dfs, axis=0, ignore_index=True)\n",
    "    final_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/_project_max_{case}_monthlly.csv')\n",
    "    final_df\n",
    "\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "\n",
    "    # Define the directory where the CSV files are located\n",
    "    directory = f'/Volumes/T7/prediction_project/{case}'\n",
    "\n",
    "    # Create a list of all 'rb' codes and years\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = list(range(2020, 2100))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "\n",
    "    def process_rb_files_daily(rb_code, years):\n",
    "        rb_df_list = []\n",
    "        for year in years:\n",
    "            folder_path = os.path.join(directory, str(year))\n",
    "            file_name = f\"{rb_code}_{year}_mlp_output_transformed.csv\"\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                temp_df = pd.read_csv(file_path)[['Time_UTC', 'Load']]\n",
    "                temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "                # Additional grouping by day\n",
    "                temp_df['Day'] = temp_df['Time_UTC'].dt.day\n",
    "                temp_df['Year'] = temp_df['Time_UTC'].dt.year\n",
    "                temp_df['Month'] = temp_df['Time_UTC'].dt.month\n",
    "                aggregated_df = temp_df.groupby(['Year', 'Month', 'Day'])['Load'].sum().reset_index()\n",
    "                rb_df_list.append(aggregated_df)\n",
    "        concatenated_df = pd.concat(rb_df_list, ignore_index=True)\n",
    "        return concatenated_df\n",
    "    def aggregate_state_and_usa_daily(final_rb_df):\n",
    "        # Read the state to rb mapping\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "        \n",
    "        # For each state, sum the relevant 'rb' daily sums\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])  # Convert string to list/set of rb codes\n",
    "            state_column = row['state']\n",
    "            # Ensure only existing columns are summed\n",
    "            rb_columns = [rb for rb in rb_list if rb in final_rb_df.columns]\n",
    "            # Summing for each state\n",
    "            final_rb_df[state_column] = final_rb_df[rb_columns].sum(axis=1)\n",
    "        \n",
    "        # Summing all states to get USA total\n",
    "        state_columns = state_to_ba_mapping['state'].tolist()\n",
    "        final_rb_df['USA'] = final_rb_df[state_columns].sum(axis=1)\n",
    "        \n",
    "        return final_rb_df\n",
    "\n",
    "    def upper_95(x):\n",
    "        return x.quantile(0.95)\n",
    "\n",
    "    def lower_5(x):\n",
    "        return x.quantile(0.05)\n",
    "    # Define a function to reorganize each column into an array grouped by Year\n",
    "    def reorganize_into_array(series):\n",
    "        # Pre-fill the array with NaNs to handle missing weekdays\n",
    "        array = [np.nan] * 7  # One entry for each day of the week\n",
    "        for idx, value in series.items():\n",
    "            # idx is a tuple (Year, weekday), value is the column value\n",
    "            # Subtract 1 from idx[1] if weekday starts from 1 in your dataset\n",
    "            array[idx[1]] = value  # Or idx[1] - 1 if weekday is 1-based\n",
    "        return array\n",
    "\n",
    "    final_dfs = []\n",
    "    for year in years:\n",
    "        # Process data for the year (pseudo-code placeholders for actual processing)\n",
    "        First=True\n",
    "        for rb_code in rb_codes:\n",
    "            temp_df = process_rb_files_daily(rb_code, [year])\n",
    "            if First:\n",
    "                processed_df=temp_df.copy()\n",
    "                First=False\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "            else:\n",
    "                # Assuming processed_df and temp_df are defined and have the appropriate columns\n",
    "                processed_df = processed_df.merge(temp_df, on=['Year', 'Month', 'Day'], how='inner')\n",
    "                processed_df[rb_code]=processed_df['Load']\n",
    "                processed_df=processed_df.drop(['Load'], axis=1)\n",
    "        processed_df = aggregate_state_and_usa_daily(processed_df)\n",
    "        processed_df['Date'] = pd.to_datetime(processed_df[['Year', 'Month', 'Day']])\n",
    "\n",
    "        # Create the 'weekday' column, note that dt.dayofweek returns Monday=0 through Sunday=6, so we add 1\n",
    "        processed_df['weekday'] = processed_df['Date'].dt.dayofweek \n",
    "        processed_df=processed_df.drop(['Month','Day','Date'], axis=1)\n",
    "        # Group by 'weekday' and 'Year', then apply the aggregations\n",
    "        aggregations = {col: ['mean','max', upper_95, lower_5] for col in processed_df.columns if col not in ['Year', 'weekday']}\n",
    "        grouped_df = processed_df.groupby(['Year', 'weekday']).agg(aggregations)\n",
    "        # Now, flatten the MultiIndex in columns created by aggregation\n",
    "        grouped_df.columns = ['{}_{}'.format(col[0], col[1]) for col in grouped_df.columns]\n",
    "\n",
    "        # To further match your requirement, rename the aggregation methods in column names\n",
    "        grouped_df.rename(columns=lambda x: x.replace('mean', 'mean').replace('upper_95', 'upper').replace('lower_5', 'lower'), inplace=True)\n",
    "\n",
    "        # Reset index to turn 'Year' and 'weekday' back into columns if needed\n",
    "        grouped_df= grouped_df.reset_index()\n",
    "        # First, let's ensure 'weekday' is in the correct data type if not already\n",
    "        grouped_df['weekday'] = grouped_df['weekday'].astype(int)\n",
    "        final_dfs.append(grouped_df)\n",
    "        \n",
    "\n",
    "    final_df = pd.concat(final_dfs, axis=0, ignore_index=True)\n",
    "    final_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/_project_mock_{case}_weekly.csv')\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # Define the directory where the CSV files are located and other initial setups\n",
    "    directory = f'/Volumes/T7/prediction_project/{case}'\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = list(range(2020, 2100))\n",
    "\n",
    "    def string_to_set(string):\n",
    "        elements = string.replace('{', '').replace('}', '').split(',')\n",
    "        return set(e.strip().replace(\"'\", \"\") for e in elements)\n",
    "\n",
    "    def upper_95(x):\n",
    "        return x.quantile(0.95)\n",
    "\n",
    "    def lower_5(x):\n",
    "        return x.quantile(0.05)\n",
    "\n",
    "    def process_rb_files_hourly(rb_code, year):\n",
    "        file_name = f\"{rb_code}_{year}_mlp_output_transformed.csv\"\n",
    "        file_path = os.path.join(directory, str(year), file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            temp_df = pd.read_csv(file_path, usecols=['Time_UTC', 'Load'])\n",
    "            temp_df['Time_UTC'] = pd.to_datetime(temp_df['Time_UTC'])\n",
    "            temp_df[rb_code]=temp_df['Load']\n",
    "            # temp_df['Hour'] = temp_df['Time_UTC'].dt.hour\n",
    "            # temp_df['Year'] = temp_df['Time_UTC'].dt.year\n",
    "            # temp_df['Weekend_or_Weekday'] = np.where(temp_df['Time_UTC'].dt.dayofweek < 5, 'Weekday', 'Weekend')\n",
    "            # temp_df['Day'] = temp_df['Time_UTC'].dt.day\n",
    "            # temp_df['Month'] = temp_df['Time_UTC'].dt.month\n",
    "            temp_df.drop(columns=['Load'], inplace=True)\n",
    "            return temp_df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def aggregate_state_and_usa_daily(df, state_to_ba_mapping):\n",
    "        for index, row in state_to_ba_mapping.iterrows():\n",
    "            rb_list = string_to_set(row['reeds_ba_list'])\n",
    "            state_column = row['state']\n",
    "            df[state_column] = df[[col for col in df.columns if col in rb_list]].sum(axis=1)\n",
    "        df['USA'] = df[[row['state'] for index, row in state_to_ba_mapping.iterrows()]].sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def process_yearly_data(year):\n",
    "        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "        First = True\n",
    "\n",
    "        for rb_code in rb_codes:\n",
    "            temp_df = process_rb_files_hourly(rb_code, year)\n",
    "            if First:\n",
    "                processed_df=temp_df.copy()\n",
    "                First=False\n",
    "            else:\n",
    "                processed_df = processed_df.merge(temp_df, on=['Time_UTC'], how='inner')\n",
    "        yearly_data=processed_df \n",
    "\n",
    "        yearly_data = aggregate_state_and_usa_daily(yearly_data, state_to_ba_mapping)\n",
    "        yearly_data['Hour'] = yearly_data['Time_UTC'].dt.hour\n",
    "        yearly_data['Year'] = yearly_data['Time_UTC'].dt.year\n",
    "        yearly_data['Weekend_or_Weekday'] = np.where(yearly_data['Time_UTC'].dt.dayofweek < 5, 'Weekday', 'Weekend')\n",
    "        yearly_data.drop(columns=['Time_UTC'], inplace=True)\n",
    "        columns_to_aggregate = [col for col in yearly_data.columns if col not in ['Hour', 'Year', 'Weekend_or_Weekday']]\n",
    "        aggregations = {col: ['mean', upper_95, lower_5,'max'] for col in columns_to_aggregate}\n",
    "            \n",
    "        # Group by 'Hour', 'Year', 'Weekend_or_Weekday', then aggregate\n",
    "        grouped_yearly_data = yearly_data.groupby(['Hour', 'Year', 'Weekend_or_Weekday']).agg(aggregations)\n",
    "        # Flatten the MultiIndex columns if you have multiple columns being aggregated\n",
    "        grouped_yearly_data.columns = ['_'.join(col).strip() for col in grouped_yearly_data.columns.values]\n",
    "\n",
    "        # Reset index if you want 'Hour', 'Year', 'Weekend_or_Weekday' back as regular columns\n",
    "        grouped_yearly_data.reset_index(inplace=True)\n",
    "        grouped_yearly_data.rename(columns=lambda x: x.replace('mean', 'mean').replace('upper_95', 'upper').replace('lower_5', 'lower'), inplace=True)\n",
    "\n",
    "        return grouped_yearly_data\n",
    "\n",
    "    final_dfs = []\n",
    "    for year in years:\n",
    "        yearly_data = process_yearly_data(year)\n",
    "        if not yearly_data.empty:\n",
    "            final_dfs.append(yearly_data)\n",
    "\n",
    "    final_df = pd.concat(final_dfs, ignore_index=True)\n",
    "    final_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/_project_mock_{case}_yearly_aggregated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_54211/1109302568.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty file: Wyoming_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Wisconsin_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p20_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p26_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Nebraska_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New Mexico_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p21_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p27_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Oregon_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p131_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p47_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Arkansas_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p41_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Oklahoma_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p60_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: North Dakota_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p111_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New Jersey_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p130_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Pennsylvania_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p46_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p40_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p67_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p61_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p15_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p38_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p13_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New Hampshire_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p34_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p32_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p80_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p19_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p2_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p4_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p8_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: South Dakota_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: California_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p14_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p39_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p12_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p35_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p33_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p81_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p18_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p3_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Idaho_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p5_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Iowa_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p9_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p129_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p104_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p108_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p78_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p53_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p55_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Illinois_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p72_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p59_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p74_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p103_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p128_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p105_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p122_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Indiana_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p79_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p52_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p54_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p73_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p75_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Texas_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Massachusetts_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p29_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p23_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p25_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Minnesota_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Ohio_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p28_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Nevada_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Maine_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p22_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p24_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p113_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New York_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p134_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p119_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p132_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p44_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p69_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p42_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p65_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Connecticut_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p63_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p48_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p112_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p133_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p45_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p68_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Colorado_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p43_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p64_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Arizona_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p62_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p49_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p16_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p10_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Montana_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: usa_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p37_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p85_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Rhode Island_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p31_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p83_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p1_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p7_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p17_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p11_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p36_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p84_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p30_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p82_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p6_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p107_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p126_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Washington_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p50_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p56_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Kansas_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p71_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Utah_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p77_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p106_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p127_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p51_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p57_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Vermont_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p70_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Missouri_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p76_extreme_demand_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Michigan_extreme_demand_outliers_max_projection_project_.csv\n",
      "Year\n",
      "2055    239577.864935\n",
      "2056    244207.547077\n",
      "2057    246216.726744\n",
      "2058    250468.211267\n",
      "2059    254438.916875\n",
      "2060    254720.928441\n",
      "2061    255752.938728\n",
      "2062    257542.444279\n",
      "2063    261524.686058\n",
      "2064    265576.323187\n",
      "2065    269659.525374\n",
      "2066    273377.273512\n",
      "2067    277409.759966\n",
      "2068    279908.191362\n",
      "2069    283876.961033\n",
      "2070    288193.348080\n",
      "2071    292643.533102\n",
      "2072    297525.224337\n",
      "2073    302674.110487\n",
      "2074    307429.872667\n",
      "2075    312753.284774\n",
      "2076    318147.329877\n",
      "2077    321971.667929\n",
      "2078    327389.671849\n",
      "2079    332697.579619\n",
      "2080    337611.577900\n",
      "2081    343077.554998\n",
      "2082    347605.023392\n",
      "2083    352916.798275\n",
      "2084    358966.806048\n",
      "2085    363784.361971\n",
      "2086    369092.735836\n",
      "2087    372273.345877\n",
      "2088    378111.568877\n",
      "2089    384629.516094\n",
      "2090    390097.631170\n",
      "2091    397765.221411\n",
      "2092    404534.789162\n",
      "2093    412422.887804\n",
      "2094    418564.288570\n",
      "2095    425946.097954\n",
      "2096    434174.678509\n",
      "2097    440382.906846\n",
      "2098    447314.279418\n",
      "2099    454540.991384\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2051    227771.367870\n",
      "2052    230123.821171\n",
      "2053    234728.790994\n",
      "2054    238990.357907\n",
      "2055    241121.815104\n",
      "2056    248405.717295\n",
      "2057    252724.113459\n",
      "2058    259134.206640\n",
      "2059    263355.571175\n",
      "2060    266395.707508\n",
      "2061    271813.447463\n",
      "2062    275040.993154\n",
      "2063    279922.154967\n",
      "2064    283668.873221\n",
      "2065    287918.399357\n",
      "2066    290360.880546\n",
      "2067    294881.905475\n",
      "2068    299648.548408\n",
      "2069    303542.868528\n",
      "2070    307975.563209\n",
      "2071    312047.480294\n",
      "2072    315978.153096\n",
      "2073    320176.597049\n",
      "2074    324841.646273\n",
      "2075    328681.077222\n",
      "2076    333350.516818\n",
      "2077    336607.124811\n",
      "2078    341077.148905\n",
      "2079    344163.328367\n",
      "2080    349630.549536\n",
      "2081    353914.141154\n",
      "2082    358148.714222\n",
      "2083    361933.777207\n",
      "2084    368207.741225\n",
      "2085    372882.369692\n",
      "2086    377678.947731\n",
      "2087    382647.205477\n",
      "2088    387377.135594\n",
      "2089    392620.312830\n",
      "2090    398012.329796\n",
      "2091    403626.746663\n",
      "2092    409482.899825\n",
      "2093    415229.341173\n",
      "2094    419923.354632\n",
      "2095    426273.334175\n",
      "2096    432567.835486\n",
      "2097    439233.380505\n",
      "2098    445245.627862\n",
      "2099    452276.682103\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2044    47448.190207\n",
      "2045    47185.693618\n",
      "2046    47376.029118\n",
      "2047    47597.437255\n",
      "2048    47903.176853\n",
      "2049    47502.997069\n",
      "2050    47419.832369\n",
      "2051    47661.901442\n",
      "2052    48095.487375\n",
      "2053    48334.171269\n",
      "2054    48675.012058\n",
      "2055    48854.217702\n",
      "2056    49133.379379\n",
      "2057    49443.717733\n",
      "2058    49733.505524\n",
      "2059    50070.464043\n",
      "2060    50328.027086\n",
      "2061    50549.536488\n",
      "2062    50747.278587\n",
      "2063    50781.342116\n",
      "2064    51057.451942\n",
      "2065    51287.504953\n",
      "2066    51408.353769\n",
      "2067    51629.872048\n",
      "2068    51854.400458\n",
      "2069    52205.142615\n",
      "2070    52530.869314\n",
      "2071    52821.255593\n",
      "2072    53118.712141\n",
      "2073    53439.182124\n",
      "2074    53747.034958\n",
      "2075    54099.598331\n",
      "2076    54599.025327\n",
      "2077    54771.504488\n",
      "2078    55088.840297\n",
      "2079    55417.913441\n",
      "2080    55833.604134\n",
      "2081    55857.735144\n",
      "2082    55512.465413\n",
      "2083    55924.116829\n",
      "2084    56107.386973\n",
      "2085    56455.504253\n",
      "2086    56824.832508\n",
      "2087    57205.070222\n",
      "2088    57498.381083\n",
      "2089    57773.885623\n",
      "2090    58250.182646\n",
      "2091    58573.576871\n",
      "2092    58949.377782\n",
      "2093    59358.883053\n",
      "2094    59263.800849\n",
      "2095    59161.392897\n",
      "2096    59320.085553\n",
      "2097    59859.144083\n",
      "2098    60444.509388\n",
      "2099    61026.319250\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Florida_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2036    109562.534992\n",
      "2037    110312.786603\n",
      "2038    112070.914912\n",
      "2039    114171.130988\n",
      "2040    116368.085235\n",
      "            ...      \n",
      "2095    318253.089303\n",
      "2096    325695.966551\n",
      "2097    333040.689989\n",
      "2098    340634.680958\n",
      "2099    348343.391269\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Skipping empty file: Mississippi_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2049    392579.733099\n",
      "2050    396450.935037\n",
      "2051    401830.179196\n",
      "2052    407109.291572\n",
      "2053    408941.898112\n",
      "2054    413074.143299\n",
      "2055    413184.946073\n",
      "2056    414837.713808\n",
      "2057    416615.601610\n",
      "2058    419622.564044\n",
      "2059    423083.131591\n",
      "2060    425243.346311\n",
      "2061    428551.561799\n",
      "2062    431503.944137\n",
      "2063    434588.067031\n",
      "2064    436081.826511\n",
      "2065    437728.926764\n",
      "2066    440267.032984\n",
      "2067    444046.015511\n",
      "2068    448141.635512\n",
      "2069    452306.255866\n",
      "2070    455904.511252\n",
      "2071    460166.264212\n",
      "2072    464183.671076\n",
      "2073    467983.266402\n",
      "2074    471837.363396\n",
      "2075    476142.885183\n",
      "2076    480058.379779\n",
      "2077    481317.731445\n",
      "2078    484698.826289\n",
      "2079    489016.851340\n",
      "2080    493737.702816\n",
      "2081    498257.261272\n",
      "2082    502699.305635\n",
      "2083    507060.629902\n",
      "2084    511362.124126\n",
      "2085    516212.395405\n",
      "2086    521857.672939\n",
      "2087    526980.061350\n",
      "2088    532045.462318\n",
      "2089    537253.503878\n",
      "2090    542640.037108\n",
      "2091    547859.614794\n",
      "2092    553015.444456\n",
      "2093    558102.302735\n",
      "2094    562774.231763\n",
      "2095    566433.104428\n",
      "2096    567159.410807\n",
      "2097    569697.043592\n",
      "2098    574419.575997\n",
      "2099    581006.630222\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2030     49757.707366\n",
      "2031     50441.861070\n",
      "2032     51232.836132\n",
      "2033     52445.212052\n",
      "2034     53203.335756\n",
      "            ...      \n",
      "2095    245216.754437\n",
      "2096    251945.947252\n",
      "2097    258691.957461\n",
      "2098    265684.008672\n",
      "2099    272818.590756\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Year\n",
      "2035     8111.127522\n",
      "2036     8025.988828\n",
      "2037     8116.248859\n",
      "2038     8084.216041\n",
      "2039     8071.124861\n",
      "            ...     \n",
      "2095    18467.740213\n",
      "2096    18841.238277\n",
      "2097    19202.476819\n",
      "2098    19580.780945\n",
      "2099    19968.160068\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2047    23535.289727\n",
      "2048    24637.317850\n",
      "2049    24821.835495\n",
      "2050    25158.781613\n",
      "2051    25616.676999\n",
      "2052    25872.900698\n",
      "2053    26012.315092\n",
      "2054    26405.221309\n",
      "2055    26829.169302\n",
      "2056    27295.725907\n",
      "2057    27741.523624\n",
      "2058    28242.412976\n",
      "2059    28721.768332\n",
      "2060    29203.907594\n",
      "2061    29619.820298\n",
      "2062    30111.505918\n",
      "2063    30585.891889\n",
      "2064    30985.338728\n",
      "2065    31513.342423\n",
      "2066    31887.278687\n",
      "2067    32381.624963\n",
      "2068    32956.827408\n",
      "2069    33357.175452\n",
      "2070    33920.982267\n",
      "2071    34416.236956\n",
      "2072    34963.903626\n",
      "2073    35539.865370\n",
      "2074    36153.342881\n",
      "2075    36801.109337\n",
      "2076    37429.754039\n",
      "2077    38078.390588\n",
      "2078    38663.221762\n",
      "2079    39369.019409\n",
      "2080    40051.686948\n",
      "2081    40804.536416\n",
      "2082    41401.479794\n",
      "2083    41908.394430\n",
      "2084    42107.698296\n",
      "2085    42816.359224\n",
      "2086    43573.057160\n",
      "2087    44317.821591\n",
      "2088    45007.872556\n",
      "2089    45770.758098\n",
      "2090    46620.903482\n",
      "2091    47400.100333\n",
      "2092    48255.341956\n",
      "2093    49139.641575\n",
      "2094    50059.778127\n",
      "2095    50861.614339\n",
      "2096    51855.865201\n",
      "2097    52217.257677\n",
      "2098    52780.600496\n",
      "2099    53549.287600\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Alabama_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2040    19140.797752\n",
      "2041    19443.878353\n",
      "2042    19652.315347\n",
      "2043    19915.890957\n",
      "2044    20210.685665\n",
      "2045    20507.987024\n",
      "2046    20785.550626\n",
      "2047    21100.449600\n",
      "2048    21435.151203\n",
      "2049    21739.615526\n",
      "2050    22096.881078\n",
      "2051    22378.135262\n",
      "2052    22573.126131\n",
      "2053    22854.725323\n",
      "2054    23142.508443\n",
      "2055    23416.919477\n",
      "2056    23756.968237\n",
      "2057    24055.265174\n",
      "2058    24337.481342\n",
      "2059    24673.551557\n",
      "2060    25049.462950\n",
      "2061    25375.304651\n",
      "2062    25746.578965\n",
      "2063    26048.299011\n",
      "2064    26396.353756\n",
      "2065    26807.230488\n",
      "2066    27016.748279\n",
      "2067    27374.644020\n",
      "2068    27712.989478\n",
      "2069    28165.430709\n",
      "2070    28520.021458\n",
      "2071    28956.009929\n",
      "2072    29379.439755\n",
      "2073    29757.390968\n",
      "2074    30172.397201\n",
      "2075    30251.544379\n",
      "2076    30506.304880\n",
      "2077    31090.204251\n",
      "2078    31724.775328\n",
      "2079    32365.612187\n",
      "2080    33049.383925\n",
      "2081    33692.807402\n",
      "2082    34369.593575\n",
      "2083    35050.390377\n",
      "2084    35784.421172\n",
      "2085    36477.978554\n",
      "2086    37213.594654\n",
      "2087    37960.888308\n",
      "2088    38722.396454\n",
      "2089    39475.556267\n",
      "2090    40256.671205\n",
      "2091    41061.450449\n",
      "2092    41905.079551\n",
      "2093    42700.769713\n",
      "2094    43525.044762\n",
      "2095    44387.232733\n",
      "2096    45280.059443\n",
      "2097    46147.567996\n",
      "2098    47048.302414\n",
      "2099    47956.957496\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Maryland_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2077    34604.242148\n",
      "2079    33588.305186\n",
      "2080    32245.454053\n",
      "2081    32471.913009\n",
      "2082    31589.068230\n",
      "2083    31100.994577\n",
      "2084    30752.401187\n",
      "2085    31438.791174\n",
      "2086    30659.747115\n",
      "2087    30905.252000\n",
      "2088    30495.607108\n",
      "2089    31166.392963\n",
      "2090    30807.514428\n",
      "2091    30711.234716\n",
      "2092    30984.423971\n",
      "2093    31166.124100\n",
      "2094    30232.583127\n",
      "2095    31024.489182\n",
      "2096    31044.730139\n",
      "2097    30778.443680\n",
      "2098    30895.342064\n",
      "2099    30429.465924\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2041    169433.491161\n",
      "2042    170764.212984\n",
      "2043    171301.463059\n",
      "2044    173200.180019\n",
      "2045    173434.837373\n",
      "2046    175357.233979\n",
      "2047    177729.218273\n",
      "2048    180323.492839\n",
      "2049    183189.465399\n",
      "2050    186017.973088\n",
      "2051    188690.179044\n",
      "2052    191957.204774\n",
      "2053    194750.059173\n",
      "2054    197445.556809\n",
      "2055    199717.840613\n",
      "2056    202420.718720\n",
      "2057    205217.290304\n",
      "2058    208290.854199\n",
      "2059    211274.632861\n",
      "2060    214190.989794\n",
      "2061    217469.250035\n",
      "2062    220798.082547\n",
      "2063    224015.536022\n",
      "2064    227698.040721\n",
      "2065    230844.652282\n",
      "2066    234642.681906\n",
      "2067    238133.270843\n",
      "2068    241561.437038\n",
      "2069    245332.120401\n",
      "2070    249284.709229\n",
      "2071    253101.074528\n",
      "2072    256605.664994\n",
      "2073    260690.304383\n",
      "2074    264167.450835\n",
      "2075    267979.312312\n",
      "2076    271290.270699\n",
      "2077    273968.390877\n",
      "2078    276823.303077\n",
      "2079    279636.590992\n",
      "2080    283214.199200\n",
      "2081    287637.420566\n",
      "2082    292688.760313\n",
      "2083    297689.914922\n",
      "2084    302972.168242\n",
      "2085    308203.715652\n",
      "2086    313610.187987\n",
      "2087    319076.022566\n",
      "2088    324581.133247\n",
      "2089    330168.248507\n",
      "2090    335812.633706\n",
      "2091    341647.075866\n",
      "2092    347668.084395\n",
      "2093    353521.854773\n",
      "2094    359468.911831\n",
      "2095    365670.432521\n",
      "2096    372012.345324\n",
      "2097    378264.283952\n",
      "2098    384686.367631\n",
      "2099    391190.510737\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p87_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2035    2362.025284\n",
      "2036    2268.190978\n",
      "2037    2255.735184\n",
      "2038    2264.454932\n",
      "2039    2284.037086\n",
      "           ...     \n",
      "2095    3487.444160\n",
      "2096    3515.581012\n",
      "2097    3548.623159\n",
      "2098    3576.764640\n",
      "2099    3602.095415\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2040    18163.409953\n",
      "2041    18171.916346\n",
      "2042    18234.074646\n",
      "2043    18270.763095\n",
      "2044    18429.952954\n",
      "2045    18551.010081\n",
      "2046    18468.287553\n",
      "2047    18429.088445\n",
      "2048    18505.810335\n",
      "2049    18609.139309\n",
      "2050    18689.093878\n",
      "2051    18795.960443\n",
      "2052    18915.081850\n",
      "2053    19023.803945\n",
      "2054    19093.304566\n",
      "2055    19189.245113\n",
      "2056    19322.838807\n",
      "2057    19438.081773\n",
      "2058    19573.168531\n",
      "2059    19636.240490\n",
      "2060    19722.887658\n",
      "2061    19788.215843\n",
      "2062    19777.861075\n",
      "2063    19865.164847\n",
      "2064    19979.890979\n",
      "2065    20010.599086\n",
      "2066    20051.946806\n",
      "2067    20171.646026\n",
      "2068    20304.846913\n",
      "2069    20401.420001\n",
      "2070    20449.878270\n",
      "2071    20577.913568\n",
      "2072    20767.617149\n",
      "2073    20875.944581\n",
      "2074    21021.939633\n",
      "2075    21165.047690\n",
      "2076    21279.171158\n",
      "2077    21405.402151\n",
      "2078    21531.789990\n",
      "2079    21509.978839\n",
      "2080    21431.584385\n",
      "2081    21502.194389\n",
      "2082    21628.019119\n",
      "2083    21726.033209\n",
      "2084    21945.461085\n",
      "2085    22056.874630\n",
      "2086    22209.412419\n",
      "2087    22389.482113\n",
      "2088    22579.151438\n",
      "2089    22766.865155\n",
      "2090    22875.817940\n",
      "2091    23001.504198\n",
      "2092    22943.916355\n",
      "2093    22966.263009\n",
      "2094    23220.706433\n",
      "2095    23492.701459\n",
      "2096    23768.190132\n",
      "2097    24028.453990\n",
      "2098    24303.382146\n",
      "2099    24579.083964\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2049    107757.980025\n",
      "2050    106647.756919\n",
      "2051    105998.700533\n",
      "2052    107469.931687\n",
      "2053    107797.881242\n",
      "2054    108412.110011\n",
      "2055    107578.085427\n",
      "2056    106913.055562\n",
      "2057    107350.103212\n",
      "2058    107831.840627\n",
      "2059    108150.778330\n",
      "2060    108797.227639\n",
      "2061    109127.107494\n",
      "2062    109549.358739\n",
      "2063    110299.818376\n",
      "2064    110409.252611\n",
      "2065    110985.713221\n",
      "2066    110757.664676\n",
      "2067    111337.369358\n",
      "2068    111821.253883\n",
      "2069    112357.086915\n",
      "2070    112961.178646\n",
      "2071    113437.735998\n",
      "2072    113792.119200\n",
      "2073    114209.116560\n",
      "2074    114593.043288\n",
      "2075    114882.353422\n",
      "2076    115420.109753\n",
      "2077    116037.234020\n",
      "2078    116596.289187\n",
      "2079    116972.461304\n",
      "2080    118036.164084\n",
      "2081    118560.181803\n",
      "2082    119059.977084\n",
      "2083    119503.350721\n",
      "2084    119488.889873\n",
      "2085    119364.862511\n",
      "2086    119876.648774\n",
      "2087    120753.771644\n",
      "2088    121400.957366\n",
      "2089    122087.038468\n",
      "2090    122773.233880\n",
      "2091    123664.147532\n",
      "2092    124462.708745\n",
      "2093    124843.198415\n",
      "2094    125333.371298\n",
      "2095    125783.196443\n",
      "2096    126216.346534\n",
      "2097    126632.975230\n",
      "2098    126793.724379\n",
      "2099    126929.668739\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032    8.252542e+05\n",
      "2033    8.112729e+05\n",
      "2034    8.211306e+05\n",
      "2035    8.287225e+05\n",
      "2036    8.332054e+05\n",
      "            ...     \n",
      "2095    1.911685e+06\n",
      "2096    1.943887e+06\n",
      "2097    1.974617e+06\n",
      "2098    2.006859e+06\n",
      "2099    2.039466e+06\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2032     52520.445136\n",
      "2033     48325.054151\n",
      "2034     48446.181789\n",
      "2035     49382.264950\n",
      "2036     50276.466900\n",
      "            ...      \n",
      "2095     97412.418303\n",
      "2096     98760.611108\n",
      "2097    100089.570839\n",
      "2098    101447.970602\n",
      "2099    102819.191189\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2033    5061.226883\n",
      "2034    5099.860942\n",
      "2035    4841.507812\n",
      "2036    4646.162088\n",
      "2037    4557.359897\n",
      "           ...     \n",
      "2095    8412.256529\n",
      "2096    8531.782975\n",
      "2097    8654.129162\n",
      "2098    8776.869081\n",
      "2099    8899.743707\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2029     86406.945857\n",
      "2030     88804.253326\n",
      "2031     90690.789437\n",
      "2032     92696.411757\n",
      "2033     94971.363066\n",
      "            ...      \n",
      "2095    745081.081755\n",
      "2096    770371.963030\n",
      "2097    795275.337365\n",
      "2098    821502.166610\n",
      "2099    848433.008794\n",
      "Name: Load_sum, Length: 71, dtype: float64\n",
      "Year\n",
      "2033    42224.210141\n",
      "2034    41614.083864\n",
      "2035    42361.871482\n",
      "2036    43185.515879\n",
      "2037    43676.834579\n",
      "            ...     \n",
      "2095    81305.416160\n",
      "2096    82341.790283\n",
      "2097    83366.738035\n",
      "2098    84415.316263\n",
      "2099    85472.426164\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2035     65431.159167\n",
      "2036     66856.945621\n",
      "2037     68247.926441\n",
      "2038     69300.927672\n",
      "2039     70682.194473\n",
      "            ...      \n",
      "2095    208938.023594\n",
      "2096    214054.947758\n",
      "2097    219114.185871\n",
      "2098    224355.595709\n",
      "2099    229672.765912\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Skipping empty file: Georgia_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2033    427.001216\n",
      "2034    425.670166\n",
      "2035    431.844168\n",
      "2036    438.593486\n",
      "2037    443.426555\n",
      "           ...    \n",
      "2095    932.136587\n",
      "2096    948.621505\n",
      "2097    964.794240\n",
      "2098    981.558863\n",
      "2099    998.353362\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: p80_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2031     6369.110436\n",
      "2032     6526.136497\n",
      "2033     6655.236314\n",
      "2034     6760.135729\n",
      "2035     6867.971592\n",
      "            ...     \n",
      "2095    16107.002144\n",
      "2096    16418.820825\n",
      "2097    16722.832803\n",
      "2098    17032.654339\n",
      "2099    17346.765025\n",
      "Name: Load_sum, Length: 69, dtype: float64\n",
      "Skipping empty file: Louisiana_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2066    45391.902067\n",
      "2067    45831.529100\n",
      "2068    46194.615829\n",
      "2069    46381.639661\n",
      "2070    46576.299148\n",
      "2071    46983.562725\n",
      "2072    47357.931516\n",
      "2073    47726.171021\n",
      "2074    48232.876148\n",
      "2075    48581.053751\n",
      "2076    48951.351576\n",
      "2077    49193.165376\n",
      "2078    49794.566084\n",
      "2079    50224.845459\n",
      "2080    50554.087473\n",
      "2081    51093.811140\n",
      "2082    51482.847548\n",
      "2083    51713.521991\n",
      "2084    52171.173651\n",
      "2085    52647.989986\n",
      "2086    52947.654356\n",
      "2087    53279.974954\n",
      "2088    53719.814581\n",
      "2089    54100.031142\n",
      "2090    54521.756270\n",
      "2091    54985.592092\n",
      "2092    55350.671207\n",
      "2093    55654.600688\n",
      "2094    56017.733315\n",
      "2095    56542.313290\n",
      "2096    56957.880528\n",
      "2097    57389.941675\n",
      "2098    57746.256771\n",
      "2099    58144.462690\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p86_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2044     63632.025870\n",
      "2045     62674.060326\n",
      "2046     63692.460718\n",
      "2047     64157.049419\n",
      "2048     64843.989096\n",
      "2049     65285.582748\n",
      "2050     66001.444950\n",
      "2051     66914.922408\n",
      "2052     67647.617118\n",
      "2053     68477.570186\n",
      "2054     69322.098257\n",
      "2055     70122.950060\n",
      "2056     70930.182638\n",
      "2057     71861.810603\n",
      "2058     72729.203693\n",
      "2059     73568.072991\n",
      "2060     74299.652136\n",
      "2061     75179.302753\n",
      "2062     75963.917125\n",
      "2063     76599.223911\n",
      "2064     77508.321072\n",
      "2065     78327.796289\n",
      "2066     79174.445244\n",
      "2067     80240.793582\n",
      "2068     81262.984165\n",
      "2069     82128.573086\n",
      "2070     83085.472483\n",
      "2071     84126.950718\n",
      "2072     85044.164709\n",
      "2073     86167.888243\n",
      "2074     87213.260096\n",
      "2075     88271.184793\n",
      "2076     89463.646822\n",
      "2077     90409.370277\n",
      "2078     91554.226977\n",
      "2079     92717.418508\n",
      "2080     94003.168134\n",
      "2081     95157.485368\n",
      "2082     96319.228301\n",
      "2083     97477.217703\n",
      "2084     98788.598604\n",
      "2085     99724.706040\n",
      "2086    100925.242338\n",
      "2087    101996.452122\n",
      "2088    103035.493061\n",
      "2089    104177.847565\n",
      "2090    104752.979701\n",
      "2091    105737.782917\n",
      "2092    106554.776849\n",
      "2093    107655.911101\n",
      "2094    109124.384646\n",
      "2095    110731.015360\n",
      "2096    112371.951425\n",
      "2097    113992.298677\n",
      "2098    115636.731264\n",
      "2099    117299.026826\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2059    29212.994466\n",
      "2060    29690.544901\n",
      "2061    27613.486670\n",
      "2062    26569.807394\n",
      "2063    25725.124454\n",
      "2064    26590.360954\n",
      "2065    25066.354346\n",
      "2066    24707.901339\n",
      "2067    24137.329202\n",
      "2068    24346.875428\n",
      "2069    23837.359250\n",
      "2070    23188.631241\n",
      "2071    22647.650057\n",
      "2072    21890.862353\n",
      "2073    21439.796538\n",
      "2074    20535.736156\n",
      "2075    19646.630356\n",
      "2076    19287.120931\n",
      "2077    18301.766417\n",
      "2078    17687.077320\n",
      "2079    16711.459613\n",
      "2080    16343.972367\n",
      "2081    15256.743926\n",
      "2082    14667.066603\n",
      "2083    13686.195435\n",
      "2084    13247.860142\n",
      "2085    12649.919429\n",
      "2086    11299.690932\n",
      "2087    10527.891784\n",
      "2088     9455.713159\n",
      "2089     8434.021863\n",
      "2090     7534.827759\n",
      "2091     6756.483722\n",
      "2092     6234.717372\n",
      "2093     5373.233090\n",
      "2094     4566.634066\n",
      "2095     3859.715293\n",
      "2096     3307.554219\n",
      "2097     2518.027192\n",
      "2098     1926.616218\n",
      "2099     1213.092537\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2064    5890.997402\n",
      "2065    5992.309259\n",
      "2066    5820.053744\n",
      "2067    5782.064476\n",
      "2068    5839.255167\n",
      "2069    5864.077287\n",
      "2070    5869.882027\n",
      "2071    5881.462577\n",
      "2072    5903.957266\n",
      "2073    5831.172705\n",
      "2074    5759.721948\n",
      "2075    5749.890820\n",
      "2076    5774.563474\n",
      "2077    5795.197826\n",
      "2078    5817.998385\n",
      "2079    5853.324387\n",
      "2080    5919.625424\n",
      "2081    5920.693556\n",
      "2082    5958.899434\n",
      "2083    5974.843839\n",
      "2084    6022.018605\n",
      "2085    6044.230109\n",
      "2086    6069.034807\n",
      "2087    6080.193684\n",
      "2088    6136.954352\n",
      "2089    6155.536130\n",
      "2090    6183.914136\n",
      "2091    6200.144225\n",
      "2092    6253.602013\n",
      "2093    6278.168334\n",
      "2094    6316.316885\n",
      "2095    6330.293831\n",
      "2096    6347.645362\n",
      "2097    6393.169850\n",
      "2098    6392.138882\n",
      "2099    6402.797137\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2088    114605.153736\n",
      "2089    107645.655633\n",
      "2090    105827.595691\n",
      "2091    106784.520751\n",
      "2092    106003.192342\n",
      "2093    106804.806232\n",
      "2094    106976.244498\n",
      "2095    108082.731575\n",
      "2096    107959.078062\n",
      "2097    108477.517872\n",
      "2098    109318.604740\n",
      "2099    109519.874643\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2064    37513.801956\n",
      "2065    38996.111380\n",
      "2066    39331.672229\n",
      "2067    39012.150559\n",
      "2068    39374.453414\n",
      "2069    39483.101770\n",
      "2070    39645.504310\n",
      "2071    39817.110918\n",
      "2072    39810.426870\n",
      "2073    39673.956788\n",
      "2074    39929.741309\n",
      "2075    39545.000382\n",
      "2076    39586.444802\n",
      "2077    39508.638759\n",
      "2078    39702.983547\n",
      "2079    39918.880661\n",
      "2080    39949.385571\n",
      "2081    40173.336720\n",
      "2082    40170.632524\n",
      "2083    40112.062878\n",
      "2084    40307.674803\n",
      "2085    40559.668019\n",
      "2086    40714.536688\n",
      "2087    40899.071702\n",
      "2088    40713.854776\n",
      "2089    40896.825386\n",
      "2090    40982.072920\n",
      "2091    41260.897146\n",
      "2092    41348.919805\n",
      "2093    41556.216362\n",
      "2094    41420.079943\n",
      "2095    41576.391769\n",
      "2096    41695.073644\n",
      "2097    41867.054958\n",
      "2098    42004.571091\n",
      "2099    42084.861725\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Texas_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p58_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2097    2085.934886\n",
      "2098    2005.777017\n",
      "2099    1900.535005\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p105_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p128_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p103_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p124_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Indiana_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p122_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p109_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Illinois_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p55_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2052    90197.466355\n",
      "2053    86186.280072\n",
      "2054    84628.541371\n",
      "2055    81331.594103\n",
      "2056    79755.973994\n",
      "2057    77556.295141\n",
      "2058    76131.070225\n",
      "2059    74710.012244\n",
      "2060    73097.029462\n",
      "2061    71884.565230\n",
      "2062    71415.045051\n",
      "2063    69884.221814\n",
      "2064    69087.763545\n",
      "2065    66477.444032\n",
      "2066    64558.288636\n",
      "2067    63263.046263\n",
      "2068    61844.116016\n",
      "2069    60311.616322\n",
      "2070    58022.226173\n",
      "2071    56238.936197\n",
      "2072    53620.650954\n",
      "2073    52917.421482\n",
      "2074    50252.367591\n",
      "2075    48701.077298\n",
      "2076    46428.465023\n",
      "2077    43551.046036\n",
      "2078    40812.738095\n",
      "2079    37541.220068\n",
      "2080    34593.362837\n",
      "2081    32264.955258\n",
      "2082    29586.659473\n",
      "2083    26772.467477\n",
      "2084    24713.935058\n",
      "2085    21954.422258\n",
      "2086    19426.678038\n",
      "2087    16650.225286\n",
      "2088    14096.271294\n",
      "2089    11277.598266\n",
      "2090     8528.466368\n",
      "2091     5864.005422\n",
      "2092     3403.611859\n",
      "2093      565.058928\n",
      "2094    -2692.120125\n",
      "2095    -5304.473219\n",
      "2096    -8192.240739\n",
      "2097   -11017.443794\n",
      "2098   -14120.406794\n",
      "2099   -16994.114143\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2083    23312.972008\n",
      "2084    22326.741301\n",
      "2085    21545.636727\n",
      "2086    22251.043858\n",
      "2087    21477.059959\n",
      "2088    21424.709572\n",
      "2089    21386.815850\n",
      "2090    21381.789905\n",
      "2091    21540.379259\n",
      "2092    21257.493428\n",
      "2093    21234.252739\n",
      "2094    20897.972558\n",
      "2095    21009.888568\n",
      "2096    20693.821574\n",
      "2097    20595.108762\n",
      "2098    20487.905929\n",
      "2099    20187.392115\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2087    6518.861380\n",
      "2088    6512.238504\n",
      "2089    6481.252027\n",
      "2090    6491.601901\n",
      "2091    6479.554614\n",
      "2092    6498.961445\n",
      "2093    6486.091952\n",
      "2094    6490.249026\n",
      "2095    6545.958436\n",
      "2096    6532.330768\n",
      "2097    6543.151458\n",
      "2098    6587.499068\n",
      "2099    6577.647477\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2050    37471.144989\n",
      "2051    36695.199495\n",
      "2052    38230.097295\n",
      "2053    39253.639955\n",
      "2054    40001.163981\n",
      "2055    40632.321860\n",
      "2056    41777.727922\n",
      "2057    42844.271510\n",
      "2058    43509.927799\n",
      "2059    44613.803790\n",
      "2060    45208.259614\n",
      "2061    46059.163913\n",
      "2062    47056.808424\n",
      "2063    48050.037228\n",
      "2064    49109.389973\n",
      "2065    50208.540853\n",
      "2066    51101.095980\n",
      "2067    52233.508277\n",
      "2068    53210.858364\n",
      "2069    54255.621650\n",
      "2070    55597.793421\n",
      "2071    56808.277663\n",
      "2072    57962.711213\n",
      "2073    59106.983246\n",
      "2074    60399.970860\n",
      "2075    61776.125400\n",
      "2076    62996.180521\n",
      "2077    64171.370326\n",
      "2078    65241.567440\n",
      "2079    66712.284668\n",
      "2080    68110.100213\n",
      "2081    69244.790547\n",
      "2082    70641.608797\n",
      "2083    72047.759960\n",
      "2084    73618.912512\n",
      "2085    75197.688211\n",
      "2086    76745.085044\n",
      "2087    78085.243328\n",
      "2088    79667.797895\n",
      "2089    80978.122796\n",
      "2090    82819.947271\n",
      "2091    84785.351826\n",
      "2092    86523.258097\n",
      "2093    88294.034854\n",
      "2094    90107.032111\n",
      "2095    91924.920619\n",
      "2096    93546.439994\n",
      "2097    95437.166551\n",
      "2098    97309.557645\n",
      "2099    99131.184910\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2092    31998.389612\n",
      "2093    27804.239736\n",
      "2094    28729.264821\n",
      "2095    26682.947902\n",
      "2096    25977.659832\n",
      "2097    26227.574411\n",
      "2098    23846.130024\n",
      "2099    24611.001656\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p104_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2060    21256.832908\n",
      "2061    21545.353017\n",
      "2062    21405.297225\n",
      "2063    21503.533693\n",
      "2064    21653.783283\n",
      "2065    21756.993485\n",
      "2066    21907.056199\n",
      "2067    22078.360931\n",
      "2068    22189.034006\n",
      "2069    22306.076349\n",
      "2070    22412.497920\n",
      "2071    22564.967989\n",
      "2072    22679.091334\n",
      "2073    22825.145924\n",
      "2074    22927.592178\n",
      "2075    23012.483329\n",
      "2076    23172.908828\n",
      "2077    23264.681552\n",
      "2078    23399.528620\n",
      "2079    23526.975700\n",
      "2080    23665.273925\n",
      "2081    23762.067630\n",
      "2082    23856.161726\n",
      "2083    23984.553070\n",
      "2084    24083.049514\n",
      "2085    24210.067524\n",
      "2086    24351.142406\n",
      "2087    24472.886732\n",
      "2088    24567.243711\n",
      "2089    24669.540486\n",
      "2090    24777.671077\n",
      "2091    24936.772765\n",
      "2092    25085.643716\n",
      "2093    25140.745216\n",
      "2094    25277.568412\n",
      "2095    25420.301289\n",
      "2096    25483.662481\n",
      "2097    25643.435432\n",
      "2098    25780.136314\n",
      "2099    25863.577741\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p102_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p125_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p123_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p108_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2051    288226.160745\n",
      "2052    298691.003349\n",
      "2053    301874.538132\n",
      "2054    306162.977149\n",
      "2055    308751.543415\n",
      "2056    310650.314103\n",
      "2057    313797.212850\n",
      "2058    315556.876123\n",
      "2059    319497.916760\n",
      "2060    325068.573270\n",
      "2061    329584.977668\n",
      "2062    334368.790663\n",
      "2063    337867.495883\n",
      "2064    342459.309740\n",
      "2065    346193.857474\n",
      "2066    350629.829407\n",
      "2067    357623.005022\n",
      "2068    363203.305341\n",
      "2069    369479.435810\n",
      "2070    375847.251356\n",
      "2071    381618.733554\n",
      "2072    387303.714486\n",
      "2073    395080.240851\n",
      "2074    399966.246850\n",
      "2075    405453.128565\n",
      "2076    413141.186976\n",
      "2077    417609.646115\n",
      "2078    424231.764321\n",
      "2079    431462.039323\n",
      "2080    438206.148953\n",
      "2081    444243.689055\n",
      "2082    449316.914287\n",
      "2083    454535.661295\n",
      "2084    462000.375364\n",
      "2085    470513.168619\n",
      "2086    478120.700274\n",
      "2087    486652.693462\n",
      "2088    495071.735355\n",
      "2089    503266.107575\n",
      "2090    512145.418047\n",
      "2091    521194.192208\n",
      "2092    530605.710389\n",
      "2093    538922.532059\n",
      "2094    545984.603569\n",
      "2095    554424.652635\n",
      "2096    559780.073930\n",
      "2097    567575.841814\n",
      "2098    575166.617823\n",
      "2099    584854.487551\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Kentucky_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p99_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2036    121760.563755\n",
      "2037    106163.711325\n",
      "2038    102376.756667\n",
      "2039    101550.600572\n",
      "2040    102043.030247\n",
      "            ...      \n",
      "2095     82969.010467\n",
      "2096     82915.732901\n",
      "2097     82894.750370\n",
      "2098     82798.970175\n",
      "2099     82657.531530\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Skipping empty file: p95_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2030     32973.893301\n",
      "2031     33757.439131\n",
      "2032     34024.277019\n",
      "2033     34533.229046\n",
      "2034     35092.662352\n",
      "            ...      \n",
      "2095    232714.589929\n",
      "2096    239864.390483\n",
      "2097    246796.418135\n",
      "2098    254132.836703\n",
      "2099    261550.417395\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Skipping empty file: p93_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2034    26670.432388\n",
      "2035    25936.390774\n",
      "2036    25975.381894\n",
      "2037    25502.824196\n",
      "2038    25299.441745\n",
      "            ...     \n",
      "2095    30194.856251\n",
      "2096    30378.975174\n",
      "2097    30560.183803\n",
      "2098    30737.828344\n",
      "2099    30907.225556\n",
      "Name: Load_sum, Length: 66, dtype: float64\n",
      "Year\n",
      "2043    132457.085825\n",
      "2044    134674.207785\n",
      "2045    137954.490276\n",
      "2046    141964.497321\n",
      "2047    145435.062148\n",
      "2048    148743.581913\n",
      "2049    151699.674135\n",
      "2050    155322.343280\n",
      "2051    158567.676112\n",
      "2052    162259.675935\n",
      "2053    165705.542463\n",
      "2054    169194.164847\n",
      "2055    173264.655640\n",
      "2056    177256.383727\n",
      "2057    181185.817046\n",
      "2058    185718.326618\n",
      "2059    190145.322913\n",
      "2060    194238.150805\n",
      "2061    198776.649873\n",
      "2062    203509.021130\n",
      "2063    208434.976213\n",
      "2064    213676.455480\n",
      "2065    218500.119159\n",
      "2066    223013.711431\n",
      "2067    227947.697585\n",
      "2068    233409.676404\n",
      "2069    238290.765006\n",
      "2070    244076.451474\n",
      "2071    249642.943837\n",
      "2072    256015.028908\n",
      "2073    261662.190641\n",
      "2074    268007.830075\n",
      "2075    275131.599441\n",
      "2076    282100.941710\n",
      "2077    288288.884487\n",
      "2078    295775.563766\n",
      "2079    302643.867054\n",
      "2080    310387.349861\n",
      "2081    317988.082451\n",
      "2082    325976.652215\n",
      "2083    334105.076752\n",
      "2084    342358.585496\n",
      "2085    350962.105761\n",
      "2086    359970.459038\n",
      "2087    368864.256382\n",
      "2088    377683.687773\n",
      "2089    387007.660571\n",
      "2090    395762.738416\n",
      "2091    405722.577082\n",
      "2092    416068.647862\n",
      "2093    425870.650617\n",
      "2094    436405.716875\n",
      "2095    445487.560275\n",
      "2096    448414.193660\n",
      "2097    456187.247611\n",
      "2098    468970.152476\n",
      "2099    482108.208694\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p98_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p94_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2032     5410.859965\n",
      "2033     5299.944629\n",
      "2034     6135.337711\n",
      "2035     6392.739698\n",
      "2036     6641.508687\n",
      "            ...     \n",
      "2095    22255.753541\n",
      "2096    22820.778334\n",
      "2097    23351.001289\n",
      "2098    23915.088037\n",
      "2099    24490.080680\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: West Virginia_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2032    53394.081131\n",
      "2033    51670.538633\n",
      "2034    52630.084348\n",
      "2035    53536.957885\n",
      "2036    53509.252101\n",
      "            ...     \n",
      "2095    59383.637094\n",
      "2096    59755.143483\n",
      "2097    60102.524562\n",
      "2098    60456.462926\n",
      "2099    60796.009860\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p92_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2070    248222.589163\n",
      "2071    248658.291518\n",
      "2072    246495.058722\n",
      "2073    246572.963330\n",
      "2074    246176.400996\n",
      "2075    247423.505561\n",
      "2076    247831.466756\n",
      "2077    248902.679488\n",
      "2078    249069.590470\n",
      "2079    247813.497350\n",
      "2080    249182.667431\n",
      "2081    246959.626256\n",
      "2082    247919.854538\n",
      "2083    245888.619343\n",
      "2084    246038.423813\n",
      "2085    245995.103210\n",
      "2086    246487.840866\n",
      "2087    246822.986052\n",
      "2088    246263.859417\n",
      "2089    246811.833360\n",
      "2090    247702.233759\n",
      "2091    248290.054975\n",
      "2092    248496.416858\n",
      "2093    248563.799599\n",
      "2094    248489.154842\n",
      "2095    248923.282660\n",
      "2096    249636.818916\n",
      "2097    249681.898526\n",
      "2098    250217.660019\n",
      "2099    250201.828914\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2036     9718.279866\n",
      "2037     9743.668890\n",
      "2038     9271.263020\n",
      "2039     8875.297946\n",
      "2040     8902.933438\n",
      "            ...     \n",
      "2095    20083.226823\n",
      "2096    20443.785717\n",
      "2097    20801.079012\n",
      "2098    21163.650469\n",
      "2099    21528.458750\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Skipping empty file: Delaware_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2035    41389.379580\n",
      "2036    39663.859996\n",
      "2037    38767.997867\n",
      "2038    37810.223896\n",
      "2039    38245.758543\n",
      "            ...     \n",
      "2095    36086.475873\n",
      "2096    36430.426141\n",
      "2097    36759.253441\n",
      "2098    37075.414833\n",
      "2099    37367.754833\n",
      "Name: Load_sum, Length: 65, dtype: float64\n",
      "Year\n",
      "2067    34540.193251\n",
      "2069    34766.409654\n",
      "2070    33750.615659\n",
      "2071    34016.817641\n",
      "2072    33808.631089\n",
      "2073    33853.937096\n",
      "2074    33951.312206\n",
      "2075    33921.753471\n",
      "2076    34164.439699\n",
      "2077    33926.889829\n",
      "2078    34156.610019\n",
      "2079    34159.544214\n",
      "2080    34202.233350\n",
      "2081    34272.234857\n",
      "2082    34383.192298\n",
      "2083    34474.833652\n",
      "2084    34610.818925\n",
      "2085    34689.380426\n",
      "2086    34827.764839\n",
      "2087    34950.542742\n",
      "2088    35022.470106\n",
      "2089    35157.533814\n",
      "2090    35279.389333\n",
      "2091    35381.108147\n",
      "2092    35563.258987\n",
      "2093    35619.484544\n",
      "2094    35653.355028\n",
      "2095    35798.862424\n",
      "2096    36038.875457\n",
      "2097    36132.850724\n",
      "2098    36154.133038\n",
      "2099    36272.584744\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p61_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p67_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2038    1.970709e+05\n",
      "2039    2.073931e+05\n",
      "2040    2.207407e+05\n",
      "2041    2.345017e+05\n",
      "2042    2.489704e+05\n",
      "            ...     \n",
      "2095    5.186737e+06\n",
      "2096    5.497666e+06\n",
      "2097    5.827366e+06\n",
      "2098    6.175723e+06\n",
      "2099    6.544009e+06\n",
      "Name: Load_sum, Length: 62, dtype: float64\n",
      "Skipping empty file: p111_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p117_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2066    45391.902067\n",
      "2067    45831.529100\n",
      "2068    46194.615829\n",
      "2069    46381.639661\n",
      "2070    46576.299148\n",
      "2071    46983.562725\n",
      "2072    47357.931516\n",
      "2073    47726.171021\n",
      "2074    48232.876148\n",
      "2075    48581.053751\n",
      "2076    48951.351576\n",
      "2077    49193.165376\n",
      "2078    49794.566084\n",
      "2079    50224.845459\n",
      "2080    50554.087473\n",
      "2081    51093.811140\n",
      "2082    51482.847548\n",
      "2083    51713.521991\n",
      "2084    52171.173651\n",
      "2085    52647.989986\n",
      "2086    52947.654356\n",
      "2087    53279.974954\n",
      "2088    53719.814581\n",
      "2089    54100.031142\n",
      "2090    54521.756270\n",
      "2091    54985.592092\n",
      "2092    55350.671207\n",
      "2093    55654.600688\n",
      "2094    56017.733315\n",
      "2095    56542.313290\n",
      "2096    56957.880528\n",
      "2097    57389.941675\n",
      "2098    57746.256771\n",
      "2099    58144.462690\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Pennsylvania_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: New Jersey_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Tennessee_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Oklahoma_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2040    55423.106794\n",
      "2041    55935.357240\n",
      "2042    54681.524593\n",
      "2043    55168.204528\n",
      "2044    55375.516487\n",
      "2045    55693.126144\n",
      "2046    55691.428150\n",
      "2047    55780.676658\n",
      "2048    55881.960073\n",
      "2049    55496.243780\n",
      "2050    55433.094395\n",
      "2051    55295.576187\n",
      "2052    55643.435973\n",
      "2053    55195.307565\n",
      "2054    55355.368470\n",
      "2055    54669.045022\n",
      "2056    54553.911153\n",
      "2057    54242.482584\n",
      "2058    53484.737770\n",
      "2059    52770.994209\n",
      "2060    51738.733170\n",
      "2061    51298.769229\n",
      "2062    50843.106352\n",
      "2063    50373.556054\n",
      "2064    50008.953753\n",
      "2065    49669.939899\n",
      "2066    49000.355280\n",
      "2067    48533.178661\n",
      "2068    47837.320302\n",
      "2069    47227.272200\n",
      "2070    46741.475379\n",
      "2071    46147.103813\n",
      "2072    45598.216912\n",
      "2073    44946.564789\n",
      "2074    44310.000142\n",
      "2075    43913.490776\n",
      "2076    43124.452438\n",
      "2077    42032.847755\n",
      "2078    40579.884508\n",
      "2079    39569.377668\n",
      "2080    38438.304530\n",
      "2081    37700.194847\n",
      "2082    37123.982053\n",
      "2083    36432.693499\n",
      "2084    35920.343782\n",
      "2085    35314.599968\n",
      "2086    34742.443456\n",
      "2087    34117.951065\n",
      "2088    33315.781186\n",
      "2089    32691.844991\n",
      "2090    32010.008824\n",
      "2091    31354.492596\n",
      "2092    30708.268030\n",
      "2093    29941.575700\n",
      "2094    29095.212306\n",
      "2095    28414.794380\n",
      "2096    27644.068483\n",
      "2097    26933.020110\n",
      "2098    26142.774397\n",
      "2099    25321.616629\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Arkansas_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p47_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p60_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p66_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: South Carolina_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p110_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p116_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2077    135924.064476\n",
      "2078    141671.103560\n",
      "2079    142063.617729\n",
      "2080    136001.379757\n",
      "2081    136231.589941\n",
      "2082    135957.661957\n",
      "2083    135510.264613\n",
      "2084    135686.092840\n",
      "2085    135208.840977\n",
      "2086    134588.986787\n",
      "2087    134103.309495\n",
      "2088    133891.093380\n",
      "2089    133639.781654\n",
      "2090    133320.629300\n",
      "2091    133068.620283\n",
      "2092    132920.958576\n",
      "2093    132353.779845\n",
      "2094    131777.961943\n",
      "2095    131590.245273\n",
      "2096    131378.530313\n",
      "2097    131420.690178\n",
      "2098    130606.590658\n",
      "2099    130471.600583\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2048     59326.032083\n",
      "2049     59284.201316\n",
      "2050     60235.058768\n",
      "2051     60418.393551\n",
      "2052     61662.129186\n",
      "2053     62278.338107\n",
      "2054     62805.820137\n",
      "2055     63709.005098\n",
      "2056     64433.091960\n",
      "2057     64677.587219\n",
      "2058     65109.385577\n",
      "2059     66065.661319\n",
      "2060     67078.611371\n",
      "2061     67956.879722\n",
      "2062     69137.772032\n",
      "2063     70343.111557\n",
      "2064     71399.379912\n",
      "2065     72516.206799\n",
      "2066     73632.964658\n",
      "2067     74405.805614\n",
      "2068     75639.979818\n",
      "2069     76682.252339\n",
      "2070     77602.535655\n",
      "2071     78262.410623\n",
      "2072     79487.672712\n",
      "2073     80678.813843\n",
      "2074     81606.854906\n",
      "2075     82751.376307\n",
      "2076     84022.391785\n",
      "2077     85143.791521\n",
      "2078     86595.245939\n",
      "2079     87942.096596\n",
      "2080     89462.254737\n",
      "2081     91053.344915\n",
      "2082     92535.979867\n",
      "2083     94011.165664\n",
      "2084     95611.256402\n",
      "2085     97189.184520\n",
      "2086     98676.730854\n",
      "2087    100047.397463\n",
      "2088    101405.719277\n",
      "2089    102442.267501\n",
      "2090    103582.279678\n",
      "2091    105328.118415\n",
      "2092    107414.003257\n",
      "2093    109390.272860\n",
      "2094    111409.010963\n",
      "2095    113548.901189\n",
      "2096    115790.856685\n",
      "2097    117988.774479\n",
      "2098    120252.043554\n",
      "2099    122543.666478\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p88_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2030     56638.206707\n",
      "2031     57676.200773\n",
      "2032     57530.147277\n",
      "2033     58020.805459\n",
      "2034     58658.809883\n",
      "            ...      \n",
      "2095    156848.466864\n",
      "2096    159510.185448\n",
      "2097    162035.085195\n",
      "2098    164697.617344\n",
      "2099    167402.350993\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Year\n",
      "2045    39028.223680\n",
      "2046    39559.430609\n",
      "2047    40168.835473\n",
      "2048    40227.123503\n",
      "2049    40568.956950\n",
      "2050    41017.684256\n",
      "2051    41460.431069\n",
      "2052    42142.545518\n",
      "2053    42728.078135\n",
      "2054    43282.764565\n",
      "2055    43931.238861\n",
      "2056    44504.868978\n",
      "2057    45134.077606\n",
      "2058    45632.813323\n",
      "2059    46044.274761\n",
      "2060    46670.487908\n",
      "2061    47307.623122\n",
      "2062    47899.045863\n",
      "2063    48577.654623\n",
      "2064    49202.207953\n",
      "2065    49750.939896\n",
      "2066    50476.204450\n",
      "2067    51011.265428\n",
      "2068    51837.191861\n",
      "2069    52542.728150\n",
      "2070    53250.531721\n",
      "2071    54092.784456\n",
      "2072    54859.750883\n",
      "2073    55588.677864\n",
      "2074    56413.326554\n",
      "2075    56926.706464\n",
      "2076    57551.247323\n",
      "2077    58023.201122\n",
      "2078    58840.820230\n",
      "2079    59760.158783\n",
      "2080    60632.135931\n",
      "2081    61430.467111\n",
      "2082    62360.756941\n",
      "2083    63155.104390\n",
      "2084    64306.037569\n",
      "2085    65145.670797\n",
      "2086    65960.294003\n",
      "2087    66705.110031\n",
      "2088    66598.569393\n",
      "2089    67671.353234\n",
      "2090    68938.947547\n",
      "2091    70227.638182\n",
      "2092    71592.103268\n",
      "2093    72885.909168\n",
      "2094    74222.311968\n",
      "2095    75606.088373\n",
      "2096    77066.383968\n",
      "2097    78477.713522\n",
      "2098    79940.970305\n",
      "2099    81427.253695\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2077    13104.245560\n",
      "2078    12958.590159\n",
      "2079    12649.533476\n",
      "2080    12283.664009\n",
      "2081    12096.795857\n",
      "2082    11925.667999\n",
      "2083    11691.848218\n",
      "2084    11751.996376\n",
      "2085    11731.029852\n",
      "2086    11631.525125\n",
      "2087    11580.682616\n",
      "2088    11319.063137\n",
      "2089    11398.141194\n",
      "2090    11474.742377\n",
      "2091    11352.363352\n",
      "2092    11432.969177\n",
      "2093    11285.388138\n",
      "2094    11059.687843\n",
      "2095    11072.775390\n",
      "2096    10993.184304\n",
      "2097    10954.187421\n",
      "2098    10762.215346\n",
      "2099    10644.286774\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032     40582.427461\n",
      "2033     41261.724185\n",
      "2034     40811.202509\n",
      "2035     40725.226995\n",
      "2036     40995.025359\n",
      "            ...      \n",
      "2095    145433.625568\n",
      "2096    149515.959793\n",
      "2097    153560.745418\n",
      "2098    157797.281006\n",
      "2099    162104.943477\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p84_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2036    4.293849e+04\n",
      "2037    4.459172e+04\n",
      "2038    4.715718e+04\n",
      "2039    4.989501e+04\n",
      "2040    5.305176e+04\n",
      "            ...     \n",
      "2095    1.336785e+06\n",
      "2096    1.417947e+06\n",
      "2097    1.506312e+06\n",
      "2098    1.598602e+06\n",
      "2099    1.695796e+06\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Year\n",
      "2032     5902.392315\n",
      "2033     5825.317843\n",
      "2034     5868.860791\n",
      "2035     6002.879623\n",
      "2036     6119.433761\n",
      "            ...     \n",
      "2095    20153.393791\n",
      "2096    20681.644141\n",
      "2097    21231.284246\n",
      "2098    21793.253362\n",
      "2099    22365.553094\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2079    208430.678364\n",
      "2080    206517.900723\n",
      "2081    206743.423313\n",
      "2082    207657.824889\n",
      "2083    204565.495914\n",
      "2084    205680.911428\n",
      "2085    206632.362700\n",
      "2086    205545.946330\n",
      "2087    205068.879646\n",
      "2088    203316.998766\n",
      "2089    202564.873183\n",
      "2090    201576.281685\n",
      "2091    201682.625487\n",
      "2092    201256.147021\n",
      "2093    201979.459875\n",
      "2094    202027.483274\n",
      "2095    202485.941712\n",
      "2096    203800.836675\n",
      "2097    204185.454037\n",
      "2098    204397.137247\n",
      "2099    205107.075484\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p89_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2040     68588.993387\n",
      "2041     69065.574684\n",
      "2042     68725.393181\n",
      "2043     68805.992286\n",
      "2044     69813.269819\n",
      "2045     70770.439565\n",
      "2046     71735.814076\n",
      "2047     72767.288905\n",
      "2048     73835.465944\n",
      "2049     74885.565291\n",
      "2050     75796.205732\n",
      "2051     76844.097667\n",
      "2052     77959.539020\n",
      "2053     78170.954072\n",
      "2054     79105.540177\n",
      "2055     80170.130193\n",
      "2056     81168.216508\n",
      "2057     82226.986910\n",
      "2058     83210.503100\n",
      "2059     84262.868794\n",
      "2060     85509.202489\n",
      "2061     86538.391450\n",
      "2062     87721.673794\n",
      "2063     88730.310245\n",
      "2064     89944.549745\n",
      "2065     91211.056364\n",
      "2066     92383.577634\n",
      "2067     93437.796224\n",
      "2068     94384.607971\n",
      "2069     95570.696107\n",
      "2070     96651.317359\n",
      "2071     97790.290621\n",
      "2072     99287.449951\n",
      "2073    100546.878694\n",
      "2074    101965.939682\n",
      "2075    103364.514007\n",
      "2076    105075.298677\n",
      "2077    105714.088256\n",
      "2078    105791.396668\n",
      "2079    107621.255970\n",
      "2080    109718.009304\n",
      "2081    111697.629265\n",
      "2082    113764.487221\n",
      "2083    115836.034027\n",
      "2084    118079.111267\n",
      "2085    120229.709163\n",
      "2086    122482.851802\n",
      "2087    124771.851398\n",
      "2088    127097.783498\n",
      "2089    129412.333594\n",
      "2090    131823.175433\n",
      "2091    134284.450928\n",
      "2092    136874.917663\n",
      "2093    139331.281674\n",
      "2094    141853.878476\n",
      "2095    144496.128676\n",
      "2096    147254.431646\n",
      "2097    149939.777929\n",
      "2098    152711.246692\n",
      "2099    155513.118719\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2030    3.864936e+05\n",
      "2031    3.928381e+05\n",
      "2032    3.924756e+05\n",
      "2033    3.920686e+05\n",
      "2034    3.946281e+05\n",
      "            ...     \n",
      "2095    1.191459e+06\n",
      "2096    1.214373e+06\n",
      "2097    1.236044e+06\n",
      "2098    1.258965e+06\n",
      "2099    1.282157e+06\n",
      "Name: Load_sum, Length: 70, dtype: float64\n",
      "Year\n",
      "2036    24618.610769\n",
      "2037    23266.668307\n",
      "2038    22700.949130\n",
      "2039    23108.474561\n",
      "2040    23536.743802\n",
      "            ...     \n",
      "2095    60115.146352\n",
      "2096    61350.183057\n",
      "2097    62586.023525\n",
      "2098    63846.314818\n",
      "2099    65121.337758\n",
      "Name: Load_sum, Length: 64, dtype: float64\n",
      "Year\n",
      "2093    15109.289040\n",
      "2094    14335.941092\n",
      "2095    13627.446804\n",
      "2096    13817.522643\n",
      "2097    13202.552533\n",
      "2098    13127.606377\n",
      "2099    12907.180027\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2040     91434.087197\n",
      "2041     92509.691383\n",
      "2042     94743.601397\n",
      "2043     97165.895728\n",
      "2044     99541.029843\n",
      "2045    101690.083808\n",
      "2046    104360.193102\n",
      "2047    106480.641497\n",
      "2048    109070.819457\n",
      "2049    111195.920736\n",
      "2050    113778.376496\n",
      "2051    116598.848902\n",
      "2052    119757.123529\n",
      "2053    122189.689116\n",
      "2054    125213.280689\n",
      "2055    128062.938564\n",
      "2056    131358.105640\n",
      "2057    134567.488448\n",
      "2058    138051.848663\n",
      "2059    141370.538642\n",
      "2060    144858.219429\n",
      "2061    147780.841797\n",
      "2062    150938.205112\n",
      "2063    153880.658345\n",
      "2064    158050.166171\n",
      "2065    161741.401876\n",
      "2066    165573.579093\n",
      "2067    170073.327468\n",
      "2068    174369.524822\n",
      "2069    178491.917790\n",
      "2070    183006.760201\n",
      "2071    187636.949954\n",
      "2072    192166.845683\n",
      "2073    197185.064290\n",
      "2074    202369.464025\n",
      "2075    207899.977655\n",
      "2076    213473.421299\n",
      "2077    218528.965023\n",
      "2078    224105.310773\n",
      "2079    229662.071922\n",
      "2080    236063.015335\n",
      "2081    241667.564011\n",
      "2082    248758.316094\n",
      "2083    254566.444394\n",
      "2084    261326.033536\n",
      "2085    268001.737722\n",
      "2086    275147.087481\n",
      "2087    281484.172207\n",
      "2088    283275.166790\n",
      "2089    284562.470243\n",
      "2090    292989.875111\n",
      "2091    302038.235210\n",
      "2092    311516.076551\n",
      "2093    320752.347919\n",
      "2094    330371.901105\n",
      "2095    340518.776072\n",
      "2096    351086.037699\n",
      "2097    361643.479117\n",
      "2098    372629.718564\n",
      "2099    383920.109825\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2083    32217.698334\n",
      "2085    31835.066840\n",
      "2086    32578.283887\n",
      "2087    32292.692415\n",
      "2088    32700.504202\n",
      "2089    33188.203675\n",
      "2090    33255.798661\n",
      "2091    33572.623647\n",
      "2092    33574.123767\n",
      "2093    33880.856695\n",
      "2094    34029.435305\n",
      "2095    34190.647011\n",
      "2096    34409.186471\n",
      "2097    34584.863135\n",
      "2098    34813.480387\n",
      "2099    35028.984848\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p85_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2040    1.698693e+05\n",
      "2041    1.777291e+05\n",
      "2042    1.887531e+05\n",
      "2043    2.012010e+05\n",
      "2044    2.135453e+05\n",
      "2045    2.264478e+05\n",
      "2046    2.393910e+05\n",
      "2047    2.540767e+05\n",
      "2048    2.692414e+05\n",
      "2049    2.844809e+05\n",
      "2050    3.014574e+05\n",
      "2051    3.187135e+05\n",
      "2052    3.371561e+05\n",
      "2053    3.564206e+05\n",
      "2054    3.769405e+05\n",
      "2055    3.986169e+05\n",
      "2056    4.206685e+05\n",
      "2057    4.458998e+05\n",
      "2058    4.699411e+05\n",
      "2059    4.957556e+05\n",
      "2060    5.245396e+05\n",
      "2061    5.546738e+05\n",
      "2062    5.866022e+05\n",
      "2063    6.204655e+05\n",
      "2064    6.579065e+05\n",
      "2065    6.955867e+05\n",
      "2066    7.366235e+05\n",
      "2067    7.785207e+05\n",
      "2068    8.248529e+05\n",
      "2069    8.732778e+05\n",
      "2070    9.240176e+05\n",
      "2071    9.774939e+05\n",
      "2072    1.035166e+06\n",
      "2073    1.094917e+06\n",
      "2074    1.159279e+06\n",
      "2075    1.226630e+06\n",
      "2076    1.298336e+06\n",
      "2077    1.372363e+06\n",
      "2078    1.451469e+06\n",
      "2079    1.536241e+06\n",
      "2080    1.626975e+06\n",
      "2081    1.722278e+06\n",
      "2082    1.821654e+06\n",
      "2083    1.927770e+06\n",
      "2084    2.042431e+06\n",
      "2085    2.160154e+06\n",
      "2086    2.288461e+06\n",
      "2087    2.423141e+06\n",
      "2088    2.569974e+06\n",
      "2089    2.723741e+06\n",
      "2090    2.885026e+06\n",
      "2091    3.056479e+06\n",
      "2092    3.239069e+06\n",
      "2093    3.431020e+06\n",
      "2094    3.633446e+06\n",
      "2095    3.849951e+06\n",
      "2096    4.079719e+06\n",
      "2097    4.321054e+06\n",
      "2098    4.577121e+06\n",
      "2099    4.848213e+06\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2093    2.618317e+07\n",
      "2094    2.745156e+07\n",
      "2095    2.844001e+07\n",
      "2096    2.907432e+07\n",
      "2097    2.968087e+07\n",
      "2098    3.037656e+07\n",
      "2099    3.134424e+07\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2060    21256.832908\n",
      "2061    21545.353017\n",
      "2062    21405.297225\n",
      "2063    21503.533693\n",
      "2064    21653.783283\n",
      "2065    21756.993485\n",
      "2066    21907.056199\n",
      "2067    22078.360931\n",
      "2068    22189.034006\n",
      "2069    22306.076349\n",
      "2070    22412.497920\n",
      "2071    22564.967989\n",
      "2072    22679.091334\n",
      "2073    22825.145924\n",
      "2074    22927.592178\n",
      "2075    23012.483329\n",
      "2076    23172.908828\n",
      "2077    23264.681552\n",
      "2078    23399.528620\n",
      "2079    23526.975700\n",
      "2080    23665.273925\n",
      "2081    23762.067630\n",
      "2082    23856.161726\n",
      "2083    23984.553070\n",
      "2084    24083.049514\n",
      "2085    24210.067524\n",
      "2086    24351.142406\n",
      "2087    24472.886732\n",
      "2088    24567.243711\n",
      "2089    24669.540486\n",
      "2090    24777.671077\n",
      "2091    24936.772765\n",
      "2092    25085.643716\n",
      "2093    25140.745216\n",
      "2094    25277.568412\n",
      "2095    25420.301289\n",
      "2096    25483.662481\n",
      "2097    25643.435432\n",
      "2098    25780.136314\n",
      "2099    25863.577741\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p57_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p51_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Michigan_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2060    39654.439610\n",
      "2061    39183.399923\n",
      "2062    39010.369731\n",
      "2063    39144.985551\n",
      "2064    39274.247133\n",
      "2065    39183.646306\n",
      "2066    38823.910714\n",
      "2067    38974.302245\n",
      "2068    38455.694555\n",
      "2069    38567.890083\n",
      "2070    38583.762259\n",
      "2071    38485.925697\n",
      "2072    38280.574310\n",
      "2073    38253.058139\n",
      "2074    38319.487951\n",
      "2075    38363.705038\n",
      "2076    38435.131148\n",
      "2077    38250.384295\n",
      "2078    38392.800229\n",
      "2079    38406.874144\n",
      "2080    38401.317300\n",
      "2081    38354.968733\n",
      "2082    38422.309184\n",
      "2083    38331.897930\n",
      "2084    38464.365472\n",
      "2085    38505.807157\n",
      "2086    38508.691101\n",
      "2087    38536.779257\n",
      "2088    38383.404325\n",
      "2089    38427.819501\n",
      "2090    38302.345094\n",
      "2091    38431.029870\n",
      "2092    38261.934210\n",
      "2093    38234.880097\n",
      "2094    38202.003082\n",
      "2095    38215.797057\n",
      "2096    38196.644392\n",
      "2097    38297.676034\n",
      "2098    38273.854649\n",
      "2099    38304.312667\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2077    74894.946568\n",
      "2078    64655.813508\n",
      "2079    66867.799069\n",
      "2080    66166.070614\n",
      "2081    62479.027663\n",
      "2082    62464.404764\n",
      "2083    60150.021595\n",
      "2084    63937.578726\n",
      "2085    61479.452026\n",
      "2086    57552.098131\n",
      "2087    59287.696726\n",
      "2088    59427.133956\n",
      "2089    59181.998355\n",
      "2090    58185.021685\n",
      "2091    58359.360331\n",
      "2092    57395.847423\n",
      "2093    55195.716502\n",
      "2094    53940.668951\n",
      "2095    53296.941264\n",
      "2096    51317.289040\n",
      "2097    49657.217174\n",
      "2098    48791.123864\n",
      "2099    46047.835711\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2054    151224.355377\n",
      "2055    150442.135575\n",
      "2056    150719.966633\n",
      "2057    151012.441254\n",
      "2058    154878.278049\n",
      "2059    156991.832061\n",
      "2060    160816.550461\n",
      "2061    165366.512862\n",
      "2062    168745.256117\n",
      "2063    173455.158936\n",
      "2064    176510.259927\n",
      "2065    180843.979123\n",
      "2066    182698.368991\n",
      "2067    185800.090281\n",
      "2068    189507.575924\n",
      "2069    192529.820568\n",
      "2070    194964.542137\n",
      "2071    197973.445095\n",
      "2072    200474.469871\n",
      "2073    203180.902017\n",
      "2074    206886.559396\n",
      "2075    208963.675351\n",
      "2076    212636.840248\n",
      "2077    215327.331314\n",
      "2078    218256.642200\n",
      "2079    221817.181834\n",
      "2080    225414.207453\n",
      "2081    228469.848578\n",
      "2082    231436.876422\n",
      "2083    233196.128105\n",
      "2084    237168.968964\n",
      "2085    240002.773170\n",
      "2086    243280.871060\n",
      "2087    246260.565160\n",
      "2088    249347.845386\n",
      "2089    253304.563060\n",
      "2090    256627.097030\n",
      "2091    260010.320499\n",
      "2092    264306.508321\n",
      "2093    267925.038339\n",
      "2094    270795.024880\n",
      "2095    275259.086171\n",
      "2096    278932.474342\n",
      "2097    283503.076164\n",
      "2098    287674.159228\n",
      "2099    292000.694002\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p106_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: North Carolina_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p100_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2091    410818.157505\n",
      "2093    415196.595139\n",
      "2094    414273.578868\n",
      "2095    420373.817811\n",
      "2096    423019.564318\n",
      "2097    424288.174657\n",
      "2098    423909.605622\n",
      "2099    423455.403800\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p121_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2095    2845.582246\n",
      "2096    4600.798888\n",
      "2097     684.259701\n",
      "2098    1930.559509\n",
      "2099      -0.656989\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2052    103179.919317\n",
      "2053     92063.858893\n",
      "2054     89799.006998\n",
      "2055     88144.641617\n",
      "2056     85833.554046\n",
      "2057     82724.741481\n",
      "2058     81326.681049\n",
      "2059     79938.208512\n",
      "2060     78551.034248\n",
      "2061     77164.165649\n",
      "2062     76744.241949\n",
      "2063     75730.246432\n",
      "2064     74863.217027\n",
      "2065     72126.208918\n",
      "2066     69980.833150\n",
      "2067     68718.299241\n",
      "2068     67604.108506\n",
      "2069     66108.099695\n",
      "2070     63548.679768\n",
      "2071     61778.970300\n",
      "2072     59169.908930\n",
      "2073     58506.375522\n",
      "2074     55844.196731\n",
      "2075     54133.031984\n",
      "2076     52071.886062\n",
      "2077     49191.991476\n",
      "2078     46634.823446\n",
      "2079     44037.330632\n",
      "2080     40326.052735\n",
      "2081     37889.170513\n",
      "2082     35086.370781\n",
      "2083     32285.667558\n",
      "2084     30261.462442\n",
      "2085     27615.869556\n",
      "2086     25102.063459\n",
      "2087     22341.775909\n",
      "2088     19949.524591\n",
      "2089     16811.132302\n",
      "2090     14564.739651\n",
      "2091     12196.615089\n",
      "2092      9217.013643\n",
      "2093      6506.972086\n",
      "2094      3508.023296\n",
      "2095       492.721005\n",
      "2096     -2368.087126\n",
      "2097     -5182.821387\n",
      "2098     -8004.909708\n",
      "2099    -10806.548584\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2060    372522.395493\n",
      "2061    379739.941573\n",
      "2062    382230.183430\n",
      "2063    381935.113586\n",
      "2064    382577.471116\n",
      "2065    383014.841303\n",
      "2066    384004.227004\n",
      "2067    382196.404325\n",
      "2068    380214.245467\n",
      "2069    379870.304259\n",
      "2070    378902.251200\n",
      "2071    380615.922966\n",
      "2072    382172.729903\n",
      "2073    383314.726606\n",
      "2074    386113.419736\n",
      "2075    386363.166763\n",
      "2076    390128.311699\n",
      "2077    390884.237813\n",
      "2078    392429.390313\n",
      "2079    394342.449777\n",
      "2080    396203.184577\n",
      "2081    395564.023352\n",
      "2082    397620.636500\n",
      "2083    398449.527383\n",
      "2084    400244.046480\n",
      "2085    401465.399409\n",
      "2086    404082.344844\n",
      "2087    405155.602864\n",
      "2088    406969.685144\n",
      "2089    409792.586876\n",
      "2090    410319.517395\n",
      "2091    413311.479196\n",
      "2092    415757.972999\n",
      "2093    416952.056673\n",
      "2094    417867.306704\n",
      "2095    418952.630577\n",
      "2096    421761.038373\n",
      "2097    423225.142883\n",
      "2098    425816.668505\n",
      "2099    427286.349376\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p50_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2065    10160.928044\n",
      "2066    10349.565218\n",
      "2067    10229.531770\n",
      "2068    10184.118889\n",
      "2069    10111.060531\n",
      "2070    10111.829640\n",
      "2071    10115.709492\n",
      "2072    10107.513125\n",
      "2073    10094.394032\n",
      "2074    10052.256547\n",
      "2075    10062.103572\n",
      "2076    10062.061093\n",
      "2077    10014.754046\n",
      "2078    10037.011086\n",
      "2079    10062.953981\n",
      "2080    10034.361922\n",
      "2081    10056.140181\n",
      "2082    10045.343286\n",
      "2083    10021.882767\n",
      "2084    10031.124534\n",
      "2085    10043.004401\n",
      "2086    10027.256295\n",
      "2087    10028.608226\n",
      "2088    10004.855634\n",
      "2089    10005.044815\n",
      "2090    10016.663112\n",
      "2091    10029.819321\n",
      "2092    10017.382660\n",
      "2093    10008.765175\n",
      "2094     9990.989720\n",
      "2095     9995.854596\n",
      "2096    10005.811073\n",
      "2097    10007.521944\n",
      "2098    10004.117592\n",
      "2099     9987.600081\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2033    134762.591332\n",
      "2034    138363.872522\n",
      "2035    137952.744534\n",
      "2036    141308.034819\n",
      "2037    144887.427154\n",
      "            ...      \n",
      "2095    536439.031782\n",
      "2096    549996.616643\n",
      "2097    563731.757243\n",
      "2098    577666.785684\n",
      "2099    591752.008499\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Year\n",
      "2054    3787.791450\n",
      "2055    3581.207624\n",
      "2056    3749.116874\n",
      "2057    3516.522942\n",
      "2058    3654.348307\n",
      "2059    3672.912841\n",
      "2060    3625.081586\n",
      "2061    3608.170533\n",
      "2062    3676.644655\n",
      "2063    3661.056432\n",
      "2064    3673.393039\n",
      "2065    3697.432901\n",
      "2066    3667.826616\n",
      "2067    3641.774355\n",
      "2068    3634.010251\n",
      "2069    3586.075001\n",
      "2070    3572.558300\n",
      "2071    3544.733598\n",
      "2072    3527.759942\n",
      "2073    3499.685553\n",
      "2074    3465.231549\n",
      "2075    3456.957542\n",
      "2076    3416.426160\n",
      "2077    3405.648576\n",
      "2078    3367.950480\n",
      "2079    3326.590608\n",
      "2080    3325.809320\n",
      "2081    3276.385958\n",
      "2082    3270.001262\n",
      "2083    3197.159515\n",
      "2084    3204.907132\n",
      "2085    3149.401049\n",
      "2086    3120.384156\n",
      "2087    3076.673152\n",
      "2088    3039.301107\n",
      "2089    3019.220299\n",
      "2090    2995.000411\n",
      "2091    2963.631664\n",
      "2092    2956.073936\n",
      "2093    2946.465621\n",
      "2094    2896.952597\n",
      "2095    2890.038286\n",
      "2096    2860.311231\n",
      "2097    2842.123593\n",
      "2098    2811.700766\n",
      "2099    2793.462804\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p107_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p101_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p126_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p120_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2051    52047.972794\n",
      "2053    52365.835506\n",
      "2054    53249.241427\n",
      "2055    54015.611731\n",
      "2056    54436.277415\n",
      "2057    55056.497416\n",
      "2058    55487.559156\n",
      "2059    55957.261330\n",
      "2060    56545.921741\n",
      "2061    57209.095095\n",
      "2062    57685.506612\n",
      "2063    58460.972450\n",
      "2064    58944.368341\n",
      "2065    59422.653947\n",
      "2066    60021.993922\n",
      "2067    60601.741041\n",
      "2068    61327.683530\n",
      "2069    61943.123844\n",
      "2070    62615.946079\n",
      "2071    63170.216618\n",
      "2072    63826.868147\n",
      "2073    64457.941959\n",
      "2074    65171.379424\n",
      "2075    65742.391417\n",
      "2076    66437.938903\n",
      "2077    67006.548402\n",
      "2078    67693.723243\n",
      "2079    68188.891841\n",
      "2080    69015.601104\n",
      "2081    69769.342365\n",
      "2082    70240.914354\n",
      "2083    70991.314258\n",
      "2084    71744.927376\n",
      "2085    72341.042735\n",
      "2086    72941.560493\n",
      "2087    73756.767823\n",
      "2088    74468.651355\n",
      "2089    74954.775946\n",
      "2090    75662.309587\n",
      "2091    76351.010846\n",
      "2092    77036.806404\n",
      "2093    77747.974294\n",
      "2094    78348.514662\n",
      "2095    79110.295362\n",
      "2096    79786.803633\n",
      "2097    80522.219441\n",
      "2098    81165.582800\n",
      "2099    81636.669098\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2029    1.354325e+05\n",
      "2030    1.380458e+05\n",
      "2031    1.408590e+05\n",
      "2032    1.435275e+05\n",
      "2033    1.467931e+05\n",
      "            ...     \n",
      "2095    9.902978e+05\n",
      "2096    1.022318e+06\n",
      "2097    1.053967e+06\n",
      "2098    1.087186e+06\n",
      "2099    1.121252e+06\n",
      "Name: Load_sum, Length: 71, dtype: float64\n",
      "Year\n",
      "2033    2.100965e+05\n",
      "2034    2.131923e+05\n",
      "2035    2.176539e+05\n",
      "2036    2.221547e+05\n",
      "2037    2.257424e+05\n",
      "            ...     \n",
      "2095    9.749703e+05\n",
      "2096    1.006203e+06\n",
      "2097    1.036753e+06\n",
      "2098    1.068495e+06\n",
      "2099    1.100599e+06\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: p96_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2033    19124.819349\n",
      "2034    19449.133902\n",
      "2035    19802.191865\n",
      "2036    20066.264503\n",
      "2037    20047.749365\n",
      "            ...     \n",
      "2095    21657.636103\n",
      "2096    21806.901415\n",
      "2097    21949.263904\n",
      "2098    22095.722690\n",
      "2099    22235.926378\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: p90_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2037    3341.549903\n",
      "2038    3656.837614\n",
      "2039    4503.321530\n",
      "2040    4700.688859\n",
      "2041    4742.341585\n",
      "           ...     \n",
      "2095    3581.400283\n",
      "2096    3593.767508\n",
      "2097    3598.257482\n",
      "2098    3607.462156\n",
      "2099    3617.795183\n",
      "Name: Load_sum, Length: 63, dtype: float64\n",
      "Year\n",
      "2054    232881.984583\n",
      "2055    230184.485220\n",
      "2056    226498.253661\n",
      "2057    226239.773336\n",
      "2058    224426.132790\n",
      "2059    224575.775740\n",
      "2060    223337.809979\n",
      "2061    223817.046900\n",
      "2062    224200.589244\n",
      "2063    224642.660569\n",
      "2064    224964.047108\n",
      "2065    225088.284859\n",
      "2066    224990.271724\n",
      "2067    225212.467728\n",
      "2068    225881.702402\n",
      "2069    225603.651268\n",
      "2070    225585.473846\n",
      "2071    225613.407655\n",
      "2072    225284.476386\n",
      "2073    226237.351779\n",
      "2074    226406.145803\n",
      "2075    225888.926381\n",
      "2076    226234.858714\n",
      "2077    225729.797181\n",
      "2078    226155.373405\n",
      "2079    226056.813759\n",
      "2080    226271.901424\n",
      "2081    225983.177935\n",
      "2082    225733.485181\n",
      "2083    225428.217178\n",
      "2084    225223.177199\n",
      "2085    225129.030386\n",
      "2086    225095.443457\n",
      "2087    224986.496013\n",
      "2088    225135.918021\n",
      "2089    224294.456593\n",
      "2090    224090.087263\n",
      "2091    224036.438233\n",
      "2092    224413.624530\n",
      "2093    224122.321836\n",
      "2094    223964.306205\n",
      "2095    223839.507501\n",
      "2096    224026.585718\n",
      "2097    223243.850954\n",
      "2098    222993.993750\n",
      "2099    222961.848627\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Ohio_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2077    135924.064476\n",
      "2078    141671.103560\n",
      "2079    142063.617729\n",
      "2080    136001.379757\n",
      "2081    136231.589941\n",
      "2082    135957.661957\n",
      "2083    135510.264613\n",
      "2084    135686.092840\n",
      "2085    135208.840977\n",
      "2086    134588.986787\n",
      "2087    134103.309495\n",
      "2088    133891.093380\n",
      "2089    133639.781654\n",
      "2090    133320.629300\n",
      "2091    133068.620283\n",
      "2092    132920.958576\n",
      "2093    132353.779845\n",
      "2094    131777.961943\n",
      "2095    131590.245273\n",
      "2096    131378.530313\n",
      "2097    131420.690178\n",
      "2098    130606.590658\n",
      "2099    130471.600583\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032     3766.951950\n",
      "2033     3936.241936\n",
      "2034     4285.438849\n",
      "2035     4441.517905\n",
      "2036     4590.901181\n",
      "            ...     \n",
      "2095    20613.099228\n",
      "2096    21313.300172\n",
      "2097    21970.523907\n",
      "2098    22682.947045\n",
      "2099    23417.273925\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Skipping empty file: p97_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2033    135509.385813\n",
      "2034    131743.490966\n",
      "2035    131384.108526\n",
      "2036    134629.581994\n",
      "2037    137977.736333\n",
      "            ...      \n",
      "2095    514183.278241\n",
      "2096    527175.838310\n",
      "2097    540380.755954\n",
      "2098    553751.697647\n",
      "2099    567261.927818\n",
      "Name: Load_sum, Length: 67, dtype: float64\n",
      "Skipping empty file: p91_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2032    3193.004502\n",
      "2033    3104.126525\n",
      "2034    3429.670116\n",
      "2035    3670.690307\n",
      "2036    3694.120783\n",
      "           ...     \n",
      "2095    3949.744457\n",
      "2096    3975.499386\n",
      "2097    3994.819374\n",
      "2098    4015.449736\n",
      "2099    4035.062744\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2056    200867.520555\n",
      "2057    198036.665574\n",
      "2058    195740.003842\n",
      "2059    195781.785743\n",
      "2060    194408.046414\n",
      "2061    194254.412926\n",
      "2062    194887.270551\n",
      "2063    194517.128006\n",
      "2064    195193.922461\n",
      "2065    195393.033290\n",
      "2066    195204.006799\n",
      "2067    195662.530034\n",
      "2068    195728.280545\n",
      "2069    196145.976479\n",
      "2070    196392.423766\n",
      "2071    196283.431602\n",
      "2072    196496.613360\n",
      "2073    196633.198231\n",
      "2074    196743.046844\n",
      "2075    196824.901121\n",
      "2076    197019.467158\n",
      "2077    196836.660234\n",
      "2078    196942.878497\n",
      "2079    196939.272986\n",
      "2080    197316.141740\n",
      "2081    196651.221029\n",
      "2082    196645.045630\n",
      "2083    196107.101325\n",
      "2084    196412.525037\n",
      "2085    195984.092151\n",
      "2086    196445.535108\n",
      "2087    196207.122468\n",
      "2088    195600.707207\n",
      "2089    195886.041887\n",
      "2090    195656.824876\n",
      "2091    195402.477295\n",
      "2092    195158.649678\n",
      "2093    194930.129586\n",
      "2094    194642.905996\n",
      "2095    194911.147005\n",
      "2096    195082.748142\n",
      "2097    194921.748123\n",
      "2098    194803.092182\n",
      "2099    194584.143455\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2041    234610.786845\n",
      "2042    233900.656011\n",
      "2043    233634.786490\n",
      "2044    235340.944888\n",
      "2045    238365.280191\n",
      "2046    238430.437874\n",
      "2047    241387.759776\n",
      "2048    244945.225128\n",
      "2049    247951.111112\n",
      "2050    252004.750075\n",
      "2051    255550.614576\n",
      "2052    259567.846938\n",
      "2053    263115.629452\n",
      "2054    267256.491805\n",
      "2055    270442.071172\n",
      "2056    273718.692690\n",
      "2057    276860.475523\n",
      "2058    279703.744518\n",
      "2059    283828.333312\n",
      "2060    288065.697285\n",
      "2061    292014.273838\n",
      "2062    296252.317088\n",
      "2063    300454.165957\n",
      "2064    304957.113992\n",
      "2065    309030.956605\n",
      "2066    313049.712091\n",
      "2067    317625.316037\n",
      "2068    322399.817500\n",
      "2069    327245.040880\n",
      "2070    332269.303066\n",
      "2071    337111.560368\n",
      "2072    341513.772495\n",
      "2073    346965.892365\n",
      "2074    351689.972895\n",
      "2075    356531.324406\n",
      "2076    361459.617878\n",
      "2077    366114.726108\n",
      "2078    370075.597376\n",
      "2079    374661.600167\n",
      "2080    378314.663724\n",
      "2081    382109.842539\n",
      "2082    385722.185900\n",
      "2083    390724.222847\n",
      "2084    397424.264701\n",
      "2085    404014.283193\n",
      "2086    410844.013031\n",
      "2087    417735.625376\n",
      "2088    424672.566974\n",
      "2089    431712.368376\n",
      "2090    438818.550491\n",
      "2091    446169.717891\n",
      "2092    453771.353087\n",
      "2093    461126.485286\n",
      "2094    468593.296477\n",
      "2095    476401.447881\n",
      "2096    484384.296749\n",
      "2097    492256.582629\n",
      "2098    500323.098896\n",
      "2099    508489.537563\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2048    11005.029809\n",
      "2049    10645.040902\n",
      "2050    10595.753976\n",
      "2051    10574.150071\n",
      "2052    10664.422185\n",
      "2053    10698.379210\n",
      "2054    10749.660361\n",
      "2055    10745.303597\n",
      "2056    10808.407126\n",
      "2057    10819.037624\n",
      "2058    10848.152894\n",
      "2059    10871.538609\n",
      "2060    10874.799566\n",
      "2061    10899.980895\n",
      "2062    10902.252774\n",
      "2063    10909.796991\n",
      "2064    10920.322371\n",
      "2065    10917.487332\n",
      "2066    10904.897754\n",
      "2067    10925.393400\n",
      "2068    10931.341379\n",
      "2069    10913.296122\n",
      "2070    10929.086018\n",
      "2071    10926.268179\n",
      "2072    10891.470142\n",
      "2073    10893.828811\n",
      "2074    10894.797827\n",
      "2075    10847.433707\n",
      "2076    10856.348094\n",
      "2077    10835.258337\n",
      "2078    10841.606550\n",
      "2079    10845.739458\n",
      "2080    10826.692939\n",
      "2081    10790.448522\n",
      "2082    10787.594281\n",
      "2083    10786.486251\n",
      "2084    10790.416021\n",
      "2085    10765.666785\n",
      "2086    10768.089121\n",
      "2087    10748.228455\n",
      "2088    10743.640274\n",
      "2089    10736.814065\n",
      "2090    10741.359185\n",
      "2091    10697.802139\n",
      "2092    10700.979279\n",
      "2093    10656.858546\n",
      "2094    10646.979176\n",
      "2095    10628.780386\n",
      "2096    10635.260047\n",
      "2097    10584.329311\n",
      "2098    10563.988549\n",
      "2099    10542.972954\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2042     53258.508544\n",
      "2043     52525.962128\n",
      "2044     53193.906833\n",
      "2045     53945.753428\n",
      "2046     54962.595755\n",
      "2047     55774.310193\n",
      "2048     56726.032735\n",
      "2049     57304.830827\n",
      "2050     58304.558480\n",
      "2051     59050.273790\n",
      "2052     59713.507142\n",
      "2053     60439.258854\n",
      "2054     61218.570171\n",
      "2055     61897.729196\n",
      "2056     62754.515711\n",
      "2057     63567.982262\n",
      "2058     64262.099622\n",
      "2059     64941.343807\n",
      "2060     65513.734425\n",
      "2061     66329.932896\n",
      "2062     66980.119537\n",
      "2063     67632.314909\n",
      "2064     68438.962219\n",
      "2065     68831.186978\n",
      "2066     69468.903075\n",
      "2067     70193.214506\n",
      "2068     70970.426725\n",
      "2069     71908.795783\n",
      "2070     72681.342847\n",
      "2071     73492.833562\n",
      "2072     74309.644172\n",
      "2073     75188.931696\n",
      "2074     76073.337836\n",
      "2075     77004.997776\n",
      "2076     77875.734907\n",
      "2077     78725.088351\n",
      "2078     79712.052746\n",
      "2079     80609.314194\n",
      "2080     81704.567532\n",
      "2081     82649.784727\n",
      "2082     83710.090728\n",
      "2083     84754.372796\n",
      "2084     85875.870147\n",
      "2085     87019.837948\n",
      "2086     88217.091060\n",
      "2087     89529.052645\n",
      "2088     90622.106598\n",
      "2089     91841.948590\n",
      "2090     93059.783188\n",
      "2091     94309.951036\n",
      "2092     95584.133508\n",
      "2093     96809.641333\n",
      "2094     97983.005483\n",
      "2095     99318.491087\n",
      "2096    100657.204562\n",
      "2097    102013.614523\n",
      "2098    103352.469518\n",
      "2099    104702.215832\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2032    2.847035e+05\n",
      "2033    2.907089e+05\n",
      "2034    2.939330e+05\n",
      "2035    2.982999e+05\n",
      "2036    3.044247e+05\n",
      "            ...     \n",
      "2095    1.373732e+06\n",
      "2096    1.416897e+06\n",
      "2097    1.459081e+06\n",
      "2098    1.503108e+06\n",
      "2099    1.547672e+06\n",
      "Name: Load_sum, Length: 68, dtype: float64\n",
      "Year\n",
      "2086    6938.153576\n",
      "2087    6898.896135\n",
      "2088    6926.291344\n",
      "2089    7072.321273\n",
      "2090    7148.845677\n",
      "2091    7324.704683\n",
      "2092    7465.769176\n",
      "2093    7531.993513\n",
      "2094    7673.194757\n",
      "2095    7741.787251\n",
      "2096    7897.380972\n",
      "2097    8040.322861\n",
      "2098    8185.419200\n",
      "2099    8316.482109\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p62_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p64_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p112_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p114_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2083    32217.698334\n",
      "2085    31835.066840\n",
      "2086    32578.283887\n",
      "2087    32292.692415\n",
      "2088    32700.504202\n",
      "2089    33188.203675\n",
      "2090    33255.798661\n",
      "2091    33572.623647\n",
      "2092    33574.123767\n",
      "2093    33880.856695\n",
      "2094    34029.435305\n",
      "2095    34190.647011\n",
      "2096    34409.186471\n",
      "2097    34584.863135\n",
      "2098    34813.480387\n",
      "2099    35028.984848\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p118_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2054    13562.665068\n",
      "2055    13360.380886\n",
      "2056    13155.598373\n",
      "2057    13067.311408\n",
      "2058    13051.685959\n",
      "2059    13036.485263\n",
      "2060    13010.208979\n",
      "2061    12988.684725\n",
      "2062    12968.433432\n",
      "2063    12956.939170\n",
      "2064    12964.491431\n",
      "2065    12926.543456\n",
      "2066    12907.939448\n",
      "2067    12915.648408\n",
      "2068    12893.041445\n",
      "2069    12884.358924\n",
      "2070    12872.549605\n",
      "2071    12862.229174\n",
      "2072    12833.896777\n",
      "2073    12826.144594\n",
      "2074    12814.663531\n",
      "2075    12785.367742\n",
      "2076    12773.887565\n",
      "2077    12767.013233\n",
      "2078    12730.543387\n",
      "2079    12721.963034\n",
      "2080    12719.309728\n",
      "2081    12704.014167\n",
      "2082    12679.731091\n",
      "2083    12628.721355\n",
      "2084    12626.949420\n",
      "2085    12593.104284\n",
      "2086    12569.801958\n",
      "2087    12548.548763\n",
      "2088    12504.830805\n",
      "2089    12483.357923\n",
      "2090    12477.150494\n",
      "2091    12461.570507\n",
      "2092    12454.746316\n",
      "2093    12438.196878\n",
      "2094    12418.848031\n",
      "2095    12375.037341\n",
      "2096    12361.285186\n",
      "2097    12315.947436\n",
      "2098    12331.033211\n",
      "2099    12282.795378\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2054    28592.005649\n",
      "2055    28893.247386\n",
      "2056    29266.689112\n",
      "2057    29546.628160\n",
      "2058    30086.355984\n",
      "2059    30601.735814\n",
      "2060    31150.589320\n",
      "2061    31764.815632\n",
      "2062    32419.418305\n",
      "2063    32905.960016\n",
      "2064    33524.385578\n",
      "2065    34094.411785\n",
      "2066    34486.389683\n",
      "2067    35068.531003\n",
      "2068    35590.223457\n",
      "2069    36084.476287\n",
      "2070    36609.676848\n",
      "2071    37081.884653\n",
      "2072    37571.336620\n",
      "2073    38178.578772\n",
      "2074    38673.486580\n",
      "2075    39294.391872\n",
      "2076    39844.654482\n",
      "2077    40272.220939\n",
      "2078    40807.394370\n",
      "2079    41366.839364\n",
      "2080    41946.019586\n",
      "2081    42508.022786\n",
      "2082    42967.717952\n",
      "2083    43469.560655\n",
      "2084    44030.477206\n",
      "2085    44687.509914\n",
      "2086    45135.074033\n",
      "2087    45730.260813\n",
      "2088    46295.957770\n",
      "2089    47022.999167\n",
      "2090    47484.910183\n",
      "2091    48242.334676\n",
      "2092    48872.418241\n",
      "2093    49487.608297\n",
      "2094    49965.857981\n",
      "2095    50666.116872\n",
      "2096    51277.235975\n",
      "2097    52011.481221\n",
      "2098    52639.845593\n",
      "2099    53266.083966\n",
      "Name: Load_sum, dtype: float64\n",
      "Year\n",
      "2040    5843.421025\n",
      "2041    5705.416593\n",
      "2042    5643.517159\n",
      "2043    5658.657699\n",
      "2044    5691.528767\n",
      "2045    5714.782921\n",
      "2046    5732.434476\n",
      "2047    5744.054923\n",
      "2048    5763.688014\n",
      "2049    5762.217465\n",
      "2050    5782.494355\n",
      "2051    5797.799680\n",
      "2052    5801.335681\n",
      "2053    5808.143556\n",
      "2054    5819.268176\n",
      "2055    5812.648870\n",
      "2056    5817.514533\n",
      "2057    5826.177688\n",
      "2058    5826.775502\n",
      "2059    5816.268757\n",
      "2060    5825.511718\n",
      "2061    5811.101991\n",
      "2062    5803.648440\n",
      "2063    5804.215365\n",
      "2064    5802.620805\n",
      "2065    5796.438748\n",
      "2066    5794.062817\n",
      "2067    5794.095294\n",
      "2068    5808.778268\n",
      "2069    5793.443489\n",
      "2070    5789.902512\n",
      "2071    5794.840846\n",
      "2072    5800.279453\n",
      "2073    5793.667580\n",
      "2074    5785.005515\n",
      "2075    5787.739184\n",
      "2076    5778.788057\n",
      "2077    5770.784186\n",
      "2078    5773.121550\n",
      "2079    5746.381175\n",
      "2080    5737.779287\n",
      "2081    5724.530503\n",
      "2082    5704.174018\n",
      "2083    5657.602312\n",
      "2084    5593.291527\n",
      "2085    5542.292747\n",
      "2086    5475.936693\n",
      "2087    5465.259663\n",
      "2088    5487.897017\n",
      "2089    5500.849257\n",
      "2090    5516.520547\n",
      "2091    5533.028854\n",
      "2092    5551.016859\n",
      "2093    5567.162428\n",
      "2094    5587.304943\n",
      "2095    5603.542494\n",
      "2096    5621.612619\n",
      "2097    5634.340318\n",
      "2098    5652.311894\n",
      "2099    5672.336623\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: p48_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2081    1.500692e+06\n",
      "2082    1.558535e+06\n",
      "2083    1.459866e+06\n",
      "2084    1.571518e+06\n",
      "2085    1.588928e+06\n",
      "2086    1.686532e+06\n",
      "2087    1.695951e+06\n",
      "2088    1.749252e+06\n",
      "2089    1.804534e+06\n",
      "2090    1.815354e+06\n",
      "2091    1.871628e+06\n",
      "2092    1.929871e+06\n",
      "2093    1.984335e+06\n",
      "2094    1.992883e+06\n",
      "2095    2.046447e+06\n",
      "2096    2.094376e+06\n",
      "2097    2.131554e+06\n",
      "2098    2.180918e+06\n",
      "2099    2.211392e+06\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Connecticut_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p65_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p113_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p115_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p132_extreme_demand_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p119_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2051    52047.972794\n",
      "2053    52365.835506\n",
      "2054    53249.241427\n",
      "2055    54015.611731\n",
      "2056    54436.277415\n",
      "2057    55056.497416\n",
      "2058    55487.559156\n",
      "2059    55957.261330\n",
      "2060    56545.921741\n",
      "2061    57209.095095\n",
      "2062    57685.506612\n",
      "2063    58460.972450\n",
      "2064    58944.368341\n",
      "2065    59422.653947\n",
      "2066    60021.993922\n",
      "2067    60601.741041\n",
      "2068    61327.683530\n",
      "2069    61943.123844\n",
      "2070    62615.946079\n",
      "2071    63170.216618\n",
      "2072    63826.868147\n",
      "2073    64457.941959\n",
      "2074    65171.379424\n",
      "2075    65742.391417\n",
      "2076    66437.938903\n",
      "2077    67006.548402\n",
      "2078    67693.723243\n",
      "2079    68188.891841\n",
      "2080    69015.601104\n",
      "2081    69769.342365\n",
      "2082    70240.914354\n",
      "2083    70991.314258\n",
      "2084    71744.927376\n",
      "2085    72341.042735\n",
      "2086    72941.560493\n",
      "2087    73756.767823\n",
      "2088    74468.651355\n",
      "2089    74954.775946\n",
      "2090    75662.309587\n",
      "2091    76351.010846\n",
      "2092    77036.806404\n",
      "2093    77747.974294\n",
      "2094    78348.514662\n",
      "2095    79110.295362\n",
      "2096    79786.803633\n",
      "2097    80522.219441\n",
      "2098    81165.582800\n",
      "2099    81636.669098\n",
      "Name: Load_sum, dtype: float64\n",
      "Skipping empty file: Virginia_extreme_demand_outliers_min_projection_project_.csv\n",
      "Year\n",
      "2095    495366.109476\n",
      "2096    493948.187874\n",
      "2097    487069.116186\n",
      "2098    477854.248440\n",
      "2099    483739.733790\n",
      "Name: Load_sum, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "def string_to_set(string):\n",
    "    # Remove curly braces and split the string\n",
    "    elements = string.replace('{', '').replace('}', '').split(',')\n",
    "    # Remove any extra whitespace and single quotes from each element\n",
    "    elements = [e.strip().replace(\"'\", \"\") for e in elements]\n",
    "    return set(elements)\n",
    "\n",
    "for case in cases:  # Expand cases as needed\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/output/future_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier_demand'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    for rb_code in rb_codes:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "\n",
    "        ci_max_path = os.path.join(ci_directory, f'{rb_code}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{rb_code}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "            \n",
    "            for year in years:\n",
    "                weather_file_path = os.path.join(directory, f'{rb_code}_WRF_Hourly_Mean_Meteorology_{year}.csv')\n",
    "                demand_file_path = f'/Volumes/T7/prediction_project/{case}/{year}/{rb_code}_{year}_mlp_output_transformed.csv'\n",
    "\n",
    "                if os.path.exists(weather_file_path) and os.path.exists(demand_file_path):\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    demand_df = pd.read_csv(demand_file_path)\n",
    "                    \n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    demand_df['Date'] = pd.to_datetime(demand_df['Time_UTC']).dt.date\n",
    "                    \n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "                    grouped_demand = demand_df.groupby('Date')['Load'].agg(['sum'])  # Changed to sum\n",
    "                    \n",
    "                    # Merge to get Load sum values for dates above and below CI thresholds\n",
    "                    grouped = grouped.merge(grouped_demand, left_index=True, right_index=True, how='left')\n",
    "                    \n",
    "                    ci_max_month = ci_max_df[(ci_max_df['rb'] == rb_code)]\n",
    "                    ci_min_month = ci_min_df[(ci_min_df['rb'] == rb_code)]\n",
    "                    \n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]]\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]]\n",
    "                    \n",
    "                    for date, row in dates_above_max_ci.iterrows():\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['sum']})\n",
    "                    \n",
    "                    for date, row in dates_below_min_ci.iterrows():\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['sum']})\n",
    "        \n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "        \n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_demand_outliers_max_{case}_project_.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_demand_outliers_min_{case}_project_.csv'), index=False)\n",
    "\n",
    "\n",
    "    # Directories initialization\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/averaged_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier_demand'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    states=['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "            'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "            'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "            'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "            'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming','usa']\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    extreme_outliers_max_list = []\n",
    "    extreme_outliers_min_list = []\n",
    "\n",
    "    for state in states:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "        ci_max_path = os.path.join(ci_directory, f'{state}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{state}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "\n",
    "            \n",
    "            for year in years:\n",
    "                all_rb_dfs = []\n",
    "                weather_file_path = os.path.join(directory, f'{state}_averaged_weather_{year}.csv')\n",
    "                if os.path.exists(weather_file_path):\n",
    "                    if state == 'usa':\n",
    "                        # If state is USA, set rb_list to include all RB codes from p1 to p134\n",
    "                        rb_list = [f'p{i}' for i in range(1, 135)]\n",
    "                    else:\n",
    "                        state_to_ba_mapping = pd.read_csv('/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/state_to_ba_mapping.csv')\n",
    "                        state_to_ba_mapping=state_to_ba_mapping[state_to_ba_mapping['state']==state]\n",
    "                        \n",
    "                        rb_list = string_to_set(state_to_ba_mapping.iloc[0]['reeds_ba_list'])\n",
    "                    for rb_code in list(rb_list):\n",
    "                        demand_file_path = f'/Volumes/T7/prediction_project/{case}/{year}/{rb_code}_{year}_mlp_output_transformed.csv'\n",
    "                        demand_df = pd.read_csv(demand_file_path)\n",
    "                    \n",
    "                        demand_df['Date'] = pd.to_datetime(demand_df['Time_UTC']).dt.date\n",
    "                        \n",
    "                        grouped_demand = demand_df.groupby('Date')['Load'].agg(['sum']).rename(columns={'sum': rb_code})  # Rename to allow identification\n",
    "\n",
    "                        all_rb_dfs.append(grouped_demand)  \n",
    "                    if all_rb_dfs:\n",
    "                        total_load_df = pd.concat(all_rb_dfs, axis=1)\n",
    "\n",
    "                        # Sum across all numerical columns for each date to calculate 'Total_Load'\n",
    "                        total_load_df['Total_Load'] = total_load_df.select_dtypes(include=[np.number]).sum(axis=1)\n",
    "\n",
    "                        total_load_df.reset_index(inplace=True)  # Reset index to turn 'Date' back into a column\n",
    "                        \n",
    "                        \n",
    "\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    # Find the dates with the highest and lowest T2 for each day\n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "\n",
    "                    total_load_df.set_index('Date', inplace=True)\n",
    "\n",
    "                    # Merge to get Load sum values for dates above and below CI thresholds\n",
    "                    grouped = grouped.merge(total_load_df, left_index=True, right_index=True, how='left')\n",
    "                        \n",
    "                    # Only proceed if the month matches\n",
    "                    ci_max_month = ci_max_df[ (ci_max_df['State'] == state)]\n",
    "                    ci_min_month = ci_min_df[ (ci_min_df['State'] == state)]\n",
    "                        \n",
    "                    \n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]]\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]]\n",
    "\n",
    " \n",
    "                    for date, row in dates_above_max_ci.iterrows():\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['Total_Load']})\n",
    "                    \n",
    "                    for date, row in dates_below_min_ci.iterrows():\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date, 'Load_sum': row['Total_Load']})\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{state}_extreme_demand_outliers_max_{case}_project_.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{state}_extreme_demand_outliers_min_{case}_project_.csv'), index=False)\n",
    "    # Initialize an empty DataFrame to store all results\n",
    "    all_results_df = pd.DataFrame()\n",
    "    folder = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier_demand'\n",
    "\n",
    "    # List all \"max\" files in the folder\n",
    "    max_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_demand_outliers_max_{case}_project_.csv')]\n",
    "\n",
    "    for file in max_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_demand_outliers_max_{case}_project_.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        # Assuming 'Total_Load' is a column in 'df'\n",
    "        # Group by 'Year' and calculate the average 'Total_Load' for each year\n",
    "        year_average_total_load = df.groupby('Year')['Load_sum'].mean().reset_index(name='average_total_load')\n",
    "\n",
    "        # Add the region to the DataFrame\n",
    "        year_average_total_load['region'] = region\n",
    "\n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, year_average_total_load], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'average_total_load']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"max\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_max_outliers_demand_summary_{case}_project_.csv', index=False)\n",
    "    all_results_df= pd.DataFrame()\n",
    "\n",
    "\n",
    "    # List all \"min\" files in the folder\n",
    "    min_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_demand_outliers_min_{case}_project_.csv')]\n",
    "\n",
    "    for file in min_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_demand_outliers_min_{case}_project_.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        print(df.groupby('Year')['Load_sum'].mean())\n",
    "        # Group by 'Year' and calculate the average 'Total_Load' for each year\n",
    "        year_average_total_load = df.groupby('Year')['Load_sum'].mean().reset_index(name='average_total_load')\n",
    "\n",
    "        # Add the region to the DataFrame\n",
    "        year_average_total_load['region'] = region\n",
    "\n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, year_average_total_load], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'average_total_load']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"min\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_min_outliers_demand_summary_{case}_project_.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty file: p5_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Texas_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Missouri_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p134_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Montana_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p74_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p61_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p113_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p106_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p79_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p46_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Nevada_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p53_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p8_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Pennsylvania_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Iowa_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Kansas_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p10_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: California_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p37_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p22_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p64_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p71_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p131_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p83_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p129_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p56_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p43_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p103_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p69_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p15_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Utah_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Idaho_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p27_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p32_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p18_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p81_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p126_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p2_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p59_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p133_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Ohio_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p73_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New Jersey_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p119_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p41_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p54_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p28_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Minnesota_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p17_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New York_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p30_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p25_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Wisconsin_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Wyoming_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p63_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p76_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p49_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p7_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New Mexico_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p84_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p51_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p44_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Arkansas_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p104_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p111_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p12_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p38_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p20_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p35_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Nebraska_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: North Dakota_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p45_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p50_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Arizona_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Rhode Island_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Vermont_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Connecticut_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p105_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p77_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Michigan_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p108_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p62_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p85_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p122_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p6_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p48_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Maine_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p34_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p21_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Illinois_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: South Dakota_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p13_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p39_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: New Hampshire_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p55_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p40_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p132_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p3_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p127_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p80_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p67_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p72_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p24_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p31_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Washington_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p29_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p16_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p42_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p57_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p128_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p68_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Colorado_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p70_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p65_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Oregon_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p1_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p82_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p130_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p33_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p26_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p19_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Indiana_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p14_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p78_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: usa_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p107_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p112_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p9_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p52_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p47_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p4_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p60_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Oklahoma_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p75_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p23_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p36_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: Massachusetts_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p11_extreme_outliers_max_projection_project_.csv\n",
      "Skipping empty file: p101_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p114_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p99_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p94_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Ohio_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p126_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: New Jersey_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p66_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p119_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Kentucky_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Arkansas_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p51_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p89_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p111_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p104_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p109_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Alabama_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p84_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p123_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p91_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: South Carolina_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p106_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p113_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Delaware_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Pennsylvania_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Maryland_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p93_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p86_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p121_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Texas_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p61_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: West Virginia_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Virginia_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p116_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p103_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p64_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p124_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Georgia_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p96_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Indiana_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p65_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p97_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p125_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p57_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p128_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p102_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Louisiana_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p117_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p120_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p87_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p92_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Oklahoma_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p60_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p112_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p107_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p47_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Illinois_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p62_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Michigan_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: North Carolina_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p108_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p90_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p48_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p122_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p85_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p88_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p50_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Connecticut_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p105_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p110_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Tennessee_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Florida_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: Mississippi_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p58_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p80_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p132_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p95_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p118_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p67_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p115_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p100_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p98_extreme_outliers_min_projection_project_.csv\n",
      "Skipping empty file: p55_extreme_outliers_min_projection_project_.csv\n"
     ]
    }
   ],
   "source": [
    "for case in cases:\n",
    "    # Specify the folder\n",
    "    # Directories initialization\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/output/future_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    rb_codes = [f'p{i}' for i in range(1, 135)]\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    extreme_outliers_max_list = []\n",
    "    extreme_outliers_min_list = []\n",
    "\n",
    "    for rb_code in rb_codes:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "        ci_max_path = os.path.join(ci_directory, f'{rb_code}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{rb_code}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "            \n",
    "            for year in years:\n",
    "                weather_file_path = os.path.join(directory, f'{rb_code}_WRF_Hourly_Mean_Meteorology_{year}.csv')\n",
    "                if os.path.exists(weather_file_path):\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    df['Month'] = pd.to_datetime(df['Time_UTC']).dt.month\n",
    "                        \n",
    "                    # Only proceed if the month matches\n",
    "                    ci_max_month = ci_max_df[ (ci_max_df['rb'] == rb_code)]\n",
    "                    ci_min_month = ci_min_df[ (ci_min_df['rb'] == rb_code)]\n",
    "                        \n",
    "                    # Find the dates with the highest and lowest T2 for each day\n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist()\n",
    "                    # print(ci_max_df)\n",
    "                    # print(grouped)\n",
    "                    # print(grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist())\n",
    "                    for date in dates_above_max_ci:\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "                    # Find all dates where min T2 is below the CI_min_lower\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]].index.tolist()\n",
    "                    for date in dates_below_min_ci:\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_outliers_max_{case}_project_.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{rb_code}_extreme_outliers_min_{case}_project_.csv'), index=False)\n",
    "\n",
    "\n",
    "    # Directories initialization\n",
    "    directory = f'/Users/ansonkong/Downloads/Data for nyu work/averaged_{case}_weather'\n",
    "    ci_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/CI_results'\n",
    "    output_directory = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    states=['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "            'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "            'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "            'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "            'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming','usa']\n",
    "    years = range(2020, 2100)\n",
    "\n",
    "    extreme_outliers_max_list = []\n",
    "    extreme_outliers_min_list = []\n",
    "\n",
    "    for state in states:\n",
    "        extreme_outliers_max_list = []\n",
    "        extreme_outliers_min_list = []\n",
    "        ci_max_path = os.path.join(ci_directory, f'{state}_CI_max.csv')\n",
    "        ci_min_path = os.path.join(ci_directory, f'{state}_CI_min.csv')\n",
    "        \n",
    "        if os.path.exists(ci_max_path) and os.path.exists(ci_min_path):\n",
    "            ci_max_df = pd.read_csv(ci_max_path)\n",
    "            ci_min_df = pd.read_csv(ci_min_path)\n",
    "            \n",
    "            for year in years:\n",
    "                weather_file_path = os.path.join(directory, f'{state}_averaged_weather_{year}.csv')\n",
    "                if os.path.exists(weather_file_path):\n",
    "                    df = pd.read_csv(weather_file_path)\n",
    "                    df['Date'] = pd.to_datetime(df['Time_UTC']).dt.date\n",
    "                    df['Month'] = pd.to_datetime(df['Time_UTC']).dt.month\n",
    "                        \n",
    "                    # Only proceed if the month matches\n",
    "                    ci_max_month = ci_max_df[ (ci_max_df['State'] == state)]\n",
    "                    ci_min_month = ci_min_df[ (ci_min_df['State'] == state)]\n",
    "                        \n",
    "                    # Find the dates with the highest and lowest T2 for each day\n",
    "                    grouped = df.groupby('Date')['T2'].agg(['max', 'min'])\n",
    "                    dates_above_max_ci = grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist()\n",
    "                    # print(ci_max_df)\n",
    "                    # print(grouped)\n",
    "                    # print(grouped[grouped['max'] > ci_max_month['CI_max_upper'].iloc[0]].index.tolist())\n",
    "                    for date in dates_above_max_ci:\n",
    "                        extreme_outliers_max_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "                    # Find all dates where min T2 is below the CI_min_lower\n",
    "                    dates_below_min_ci = grouped[grouped['min'] < ci_min_month['CI_min_lower'].iloc[0]].index.tolist()\n",
    "                    for date in dates_below_min_ci:\n",
    "                        extreme_outliers_min_list.append({'rb': rb_code, 'Year': year, 'Date': date})\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        extreme_outliers_max = pd.DataFrame(extreme_outliers_max_list)\n",
    "        extreme_outliers_min = pd.DataFrame(extreme_outliers_min_list)\n",
    "\n",
    "        # Save the results\n",
    "        extreme_outliers_max.to_csv(os.path.join(output_directory, f'{state}_extreme_outliers_max_{case}_project_.csv'), index=False)\n",
    "        extreme_outliers_min.to_csv(os.path.join(output_directory, f'{state}_extreme_outliers_min_{case}_project_.csv'), index=False)\n",
    "\n",
    "\n",
    "    folder = '/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/outlier'\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results\n",
    "    all_results_df = pd.DataFrame()\n",
    "\n",
    "    # List all \"max\" files in the folder\n",
    "    max_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_outliers_max_{case}_project_.csv')]\n",
    "\n",
    "    for file in max_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_outliers_max_{case}_project_.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        # Group by 'Year' and count the number of observations for each year\n",
    "        year_counts = df.groupby('Year').size().reset_index(name='number_of_days')\n",
    "        \n",
    "        # Add the region to the DataFrame\n",
    "        year_counts['region'] = region\n",
    "        \n",
    "        # Append the result to the all_results_df DataFrame\n",
    "        all_results_df = pd.concat([all_results_df, year_counts], ignore_index=True)\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_results_df = all_results_df[['region', 'Year', 'number_of_days']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"max\" files\n",
    "    # Optionally, save this DataFrame to a new CSV file\n",
    "    all_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_max_outliers_summary_{case}_project_.csv', index=False)\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results for minimum outliers\n",
    "    all_min_results_df = pd.DataFrame()\n",
    "\n",
    "    # List all \"min\" files in the folder\n",
    "    min_files = [f for f in os.listdir(folder) if f.endswith(f'_extreme_outliers_min_{case}_project_.csv')]\n",
    "\n",
    "    for file in min_files:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(folder, file)\n",
    "        \n",
    "        # Extract the region from the file name\n",
    "        region = file.split(f'_extreme_outliers_min_{case}_project_.csv')[0]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to read the file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {file}\")\n",
    "            continue \n",
    "        \n",
    "        # Group by 'Year' and count the number of observations for each year\n",
    "        year_counts = df.groupby('Year').size().reset_index(name='number_of_days')\n",
    "        \n",
    "        # Add the region to the DataFrame\n",
    "        year_counts['region'] = region\n",
    "        \n",
    "        # Append the result to the all_min_results_df DataFrame\n",
    "        all_min_results_df = pd.concat([all_min_results_df, year_counts], ignore_index=True)\n",
    "\n",
    "    # Reorder the DataFrame columns if needed\n",
    "    all_min_results_df = all_min_results_df[['region', 'Year', 'number_of_days']]\n",
    "\n",
    "    # You now have a DataFrame containing the region, Year, and number_of_days for all \"min\" files\n",
    "    # Optionally, save this DataFrame to a new CSV fileproject\n",
    "    all_min_results_df.to_csv(f'/Users/ansonkong/Downloads/demand_prediciton_with_weather/resources/all_min_outliers_summary_{case}__.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
